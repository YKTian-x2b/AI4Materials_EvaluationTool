{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright 2021-2023 @ Shenzhen Bay Laboratory & Peking University & Huawei Technologies Co., Ltd\n",
    "\n",
    "This code is a part of Cybertron package.\n",
    "\n",
    "The Cybertron is open-source software based on the AI-framework:\n",
    "MindSpore (https://www.mindspore.cn/)\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "\n",
    "You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License.\n",
    "\n",
    "Cybertron tutorial 10: Run MD simulation in with CybertronFF as potential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "from mindspore import load_checkpoint\n",
    "from mindspore import context\n",
    "\n",
    "from mindsponge import Molecule\n",
    "from mindsponge import Sponge\n",
    "from mindsponge import set_global_units\n",
    "from mindsponge.callback import RunInfo, WriteH5MD\n",
    "from mindsponge.control import LeapFrog\n",
    "from mindsponge.control import Langevin\n",
    "from mindsponge.optimizer import UpdaterMD\n",
    "\n",
    "from cybertron.model import MolCT\n",
    "from cybertron.readout import AtomwiseReadout\n",
    "from cybertron.cybertron import CybertronFF\n",
    "\n",
    "context.set_context(mode=context.GRAPH_MODE, device_target=\"GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_global_units('A', 'kcal/mol')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "atom_types = np.array(\n",
    "    [[6, 1, 1, 1, 6, 8, 6, 8, 6, 8, 6, 8, 6, 8, 25]], np.int32)\n",
    "coordinate = np.array([\n",
    "    [0.782936, -0.21384, 1.940403],\n",
    "    [0.90026, -1.258313, 2.084498],\n",
    "    [1.793443, 0.267702, 1.791434],\n",
    "    [0.161631, 0.247471, 2.702921],\n",
    "    [-1.775807, 0.660242, 0.992526],\n",
    "    [-2.573144, 0.82639, 1.806692],\n",
    "    [-0.793238, 0.551875, -1.559148],\n",
    "    [-0.922246, 0.719072, -2.702972],\n",
    "    [1.526357, -0.229486, -0.35567],\n",
    "    [2.624975, -0.473657, -0.641924],\n",
    "    [-0.786405, -1.533853, -0.007962],\n",
    "    [-1.266142, -2.537492, 0.254628],\n",
    "    [0.394547, 1.910025, 0.468161],\n",
    "    [0.747036, 3.027445, 0.458565],\n",
    "    [-0.163356, 0.241977, 0.175396],\n",
    "])\n",
    "system = Molecule(atomic_number=atom_types, coordinate=coordinate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = MolCT(\n",
    "    dim_feature=128,\n",
    "    num_atom_types=100,\n",
    "    n_interaction=3,\n",
    "    n_heads=8,\n",
    "    max_cycles=1,\n",
    "    cutoff=10,\n",
    "    fixed_cycles=True,\n",
    "    length_unit='A',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "readout = AtomwiseReadout(\n",
    "    model=mod,\n",
    "    dim_output=1,\n",
    "    activation=mod.activation,\n",
    "    scale=0.5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "potential = CybertronFF(\n",
    "    model=mod,\n",
    "    readout=readout,\n",
    "    atom_types=atom_types,\n",
    "    length_unit='A',\n",
    "    energy_unit='kcal/mol',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model.atom_embedding.embedding_table': Parameter (name=model.atom_embedding.embedding_table, shape=(100, 128), dtype=Float32, requires_grad=True),\n",
       " 'model.dis_filter.linear.weight': Parameter (name=model.dis_filter.linear.weight, shape=(128, 64), dtype=Float32, requires_grad=True),\n",
       " 'model.dis_filter.linear.bias': Parameter (name=model.dis_filter.linear.bias, shape=(128,), dtype=Float32, requires_grad=True),\n",
       " 'model.dis_filter.residual.nonlinear.mlp.0.weight': Parameter (name=model.dis_filter.residual.nonlinear.mlp.0.weight, shape=(128, 128), dtype=Float32, requires_grad=True),\n",
       " 'model.dis_filter.residual.nonlinear.mlp.0.bias': Parameter (name=model.dis_filter.residual.nonlinear.mlp.0.bias, shape=(128,), dtype=Float32, requires_grad=True),\n",
       " 'model.dis_filter.residual.nonlinear.mlp.1.weight': Parameter (name=model.dis_filter.residual.nonlinear.mlp.1.weight, shape=(128, 128), dtype=Float32, requires_grad=True),\n",
       " 'model.dis_filter.residual.nonlinear.mlp.1.bias': Parameter (name=model.dis_filter.residual.nonlinear.mlp.1.bias, shape=(128,), dtype=Float32, requires_grad=True),\n",
       " 'model.interactions.0.positional_embedding.norm.gamma': Parameter (name=model.interactions.0.positional_embedding.norm.gamma, shape=(128,), dtype=Float32, requires_grad=True),\n",
       " 'model.interactions.0.positional_embedding.norm.beta': Parameter (name=model.interactions.0.positional_embedding.norm.beta, shape=(128,), dtype=Float32, requires_grad=True),\n",
       " 'model.interactions.0.positional_embedding.x2q.weight': Parameter (name=model.interactions.0.positional_embedding.x2q.weight, shape=(128, 128), dtype=Float32, requires_grad=True),\n",
       " 'model.interactions.0.positional_embedding.x2k.weight': Parameter (name=model.interactions.0.positional_embedding.x2k.weight, shape=(128, 128), dtype=Float32, requires_grad=True),\n",
       " 'model.interactions.0.positional_embedding.x2v.weight': Parameter (name=model.interactions.0.positional_embedding.x2v.weight, shape=(128, 128), dtype=Float32, requires_grad=True),\n",
       " 'model.interactions.0.multi_head_attention.output.weight': Parameter (name=model.interactions.0.multi_head_attention.output.weight, shape=(128, 128), dtype=Float32, requires_grad=True),\n",
       " 'model.interactions.1.positional_embedding.norm.gamma': Parameter (name=model.interactions.1.positional_embedding.norm.gamma, shape=(128,), dtype=Float32, requires_grad=True),\n",
       " 'model.interactions.1.positional_embedding.norm.beta': Parameter (name=model.interactions.1.positional_embedding.norm.beta, shape=(128,), dtype=Float32, requires_grad=True),\n",
       " 'model.interactions.1.positional_embedding.x2q.weight': Parameter (name=model.interactions.1.positional_embedding.x2q.weight, shape=(128, 128), dtype=Float32, requires_grad=True),\n",
       " 'model.interactions.1.positional_embedding.x2k.weight': Parameter (name=model.interactions.1.positional_embedding.x2k.weight, shape=(128, 128), dtype=Float32, requires_grad=True),\n",
       " 'model.interactions.1.positional_embedding.x2v.weight': Parameter (name=model.interactions.1.positional_embedding.x2v.weight, shape=(128, 128), dtype=Float32, requires_grad=True),\n",
       " 'model.interactions.1.multi_head_attention.output.weight': Parameter (name=model.interactions.1.multi_head_attention.output.weight, shape=(128, 128), dtype=Float32, requires_grad=True),\n",
       " 'model.interactions.2.positional_embedding.norm.gamma': Parameter (name=model.interactions.2.positional_embedding.norm.gamma, shape=(128,), dtype=Float32, requires_grad=True),\n",
       " 'model.interactions.2.positional_embedding.norm.beta': Parameter (name=model.interactions.2.positional_embedding.norm.beta, shape=(128,), dtype=Float32, requires_grad=True),\n",
       " 'model.interactions.2.positional_embedding.x2q.weight': Parameter (name=model.interactions.2.positional_embedding.x2q.weight, shape=(128, 128), dtype=Float32, requires_grad=True),\n",
       " 'model.interactions.2.positional_embedding.x2k.weight': Parameter (name=model.interactions.2.positional_embedding.x2k.weight, shape=(128, 128), dtype=Float32, requires_grad=True),\n",
       " 'model.interactions.2.positional_embedding.x2v.weight': Parameter (name=model.interactions.2.positional_embedding.x2v.weight, shape=(128, 128), dtype=Float32, requires_grad=True),\n",
       " 'model.interactions.2.multi_head_attention.output.weight': Parameter (name=model.interactions.2.multi_head_attention.output.weight, shape=(128, 128), dtype=Float32, requires_grad=True),\n",
       " 'readout.decoder.output.mlp.0.weight': Parameter (name=readout.decoder.output.mlp.0.weight, shape=(64, 128), dtype=Float32, requires_grad=True),\n",
       " 'readout.decoder.output.mlp.0.bias': Parameter (name=readout.decoder.output.mlp.0.bias, shape=(64,), dtype=Float32, requires_grad=True),\n",
       " 'readout.decoder.output.mlp.1.weight': Parameter (name=readout.decoder.output.mlp.1.weight, shape=(1, 64), dtype=Float32, requires_grad=True),\n",
       " 'readout.decoder.output.mlp.1.bias': Parameter (name=readout.decoder.output.mlp.1.bias, shape=(1,), dtype=Float32, requires_grad=True),\n",
       " 'step': Parameter (name=step, shape=(), dtype=Int32, requires_grad=True),\n",
       " 'global_step': Parameter (name=global_step, shape=(1,), dtype=Int32, requires_grad=True),\n",
       " 'beta1_power': Parameter (name=beta1_power, shape=(1,), dtype=Float32, requires_grad=True),\n",
       " 'beta2_power': Parameter (name=beta2_power, shape=(1,), dtype=Float32, requires_grad=True),\n",
       " 'moment1.model.atom_embedding.embedding_table': Parameter (name=moment1.model.atom_embedding.embedding_table, shape=(100, 128), dtype=Float32, requires_grad=True),\n",
       " 'moment1.model.dis_filter.linear.weight': Parameter (name=moment1.model.dis_filter.linear.weight, shape=(128, 64), dtype=Float32, requires_grad=True),\n",
       " 'moment1.model.dis_filter.linear.bias': Parameter (name=moment1.model.dis_filter.linear.bias, shape=(128,), dtype=Float32, requires_grad=True),\n",
       " 'moment1.model.dis_filter.residual.nonlinear.mlp.0.weight': Parameter (name=moment1.model.dis_filter.residual.nonlinear.mlp.0.weight, shape=(128, 128), dtype=Float32, requires_grad=True),\n",
       " 'moment1.model.dis_filter.residual.nonlinear.mlp.0.bias': Parameter (name=moment1.model.dis_filter.residual.nonlinear.mlp.0.bias, shape=(128,), dtype=Float32, requires_grad=True),\n",
       " 'moment1.model.dis_filter.residual.nonlinear.mlp.1.weight': Parameter (name=moment1.model.dis_filter.residual.nonlinear.mlp.1.weight, shape=(128, 128), dtype=Float32, requires_grad=True),\n",
       " 'moment1.model.dis_filter.residual.nonlinear.mlp.1.bias': Parameter (name=moment1.model.dis_filter.residual.nonlinear.mlp.1.bias, shape=(128,), dtype=Float32, requires_grad=True),\n",
       " 'moment1.model.interactions.0.positional_embedding.norm.gamma': Parameter (name=moment1.model.interactions.0.positional_embedding.norm.gamma, shape=(128,), dtype=Float32, requires_grad=True),\n",
       " 'moment1.model.interactions.0.positional_embedding.norm.beta': Parameter (name=moment1.model.interactions.0.positional_embedding.norm.beta, shape=(128,), dtype=Float32, requires_grad=True),\n",
       " 'moment1.model.interactions.0.positional_embedding.x2q.weight': Parameter (name=moment1.model.interactions.0.positional_embedding.x2q.weight, shape=(128, 128), dtype=Float32, requires_grad=True),\n",
       " 'moment1.model.interactions.0.positional_embedding.x2k.weight': Parameter (name=moment1.model.interactions.0.positional_embedding.x2k.weight, shape=(128, 128), dtype=Float32, requires_grad=True),\n",
       " 'moment1.model.interactions.0.positional_embedding.x2v.weight': Parameter (name=moment1.model.interactions.0.positional_embedding.x2v.weight, shape=(128, 128), dtype=Float32, requires_grad=True),\n",
       " 'moment1.model.interactions.0.multi_head_attention.output.weight': Parameter (name=moment1.model.interactions.0.multi_head_attention.output.weight, shape=(128, 128), dtype=Float32, requires_grad=True),\n",
       " 'moment1.model.interactions.1.positional_embedding.norm.gamma': Parameter (name=moment1.model.interactions.1.positional_embedding.norm.gamma, shape=(128,), dtype=Float32, requires_grad=True),\n",
       " 'moment1.model.interactions.1.positional_embedding.norm.beta': Parameter (name=moment1.model.interactions.1.positional_embedding.norm.beta, shape=(128,), dtype=Float32, requires_grad=True),\n",
       " 'moment1.model.interactions.1.positional_embedding.x2q.weight': Parameter (name=moment1.model.interactions.1.positional_embedding.x2q.weight, shape=(128, 128), dtype=Float32, requires_grad=True),\n",
       " 'moment1.model.interactions.1.positional_embedding.x2k.weight': Parameter (name=moment1.model.interactions.1.positional_embedding.x2k.weight, shape=(128, 128), dtype=Float32, requires_grad=True),\n",
       " 'moment1.model.interactions.1.positional_embedding.x2v.weight': Parameter (name=moment1.model.interactions.1.positional_embedding.x2v.weight, shape=(128, 128), dtype=Float32, requires_grad=True),\n",
       " 'moment1.model.interactions.1.multi_head_attention.output.weight': Parameter (name=moment1.model.interactions.1.multi_head_attention.output.weight, shape=(128, 128), dtype=Float32, requires_grad=True),\n",
       " 'moment1.model.interactions.2.positional_embedding.norm.gamma': Parameter (name=moment1.model.interactions.2.positional_embedding.norm.gamma, shape=(128,), dtype=Float32, requires_grad=True),\n",
       " 'moment1.model.interactions.2.positional_embedding.norm.beta': Parameter (name=moment1.model.interactions.2.positional_embedding.norm.beta, shape=(128,), dtype=Float32, requires_grad=True),\n",
       " 'moment1.model.interactions.2.positional_embedding.x2q.weight': Parameter (name=moment1.model.interactions.2.positional_embedding.x2q.weight, shape=(128, 128), dtype=Float32, requires_grad=True),\n",
       " 'moment1.model.interactions.2.positional_embedding.x2k.weight': Parameter (name=moment1.model.interactions.2.positional_embedding.x2k.weight, shape=(128, 128), dtype=Float32, requires_grad=True),\n",
       " 'moment1.model.interactions.2.positional_embedding.x2v.weight': Parameter (name=moment1.model.interactions.2.positional_embedding.x2v.weight, shape=(128, 128), dtype=Float32, requires_grad=True),\n",
       " 'moment1.model.interactions.2.multi_head_attention.output.weight': Parameter (name=moment1.model.interactions.2.multi_head_attention.output.weight, shape=(128, 128), dtype=Float32, requires_grad=True),\n",
       " 'moment1.readout.decoder.output.mlp.0.weight': Parameter (name=moment1.readout.decoder.output.mlp.0.weight, shape=(64, 128), dtype=Float32, requires_grad=True),\n",
       " 'moment1.readout.decoder.output.mlp.0.bias': Parameter (name=moment1.readout.decoder.output.mlp.0.bias, shape=(64,), dtype=Float32, requires_grad=True),\n",
       " 'moment1.readout.decoder.output.mlp.1.weight': Parameter (name=moment1.readout.decoder.output.mlp.1.weight, shape=(1, 64), dtype=Float32, requires_grad=True),\n",
       " 'moment1.readout.decoder.output.mlp.1.bias': Parameter (name=moment1.readout.decoder.output.mlp.1.bias, shape=(1,), dtype=Float32, requires_grad=True),\n",
       " 'moment2.model.atom_embedding.embedding_table': Parameter (name=moment2.model.atom_embedding.embedding_table, shape=(100, 128), dtype=Float32, requires_grad=True),\n",
       " 'moment2.model.dis_filter.linear.weight': Parameter (name=moment2.model.dis_filter.linear.weight, shape=(128, 64), dtype=Float32, requires_grad=True),\n",
       " 'moment2.model.dis_filter.linear.bias': Parameter (name=moment2.model.dis_filter.linear.bias, shape=(128,), dtype=Float32, requires_grad=True),\n",
       " 'moment2.model.dis_filter.residual.nonlinear.mlp.0.weight': Parameter (name=moment2.model.dis_filter.residual.nonlinear.mlp.0.weight, shape=(128, 128), dtype=Float32, requires_grad=True),\n",
       " 'moment2.model.dis_filter.residual.nonlinear.mlp.0.bias': Parameter (name=moment2.model.dis_filter.residual.nonlinear.mlp.0.bias, shape=(128,), dtype=Float32, requires_grad=True),\n",
       " 'moment2.model.dis_filter.residual.nonlinear.mlp.1.weight': Parameter (name=moment2.model.dis_filter.residual.nonlinear.mlp.1.weight, shape=(128, 128), dtype=Float32, requires_grad=True),\n",
       " 'moment2.model.dis_filter.residual.nonlinear.mlp.1.bias': Parameter (name=moment2.model.dis_filter.residual.nonlinear.mlp.1.bias, shape=(128,), dtype=Float32, requires_grad=True),\n",
       " 'moment2.model.interactions.0.positional_embedding.norm.gamma': Parameter (name=moment2.model.interactions.0.positional_embedding.norm.gamma, shape=(128,), dtype=Float32, requires_grad=True),\n",
       " 'moment2.model.interactions.0.positional_embedding.norm.beta': Parameter (name=moment2.model.interactions.0.positional_embedding.norm.beta, shape=(128,), dtype=Float32, requires_grad=True),\n",
       " 'moment2.model.interactions.0.positional_embedding.x2q.weight': Parameter (name=moment2.model.interactions.0.positional_embedding.x2q.weight, shape=(128, 128), dtype=Float32, requires_grad=True),\n",
       " 'moment2.model.interactions.0.positional_embedding.x2k.weight': Parameter (name=moment2.model.interactions.0.positional_embedding.x2k.weight, shape=(128, 128), dtype=Float32, requires_grad=True),\n",
       " 'moment2.model.interactions.0.positional_embedding.x2v.weight': Parameter (name=moment2.model.interactions.0.positional_embedding.x2v.weight, shape=(128, 128), dtype=Float32, requires_grad=True),\n",
       " 'moment2.model.interactions.0.multi_head_attention.output.weight': Parameter (name=moment2.model.interactions.0.multi_head_attention.output.weight, shape=(128, 128), dtype=Float32, requires_grad=True),\n",
       " 'moment2.model.interactions.1.positional_embedding.norm.gamma': Parameter (name=moment2.model.interactions.1.positional_embedding.norm.gamma, shape=(128,), dtype=Float32, requires_grad=True),\n",
       " 'moment2.model.interactions.1.positional_embedding.norm.beta': Parameter (name=moment2.model.interactions.1.positional_embedding.norm.beta, shape=(128,), dtype=Float32, requires_grad=True),\n",
       " 'moment2.model.interactions.1.positional_embedding.x2q.weight': Parameter (name=moment2.model.interactions.1.positional_embedding.x2q.weight, shape=(128, 128), dtype=Float32, requires_grad=True),\n",
       " 'moment2.model.interactions.1.positional_embedding.x2k.weight': Parameter (name=moment2.model.interactions.1.positional_embedding.x2k.weight, shape=(128, 128), dtype=Float32, requires_grad=True),\n",
       " 'moment2.model.interactions.1.positional_embedding.x2v.weight': Parameter (name=moment2.model.interactions.1.positional_embedding.x2v.weight, shape=(128, 128), dtype=Float32, requires_grad=True),\n",
       " 'moment2.model.interactions.1.multi_head_attention.output.weight': Parameter (name=moment2.model.interactions.1.multi_head_attention.output.weight, shape=(128, 128), dtype=Float32, requires_grad=True),\n",
       " 'moment2.model.interactions.2.positional_embedding.norm.gamma': Parameter (name=moment2.model.interactions.2.positional_embedding.norm.gamma, shape=(128,), dtype=Float32, requires_grad=True),\n",
       " 'moment2.model.interactions.2.positional_embedding.norm.beta': Parameter (name=moment2.model.interactions.2.positional_embedding.norm.beta, shape=(128,), dtype=Float32, requires_grad=True),\n",
       " 'moment2.model.interactions.2.positional_embedding.x2q.weight': Parameter (name=moment2.model.interactions.2.positional_embedding.x2q.weight, shape=(128, 128), dtype=Float32, requires_grad=True),\n",
       " 'moment2.model.interactions.2.positional_embedding.x2k.weight': Parameter (name=moment2.model.interactions.2.positional_embedding.x2k.weight, shape=(128, 128), dtype=Float32, requires_grad=True),\n",
       " 'moment2.model.interactions.2.positional_embedding.x2v.weight': Parameter (name=moment2.model.interactions.2.positional_embedding.x2v.weight, shape=(128, 128), dtype=Float32, requires_grad=True),\n",
       " 'moment2.model.interactions.2.multi_head_attention.output.weight': Parameter (name=moment2.model.interactions.2.multi_head_attention.output.weight, shape=(128, 128), dtype=Float32, requires_grad=True),\n",
       " 'moment2.readout.decoder.output.mlp.0.weight': Parameter (name=moment2.readout.decoder.output.mlp.0.weight, shape=(64, 128), dtype=Float32, requires_grad=True),\n",
       " 'moment2.readout.decoder.output.mlp.0.bias': Parameter (name=moment2.readout.decoder.output.mlp.0.bias, shape=(64,), dtype=Float32, requires_grad=True),\n",
       " 'moment2.readout.decoder.output.mlp.1.weight': Parameter (name=moment2.readout.decoder.output.mlp.1.weight, shape=(1, 64), dtype=Float32, requires_grad=True),\n",
       " 'moment2.readout.decoder.output.mlp.1.bias': Parameter (name=moment2.readout.decoder.output.mlp.1.bias, shape=(1,), dtype=Float32, requires_grad=True)}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_file = 'checkpoint_c10.ckpt'\n",
    "load_checkpoint(param_file, net=potential)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = UpdaterMD(\n",
    "    system,\n",
    "    integrator=LeapFrog(system),\n",
    "    thermostat=Langevin(system, 300),\n",
    "    time_step=1e-4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MindSPONGE] Using fully connected neighbour list (not updated).\n",
      "[[58.57325]]\n"
     ]
    }
   ],
   "source": [
    "md = Sponge(system, potential, opt)\n",
    "print(md.calc_energy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb_h5md = WriteH5MD(system, 'Tutorial_C10.h5md', save_freq=10)\n",
    "cb_sim = RunInfo(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MindSPONGE] Started simulation at 2023-03-29 11:00:04\n",
      "[MindSPONGE] Step: 0, E_pot: 58.57325, E_kin: 0.0, E_tot: 58.57325, Temperature: 0.0\n",
      "[MindSPONGE] Step: 10, E_pot: 58.53354, E_kin: 0.266128, E_tot: 58.799667, Temperature: 6.377182\n",
      "[MindSPONGE] Step: 20, E_pot: 58.389404, E_kin: 0.654327, E_tot: 59.04373, Temperature: 15.679529\n",
      "[MindSPONGE] Step: 30, E_pot: 58.18113, E_kin: 1.0526463, E_tot: 59.233776, Temperature: 25.22439\n",
      "[MindSPONGE] Step: 40, E_pot: 57.88836, E_kin: 1.4170064, E_tot: 59.305367, Temperature: 33.955494\n",
      "[MindSPONGE] Step: 50, E_pot: 57.479675, E_kin: 2.1047952, E_tot: 59.58447, Temperature: 50.436863\n",
      "[MindSPONGE] Step: 60, E_pot: 56.903755, E_kin: 3.097414, E_tot: 60.001167, Temperature: 74.222824\n",
      "[MindSPONGE] Step: 70, E_pot: 56.200455, E_kin: 3.742854, E_tot: 59.94331, Temperature: 89.6894\n",
      "[MindSPONGE] Step: 80, E_pot: 55.408195, E_kin: 4.875395, E_tot: 60.283592, Temperature: 116.828285\n",
      "[MindSPONGE] Step: 90, E_pot: 54.68077, E_kin: 5.6277485, E_tot: 60.308517, Temperature: 134.85681\n",
      "[MindSPONGE] Step: 100, E_pot: 54.08075, E_kin: 6.352831, E_tot: 60.43358, Temperature: 152.23184\n",
      "[MindSPONGE] Step: 110, E_pot: 53.542736, E_kin: 6.906525, E_tot: 60.44926, Temperature: 165.49994\n",
      "[MindSPONGE] Step: 120, E_pot: 53.00764, E_kin: 7.91173, E_tot: 60.919373, Temperature: 189.58748\n",
      "[MindSPONGE] Step: 130, E_pot: 52.426235, E_kin: 7.813053, E_tot: 60.23929, Temperature: 187.2229\n",
      "[MindSPONGE] Step: 140, E_pot: 51.842003, E_kin: 8.010284, E_tot: 59.852287, Temperature: 191.94914\n",
      "[MindSPONGE] Step: 150, E_pot: 51.41942, E_kin: 9.391947, E_tot: 60.811363, Temperature: 225.05768\n",
      "[MindSPONGE] Step: 160, E_pot: 51.20937, E_kin: 10.26529, E_tot: 61.47466, Temperature: 245.98547\n",
      "[MindSPONGE] Step: 170, E_pot: 50.80282, E_kin: 10.204857, E_tot: 61.007675, Temperature: 244.53731\n",
      "[MindSPONGE] Step: 180, E_pot: 49.878006, E_kin: 10.469845, E_tot: 60.34785, Temperature: 250.88718\n",
      "[MindSPONGE] Step: 190, E_pot: 48.376106, E_kin: 11.849728, E_tot: 60.225834, Temperature: 283.9531\n",
      "[MindSPONGE] Step: 200, E_pot: 46.71382, E_kin: 13.899315, E_tot: 60.613136, Temperature: 333.06702\n",
      "[MindSPONGE] Step: 210, E_pot: 45.214268, E_kin: 14.796503, E_tot: 60.010773, Temperature: 354.56616\n",
      "[MindSPONGE] Step: 220, E_pot: 43.945316, E_kin: 15.969079, E_tot: 59.914394, Temperature: 382.66443\n",
      "[MindSPONGE] Step: 230, E_pot: 42.725227, E_kin: 17.952093, E_tot: 60.677322, Temperature: 430.18304\n",
      "[MindSPONGE] Step: 240, E_pot: 41.303757, E_kin: 19.6205, E_tot: 60.924255, Temperature: 470.16284\n",
      "[MindSPONGE] Step: 250, E_pot: 39.67536, E_kin: 20.57149, E_tot: 60.24685, Temperature: 492.9512\n",
      "[MindSPONGE] Step: 260, E_pot: 37.955112, E_kin: 22.553034, E_tot: 60.50815, Temperature: 540.43463\n",
      "[MindSPONGE] Step: 270, E_pot: 36.268745, E_kin: 24.070358, E_tot: 60.339104, Temperature: 576.79407\n",
      "[MindSPONGE] Step: 280, E_pot: 34.715492, E_kin: 24.763136, E_tot: 59.47863, Temperature: 593.39496\n",
      "[MindSPONGE] Step: 290, E_pot: 33.349617, E_kin: 25.766521, E_tot: 59.11614, Temperature: 617.4389\n",
      "[MindSPONGE] Step: 300, E_pot: 32.22089, E_kin: 25.29617, E_tot: 57.51706, Temperature: 606.16797\n",
      "[MindSPONGE] Step: 310, E_pot: 31.543896, E_kin: 26.322792, E_tot: 57.866688, Temperature: 630.76874\n",
      "[MindSPONGE] Step: 320, E_pot: 31.214191, E_kin: 24.978607, E_tot: 56.1928, Temperature: 598.5583\n",
      "[MindSPONGE] Step: 330, E_pot: 30.930365, E_kin: 25.584461, E_tot: 56.514824, Temperature: 613.0763\n",
      "[MindSPONGE] Step: 340, E_pot: 30.522655, E_kin: 26.33097, E_tot: 56.853626, Temperature: 630.9647\n",
      "[MindSPONGE] Step: 350, E_pot: 30.158005, E_kin: 25.722878, E_tot: 55.880882, Temperature: 616.39307\n",
      "[MindSPONGE] Step: 360, E_pot: 29.828302, E_kin: 25.78254, E_tot: 55.61084, Temperature: 617.82275\n",
      "[MindSPONGE] Step: 370, E_pot: 29.431414, E_kin: 26.644491, E_tot: 56.075905, Temperature: 638.47754\n",
      "[MindSPONGE] Step: 380, E_pot: 28.917255, E_kin: 25.367706, E_tot: 54.28496, Temperature: 607.88214\n",
      "[MindSPONGE] Step: 390, E_pot: 28.40858, E_kin: 25.29867, E_tot: 53.707253, Temperature: 606.22784\n",
      "[MindSPONGE] Step: 400, E_pot: 28.154816, E_kin: 25.534143, E_tot: 53.688957, Temperature: 611.8705\n",
      "[MindSPONGE] Step: 410, E_pot: 28.295235, E_kin: 23.526047, E_tot: 51.82128, Temperature: 563.7508\n",
      "[MindSPONGE] Step: 420, E_pot: 28.719515, E_kin: 22.75355, E_tot: 51.473064, Temperature: 545.2396\n",
      "[MindSPONGE] Step: 430, E_pot: 29.266006, E_kin: 21.848951, E_tot: 51.11496, Temperature: 523.5628\n",
      "[MindSPONGE] Step: 440, E_pot: 29.759403, E_kin: 21.648975, E_tot: 51.40838, Temperature: 518.7708\n",
      "[MindSPONGE] Step: 450, E_pot: 30.20008, E_kin: 22.109333, E_tot: 52.309414, Temperature: 529.8023\n",
      "[MindSPONGE] Step: 460, E_pot: 30.512127, E_kin: 21.632286, E_tot: 52.144413, Temperature: 518.3709\n",
      "[MindSPONGE] Step: 470, E_pot: 30.699905, E_kin: 21.569239, E_tot: 52.269142, Temperature: 516.86017\n",
      "[MindSPONGE] Step: 480, E_pot: 30.758884, E_kin: 21.086273, E_tot: 51.845158, Temperature: 505.28687\n",
      "[MindSPONGE] Step: 490, E_pot: 30.677961, E_kin: 20.549698, E_tot: 51.22766, Temperature: 492.42905\n",
      "[MindSPONGE] Step: 500, E_pot: 30.501911, E_kin: 20.72536, E_tot: 51.227272, Temperature: 496.63843\n",
      "[MindSPONGE] Step: 510, E_pot: 30.248693, E_kin: 20.243633, E_tot: 50.492325, Temperature: 485.09485\n",
      "[MindSPONGE] Step: 520, E_pot: 29.8596, E_kin: 20.065674, E_tot: 49.925274, Temperature: 480.83044\n",
      "[MindSPONGE] Step: 530, E_pot: 29.222466, E_kin: 19.06599, E_tot: 48.288456, Temperature: 456.8752\n",
      "[MindSPONGE] Step: 540, E_pot: 28.483423, E_kin: 20.289137, E_tot: 48.77256, Temperature: 486.18527\n",
      "[MindSPONGE] Step: 550, E_pot: 27.872473, E_kin: 21.571022, E_tot: 49.443497, Temperature: 516.90283\n",
      "[MindSPONGE] Step: 560, E_pot: 27.377026, E_kin: 21.101921, E_tot: 48.478947, Temperature: 505.66187\n",
      "[MindSPONGE] Step: 570, E_pot: 26.963245, E_kin: 21.648647, E_tot: 48.611893, Temperature: 518.763\n",
      "[MindSPONGE] Step: 580, E_pot: 26.569906, E_kin: 21.676529, E_tot: 48.246437, Temperature: 519.43115\n",
      "[MindSPONGE] Step: 590, E_pot: 26.106457, E_kin: 21.411272, E_tot: 47.51773, Temperature: 513.0748\n",
      "[MindSPONGE] Step: 600, E_pot: 25.522854, E_kin: 21.761463, E_tot: 47.284317, Temperature: 521.4664\n",
      "[MindSPONGE] Step: 610, E_pot: 24.682056, E_kin: 22.779583, E_tot: 47.46164, Temperature: 545.8634\n",
      "[MindSPONGE] Step: 620, E_pot: 23.63833, E_kin: 23.151787, E_tot: 46.790115, Temperature: 554.7824\n",
      "[MindSPONGE] Step: 630, E_pot: 22.445812, E_kin: 25.733881, E_tot: 48.179695, Temperature: 616.65674\n",
      "[MindSPONGE] Step: 640, E_pot: 21.309483, E_kin: 25.82481, E_tot: 47.134293, Temperature: 618.8357\n",
      "[MindSPONGE] Step: 650, E_pot: 20.355368, E_kin: 27.071726, E_tot: 47.427094, Temperature: 648.71533\n",
      "[MindSPONGE] Step: 660, E_pot: 19.851025, E_kin: 27.547272, E_tot: 47.398296, Temperature: 660.1108\n",
      "[MindSPONGE] Step: 670, E_pot: 19.88141, E_kin: 27.742378, E_tot: 47.623787, Temperature: 664.7861\n",
      "[MindSPONGE] Step: 680, E_pot: 20.327156, E_kin: 26.410442, E_tot: 46.7376, Temperature: 632.8691\n",
      "[MindSPONGE] Step: 690, E_pot: 20.93549, E_kin: 26.12223, E_tot: 47.05772, Temperature: 625.96277\n",
      "[MindSPONGE] Step: 700, E_pot: 21.428, E_kin: 25.959677, E_tot: 47.387676, Temperature: 622.06744\n",
      "[MindSPONGE] Step: 710, E_pot: 21.729876, E_kin: 25.60331, E_tot: 47.333183, Temperature: 613.52795\n",
      "[MindSPONGE] Step: 720, E_pot: 21.915989, E_kin: 25.719215, E_tot: 47.635204, Temperature: 616.30536\n",
      "[MindSPONGE] Step: 730, E_pot: 22.077076, E_kin: 27.037687, E_tot: 49.11476, Temperature: 647.89966\n",
      "[MindSPONGE] Step: 740, E_pot: 22.264479, E_kin: 27.636524, E_tot: 49.901, Temperature: 662.2495\n",
      "[MindSPONGE] Step: 750, E_pot: 22.474766, E_kin: 25.995277, E_tot: 48.470043, Temperature: 622.92053\n",
      "[MindSPONGE] Step: 760, E_pot: 22.643682, E_kin: 25.404053, E_tot: 48.047737, Temperature: 608.7532\n",
      "[MindSPONGE] Step: 770, E_pot: 22.731983, E_kin: 24.573578, E_tot: 47.30556, Temperature: 588.85266\n",
      "[MindSPONGE] Step: 780, E_pot: 22.728642, E_kin: 23.448875, E_tot: 46.177517, Temperature: 561.90155\n",
      "[MindSPONGE] Step: 790, E_pot: 22.619665, E_kin: 22.199583, E_tot: 44.81925, Temperature: 531.9649\n",
      "[MindSPONGE] Step: 800, E_pot: 22.408655, E_kin: 22.628876, E_tot: 45.03753, Temperature: 542.2521\n",
      "[MindSPONGE] Step: 810, E_pot: 22.125278, E_kin: 22.742556, E_tot: 44.867836, Temperature: 544.97614\n",
      "[MindSPONGE] Step: 820, E_pot: 21.743801, E_kin: 22.833708, E_tot: 44.577507, Temperature: 547.16034\n",
      "[MindSPONGE] Step: 830, E_pot: 21.309362, E_kin: 21.864399, E_tot: 43.17376, Temperature: 523.93304\n",
      "[MindSPONGE] Step: 840, E_pot: 20.834702, E_kin: 23.642422, E_tot: 44.477123, Temperature: 566.5394\n",
      "[MindSPONGE] Step: 850, E_pot: 20.34487, E_kin: 23.93517, E_tot: 44.280037, Temperature: 573.5545\n",
      "[MindSPONGE] Step: 860, E_pot: 19.799, E_kin: 24.75697, E_tot: 44.55597, Temperature: 593.2472\n",
      "[MindSPONGE] Step: 870, E_pot: 19.18016, E_kin: 25.638346, E_tot: 44.818504, Temperature: 614.3675\n",
      "[MindSPONGE] Step: 880, E_pot: 18.641403, E_kin: 26.980808, E_tot: 45.62221, Temperature: 646.5367\n",
      "[MindSPONGE] Step: 890, E_pot: 18.250422, E_kin: 27.134125, E_tot: 45.384544, Temperature: 650.2106\n",
      "[MindSPONGE] Step: 900, E_pot: 17.962765, E_kin: 26.948431, E_tot: 44.911194, Temperature: 645.76086\n",
      "[MindSPONGE] Step: 910, E_pot: 17.746471, E_kin: 26.238121, E_tot: 43.984592, Temperature: 628.7398\n",
      "[MindSPONGE] Step: 920, E_pot: 17.5499, E_kin: 26.176159, E_tot: 43.72606, Temperature: 627.255\n",
      "[MindSPONGE] Step: 930, E_pot: 17.34633, E_kin: 25.022598, E_tot: 42.368927, Temperature: 599.6124\n",
      "[MindSPONGE] Step: 940, E_pot: 17.102686, E_kin: 25.334469, E_tot: 42.437157, Temperature: 607.0857\n",
      "[MindSPONGE] Step: 950, E_pot: 16.84304, E_kin: 25.029083, E_tot: 41.872124, Temperature: 599.7678\n",
      "[MindSPONGE] Step: 960, E_pot: 16.6105, E_kin: 24.212753, E_tot: 40.823254, Temperature: 580.2062\n",
      "[MindSPONGE] Step: 970, E_pot: 16.396086, E_kin: 23.416027, E_tot: 39.81211, Temperature: 561.1144\n",
      "[MindSPONGE] Step: 980, E_pot: 16.186808, E_kin: 24.145609, E_tot: 40.332417, Temperature: 578.5972\n",
      "[MindSPONGE] Step: 990, E_pot: 15.990973, E_kin: 23.134819, E_tot: 39.125793, Temperature: 554.3759\n",
      "[MindSPONGE] Finished simulation at 2023-03-29 11:00:15\n",
      "[MindSPONGE] Simulation time: 11.44 seconds.\n",
      "--------------------------------------------------------------------------------\n",
      "Run Time: 00:00:11\n"
     ]
    }
   ],
   "source": [
    "beg_time = time.time()\n",
    "md.run(1000, callbacks=[cb_sim, cb_h5md])\n",
    "end_time = time.time()\n",
    "used_time = end_time - beg_time\n",
    "m, s = divmod(used_time, 60)\n",
    "h, m = divmod(m, 60)\n",
    "print(\"Run Time: %02d:%02d:%02d\" % (h, m, s))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.5 ('mindspore-1.8')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4976b8d1b143660084a7ba7652639898bf5b269ba26f14965a18b12288aa8002"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
