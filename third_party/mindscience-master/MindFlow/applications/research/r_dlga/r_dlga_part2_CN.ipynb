{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# R_DLGA算法_part2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 概述\n",
    "\n",
    "偏微分方程的数据驱动现有方法在处理高噪声的稀疏数据等复杂情况时，性能缺乏稳定性，因此鲁棒深度学习遗传算法(R-DLGA)被提出。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 技术路径\n",
    "\n",
    "R-DLGA求解该问题的具体流程如下：\n",
    "\n",
    "1. 运行train.py来训练神经网络、生成元数据并计算导数并通过遗传算法获取潜在项\n",
    "2. 根据步骤1结果修改train_pinn.py中dict目录中的候选项\n",
    "3. 运行train_pinn.py以获得最终结果\n",
    "\n",
    "本part2部分实现了第三步的功能代码，根据步骤一的遗传算法的结果对src/util.py下的term计算、part2部分中的dict进行修改后，运行本notebook以获取最终结果。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 引入代码包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import random\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from mindspore import nn, context, ops, set_seed\n",
    "from mindspore import value_and_grad, jit, save_checkpoint\n",
    "from mindspore import load_checkpoint, load_param_into_net\n",
    "from mindspore.amp import DynamicLossScaler, auto_mixed_precision\n",
    "\n",
    "from mindflow.utils import load_yaml_config\n",
    "from mindflow.cell import MultiScaleFCSequential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下述`src`包可以在[research/r_dlga/src](https://gitee.com/mindspore/mindscience/tree/master/MindFlow/applications/research/r_dlga/src)中下载。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import create_pinn_dataset, evaluate, cal_grads, cal_terms, pinn_loss_func\n",
    "from src import get_dict_name, get_dicts, update_lib, calculate_coef, get_lefts\n",
    "\n",
    "set_seed(0)\n",
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "进行参数配置，其中--case有三种选择，\"burgers\"表示对burgers方程进行训练，\"cylinder_flow\"表示对navier_stokes2D方程的圆柱绕流数据集进行训练，\"periodic_hill\"表示对雷诺平均Navier-Stokes方程的山流动数据集进行训练。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description=\"train r_glda\")\n",
    "parser.add_argument(\"--case\", type=str, default=\"burgers\", choices=[\"burgers\", \"cylinder_flow\", \"periodic_hill\"],\n",
    "                    help=\"choose burgers, cylinder_flow or periodic_hill\")\n",
    "parser.add_argument(\"--mode\", type=str, default=\"GRAPH\", choices=[\"GRAPH\", \"PYNATIVE\"],\n",
    "                    help=\"Running in GRAPH_MODE OR PYNATIVE_MODE\")\n",
    "parser.add_argument(\"--device_target\", type=str, default=\"Ascend\", choices=[\"GPU\", \"Ascend\"],\n",
    "                    help=\"The target device to run, support 'Ascend', 'GPU'\")\n",
    "parser.add_argument(\"--device_id\", type=int, default=0,\n",
    "                    help=\"ID of the target device\")\n",
    "parser.add_argument(\"--config_file_path\", type=str,\n",
    "                    default=\"./configs/burgers.yaml\")\n",
    "input_args = parser.parse_args()\n",
    "\n",
    "context.set_context(mode=context.GRAPH_MODE if input_args.mode.upper().startswith(\"GRAPH\")\n",
    "                    else context.PYNATIVE_MODE,\n",
    "                    device_target=input_args.device_target,\n",
    "                    device_id=input_args.device_id)\n",
    "print(\n",
    "    f\"Running in {input_args.mode.upper()} mode, using device id: {input_args.device_id}.\")\n",
    "use_ascend = context.get_context(attr_key='device_target') == \"Ascend\"\n",
    "print(use_ascend)\n",
    "print(\"pid:\", os.getpid())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "确定要训练的方程的、进行yaml文件的加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get case name\n",
    "case_name = input_args.case\n",
    "\n",
    "# load configurations\n",
    "config = load_yaml_config(input_args.config_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 创建数据集\n",
    "\n",
    "与part1部分不同的是，该部分创建数据集时noise百分比为百分之0，其中database_choose、h_data_choose用于训练，database_validate, h_data_validate用于验证"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pinn_config = config[\"pinn\"]\n",
    "pinn_dataset_config = pinn_config[\"dataset\"]\n",
    "model_config = config[\"model\"]\n",
    "summary_config = config[\"summary\"]\n",
    "optimizer_config = config[\"optimizer\"]\n",
    "epochs = optimizer_config[\"epochs\"]\n",
    "\n",
    "# create dataset for training and validating\n",
    "pinn_dataset = create_pinn_dataset(\n",
    "    case_name, pinn_dataset_config)\n",
    "database_choose, h_data_choose, database_validate, h_data_validate = pinn_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加载模型\n",
    "\n",
    "在part1中使用了什么结构的模型，在该部分就要使用相应结构的模型、并加载已经保存好的模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultiScaleFCSequential(in_channels=model_config[\"in_channels\"],\n",
    "                               out_channels=model_config[\"out_channels\"],\n",
    "                               layers=model_config[\"layers\"],\n",
    "                               neurons=model_config[\"neurons\"],\n",
    "                               residual=model_config[\"residual\"],\n",
    "                               act=model_config[\"activation\"],\n",
    "                               num_scales=1)\n",
    "# load checkpoint\n",
    "ckpt_name = f\"{case_name}_nn-{epochs + 1}.ckpt\"\n",
    "ckpt_path = summary_config[\"save_checkpoint_epochs\"]\n",
    "model_dict = load_checkpoint(os.path.join(\n",
    "    ckpt_path, ckpt_name))\n",
    "load_param_into_net(model, model_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构建候选项\n",
    "\n",
    "从part1中获取到遗传算法的结果后得出方程候选项，并修改对应的Dict_name, Dict_n, Dict，并根据其计算需求修改src目录下的util.py中term计算的代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result from ga\n",
    "# need to be change by hand\n",
    "dict_name = get_dict_name(case_name)\n",
    "\n",
    "optimizer = nn.Adam(model.trainable_params(),\n",
    "                    optimizer_config[\"initial_lr\"])\n",
    "\n",
    "# set ascend\n",
    "if use_ascend:\n",
    "    loss_scaler = DynamicLossScaler(1024, 2, 100)\n",
    "    auto_mixed_precision(model, model_config[\"amp_level\"])\n",
    "else:\n",
    "    loss_scaler = None\n",
    "\n",
    "\n",
    "def forward_fn(dataset, lefts, coef_list, dict_name, terms_dict):\n",
    "    database_choose, h_data_choose = dataset\n",
    "    prediction = model(database_choose)\n",
    "    f1 = nn.MSELoss(reduction='mean')(prediction, h_data_choose)\n",
    "    loss = pinn_loss_func(f1, lefts, coef_list,\n",
    "                          dict_name, terms_dict)\n",
    "\n",
    "    if use_ascend:\n",
    "        loss = loss_scaler.scale(loss)\n",
    "    return loss\n",
    "\n",
    "\n",
    "grad_fn = value_and_grad(\n",
    "    forward_fn, None, optimizer.parameters, has_aux=False)\n",
    "\n",
    "\n",
    "@jit\n",
    "def train_step(dataset, lefts, coef_list, dict_name, terms_dict):\n",
    "    loss, grads = grad_fn(dataset, lefts, coef_list, dict_name, terms_dict)\n",
    "    if use_ascend:\n",
    "        loss = loss_scaler.unscale(loss)\n",
    "    loss = ops.depend(loss, optimizer(grads))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 开始训练\n",
    "\n",
    "定义trainer，开始训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...\n",
      "Theta:[-1.0167898e-10 -3.5906428e-01  3.8740728e-03  6.8805676e-08-2.0166181e-05 -6.6712929e-04]Dict_Name [1, 2, 3, 4, 5, 6]epoch: 100 train loss: 0.16274601 epoch time: 7609.529ms\n",
      "    predict total time: 2.933502197265625 ms\n",
      "    l2_error:  0.03319109569478303\n",
      "=================================================================================================\n",
      "Theta:[-6.1833136e-11 -3.9782286e-01  3.7144462e-03  4.6671396e-08-1.5555745e-05 -9.6872513e-04]Dict_Name [1, 2, 3, 4, 5, 6]epoch: 200 train loss: 0.12895139  epoch time: 8463.217ms\n",
      "    predict total time: 2.847433090209961 ms\n",
      "    l2_error:  0.3844950973176981\n",
      "=================================================================================================\n",
      "...\n",
      "Theta:[[-0.4369226 ][ 0.00145796][ 0.00145796]]Dict_Name [2, 3, 6]epoch: 2390 train loss: 0.08259561 epoch time: 14893.079ms\n",
      "    predict total time: 1.985311508178711 ms\n",
      "    l2_error:  0.0062716909767681125\n",
      "=================================================================================================\n",
      "Theta:[[-0.4367841 ][ 0.00145637][ 0.00145637]]Dict_Name [2, 3, 6]epoch: 2400 train loss: 0.08224905 epoch time: 15054.685ms\n",
      "    predict total time:  3.087759017944336 ms\n",
      "    l2_error:  0.006187851509927684\n",
      "=================================================================================================\n",
      "Theta:[[-0.43811008][ 0.00146007][ 0.00146007]]Dict_Name [2, 3, 6]epoch: 2410 train loss: 0.0810892 epoch time: 14595.442ms\n",
      "    predict total time: 3.632783889770508 ms\n",
      "    l2_error:  0.0062226032090417\n",
      "=================================================================================================\n",
      "End-to-End total time: 12074.045341385 s\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, epochs + 1):\n",
    "    time_beg = time.time()\n",
    "    prediction = model(database_choose)\n",
    "    grads, libraries = cal_grads(case_name, model, database_choose,\n",
    "                                 pinn_dataset_config[\"choose_train\"])\n",
    "\n",
    "    terms = cal_terms(case_name, prediction, grads)\n",
    "\n",
    "    # terms in numpy\n",
    "    terms_dict, dict_n = get_dicts(terms)\n",
    "\n",
    "    libraries = update_lib(case_name, dict_name, libraries, dict_n)\n",
    "\n",
    "    # Lasso\n",
    "    lefts = get_lefts(case_name, grads, prediction, False)\n",
    "    coef_list, lst_list = calculate_coef(lefts, libraries, epoch, config)\n",
    "\n",
    "    model.set_train(True)\n",
    "\n",
    "    # train step\n",
    "    lefts = get_lefts(case_name, grads, True)\n",
    "\n",
    "    step_train_loss = train_step((database_choose, h_data_choose),\n",
    "                                 lefts, coef_list, dict_name, terms_dict)\n",
    "    # set model to eval mode\n",
    "    model.set_train(False)\n",
    "\n",
    "    # put zeros\n",
    "    if epoch >= 1000:\n",
    "        for lst in lst_list:\n",
    "            for i in range(lst.shape[0]):\n",
    "                if np.abs(lst[i]) < pinn_config[\"kesi\"]:\n",
    "                    dict_name.pop(i)\n",
    "                    break\n",
    "\n",
    "    if epoch % summary_config[\"validate_interval_epochs\"] == 0:\n",
    "        print(\"Dict_Name %s\", dict_name)\n",
    "        # current epoch loss\n",
    "        print(\"epoch: %s train loss: %s epoch time: %.3fms\",\n",
    "              epoch, step_train_loss, (time.time() - time_beg) * 1000)\n",
    "        evaluate(model, database_validate, h_data_validate, config)\n",
    "\n",
    "    # save checkpoint\n",
    "    if epoch % summary_config[\"save_checkpoint_epochs\"] == 0:\n",
    "        ckpt_name = f\"{case_name}_pinn-{epoch + 1}.ckpt\"\n",
    "        save_checkpoint(model, os.path.join(\n",
    "            ckpt_path, ckpt_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![visual_burgers](./images/burgers.jpg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
