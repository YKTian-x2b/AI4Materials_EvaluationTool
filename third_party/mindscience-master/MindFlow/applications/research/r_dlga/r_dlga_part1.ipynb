{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# R_DLGA algorithm _part1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "The performance of data-driven partial differential equations lacks stability when dealing with complex situations such as sparse data with high noise, so the robust deep learning Genetic algorithm (R-DLGA) is proposed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Technical path\n",
    "\n",
    "The specific process of R-DLGA to solve this problem is as follows:\n",
    "\n",
    "1. Run train.py to train the neural network, generate metadata and compute derivatives and obtain potential terms through genetic algorithms\n",
    "2. Modify candidates in the train_pinn.py dict directory based on Step 1\n",
    "3. Run train_pinn.py to get the final result\n",
    "\n",
    "This part1 implements the functional code of the first step. After running the notebook, modify the term calculation under src/util.py and the dict in part2 according to the results of the genetic algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduce code packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import random\n",
    "import time\n",
    "\n",
    "from mindspore import nn, context, ops, set_seed\n",
    "from mindspore import value_and_grad, jit, data_sink, save_checkpoint\n",
    "from mindspore.amp import DynamicLossScaler, auto_mixed_precision\n",
    "\n",
    "from mindflow.utils import load_yaml_config\n",
    "from mindflow.cell import MultiScaleFCSequential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following `src` packages can be downloaded from [research/r_dlga/src] (https://gitee.com/mindspore/mindscience/tree/master/MindFlow/applications/research/r_dlga/src)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import create_dataset, evaluate, produce_meta_data\n",
    "from src import gene_algorithm\n",
    "\n",
    "set_seed(0)\n",
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters are configured, where --case has three choices, \"burgers\" means training on burgers equation, \"cylinder_flow\" means training on cylinder_flow dataset of navier_stokes2D equation, \"periodic_hill\" indicates training on a mountain flow dataset for the Reynolds mean Navier-Stokes equation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description=\"train r_glda\")\n",
    "parser.add_argument(\"--case\", type=str, default=\"burgers\", choices=[\"burgers\", \"cylinder_flow\", \"periodic_hill\"],\n",
    "                    help=\"choose burgers, cylinder_flow or periodic_hill\")\n",
    "parser.add_argument(\"--mode\", type=str, default=\"GRAPH\", choices=[\"GRAPH\", \"PYNATIVE\"],\n",
    "                    help=\"Running in GRAPH_MODE OR PYNATIVE_MODE\")\n",
    "parser.add_argument(\"--device_target\", type=str, default=\"Ascend\", choices=[\"GPU\", \"Ascend\"],\n",
    "                    help=\"The target device to run, support 'Ascend', 'GPU'\")\n",
    "parser.add_argument(\"--device_id\", type=int, default=0,\n",
    "                    help=\"ID of the target device\")\n",
    "parser.add_argument(\"--config_file_path\", type=str,\n",
    "                    default=\"./configs/burgers.yaml\")\n",
    "input_args = parser.parse_args()\n",
    "\n",
    "context.set_context(mode=context.GRAPH_MODE if input_args.mode.upper().startswith(\"GRAPH\")\n",
    "                    else context.PYNATIVE_MODE,\n",
    "                    device_target=input_args.device_target,\n",
    "                    device_id=input_args.device_id)\n",
    "print(\n",
    "    f\"Running in {input_args.mode.upper()} mode, using device id: {input_args.device_id}.\")\n",
    "use_ascend = context.get_context(attr_key='device_target') == \"Ascend\"\n",
    "print(use_ascend)\n",
    "print(\"pid:\", os.getpid())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determine the equation to be trained and load the yaml file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get case name\n",
    "case_name = input_args.case\n",
    "\n",
    "# load configurations\n",
    "config = load_yaml_config(input_args.config_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a dataset\n",
    "\n",
    "The following are three examples of datasets. During the course of using this code, you can download any dataset you want to train and place it in the src folder.\n",
    "\n",
    "train_dataset is used for training, inputs and label are used for authentication.\n",
    "\n",
    "### burgers dataset\n",
    "\n",
    "The burgers dataset is randomly sampled according to the solution domain, initial conditions and boundary value conditions, and the training dataset and test dataset are generated.\n",
    "\n",
    "Download： [physics_driven/burgers_pinns/dataset](https://download.mindspore.cn/mindscience/mindflow/dataset/applications/physics_driven/burgers_pinns/dataset/)\n",
    "\n",
    "### Cylinder_flow dataset\n",
    "\n",
    "cylinder_flow dataset is used to sample the initial conditions and boundary conditions data of a standard cylindrical flow with Reynolds number 100, respectively. For the training dataset, the problem domain and time dimension of the plane rectangle are constructed, and the known initial conditions and boundary conditions are sampled. The test set is constructed based on the points in the existing flow field.\n",
    "\n",
    "Download： [physics_driven/flow_past_cylinder/dataset](https://download.mindspore.cn/mindscience/mindflow/dataset/applications/physics_driven/flow_past_cylinder/dataset/)。\n",
    "\n",
    "### Periodic_hill Dataset\n",
    "\n",
    "The data format is npy of numpy and the dimension is [300,700, 10]. The first two dimensions are the length and width of the flow field respectively, and the last dimension contains (x, y, u, v, p, uu, uv, vv, rho, nu) a total of 10 variables. Where, x, y, u, v, p are the x coordinate, y coordinate, x direction velocity, y direction velocity, and pressure of the flow field respectively. uu, uv, vv Reynolds average statistics; rho is the fluid density and nu is the kinetic viscosity coefficient.\n",
    "\n",
    "Download： [dataset/periodic_hill_2d](https://download.mindspore.cn/mindscience/mindflow/dataset/periodic_hill_2d/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_config = config[\"dataset\"]\n",
    "model_config = config[\"model\"]\n",
    "optimizer_config = config[\"optimizer\"]\n",
    "epochs = optimizer_config[\"epochs\"]\n",
    "summary_config = config[\"summary\"]\n",
    "# create dataset for training and validating\n",
    "train_dataset, inputs, label = create_dataset(case_name, dataset_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the model\n",
    "\n",
    "The training part of the neural network in this example uses a simple fully connected network with a depth of 5 layers and an excitation function of `sin` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultiScaleFCSequential(in_channels=model_config[\"in_channels\"],\n",
    "                               out_channels=model_config[\"out_channels\"],\n",
    "                               layers=model_config[\"layers\"],\n",
    "                               neurons=model_config[\"neurons\"],\n",
    "                               residual=model_config[\"residual\"],\n",
    "                               act=model_config[\"activation\"],\n",
    "                               num_scales=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training\n",
    "\n",
    "Use **MindSpore >= 2.0.0** version, you can use functional programming paradigm to train neural networks.\n",
    "\n",
    "Data fit is used here to build the proxy model from a small amount of training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...\n",
      "epoch: 100 train loss: 0.24175334 epoch time: 518.875ms\n",
      "    predict total time: 195.06287574768066 ms\n",
      "    l2_error:  0.813042652321916\n",
      "=================================================================================================\n",
      "...\n",
      "epoch: 200 train loss: 0.05484033  epoch time: 288.337ms\n",
      "    predict total time: 2.847433090209961 ms\n",
      "    l2_error:  0.3844950973176981\n",
      "=================================================================================================\n",
      "...\n",
      "epoch: 29800 train loss: 0.00031119288 epoch time: 283.176ms\n",
      "    predict total time: 2.6092529296875 ms\n",
      "    l2_error:  0.03155383103803965\n",
      "=================================================================================================\n",
      "...\n",
      "epoch: 29900 train loss: 0.00034680235 epoch time: 246.574ms\n",
      "    predict total time:  1.9993782043457031 ms\n",
      "    l2_error:  0.031960893801602344\n",
      "=================================================================================================\n",
      "...\n",
      "epoch: 30000 train loss: 0.00030471644 epoch time: 255.045ms\n",
      "    predict total time: 1.7092227935791016 \n",
      "    l2_error:  0.03110467273470587\n",
      "=================================================================================================\n",
      "End-to-End total time: 6085.048846006393 s\n"
     ]
    }
   ],
   "source": [
    "optimizer = nn.Adam(model.trainable_params(),\n",
    "                    optimizer_config[\"initial_lr\"])\n",
    "\n",
    "# set ascend\n",
    "if use_ascend:\n",
    "    loss_scaler = DynamicLossScaler(1024, 2, 100)\n",
    "    auto_mixed_precision(model, model_config[\"amp_level\"])\n",
    "else:\n",
    "    loss_scaler = None\n",
    "\n",
    "save_ckpt_path = summary_config[\"save_checkpoint_epochs\"]\n",
    "\n",
    "# create ckpt dir\n",
    "if not os.path.exists(os.path.abspath(save_ckpt_path)):\n",
    "    os.makedirs(os.path.abspath(save_ckpt_path))\n",
    "\n",
    "# define forward function\n",
    "def forward_fn(data, label):\n",
    "    prediction = model(data)\n",
    "    loss = nn.MSELoss()(prediction, label)\n",
    "    if use_ascend:\n",
    "        loss = loss_scaler.scale(loss)\n",
    "    return loss\n",
    "\n",
    "# define gradient function\n",
    "grad_fn = value_and_grad(\n",
    "    forward_fn, None, optimizer.parameters, has_aux=False)\n",
    "\n",
    "# define train_step\n",
    "@jit\n",
    "def train_step(data, label):\n",
    "    loss, grads = grad_fn(data, label)\n",
    "    if use_ascend:\n",
    "        loss = loss_scaler.unscale(loss)\n",
    "    loss = ops.depend(loss, optimizer(grads))\n",
    "    return loss\n",
    "\n",
    "# data sink\n",
    "sink_process = data_sink(train_step, train_dataset, sink_size=1)\n",
    "steps_per_epochs = train_dataset.get_dataset_size()\n",
    "print(steps_per_epochs)\n",
    "print(train_dataset)\n",
    "\n",
    "print(\"----start training----\")\n",
    "# train loop for nn\n",
    "for epoch in range(1, epochs + 1):\n",
    "    time_beg = time.time()\n",
    "    model.set_train(True)\n",
    "    for _ in range(steps_per_epochs):\n",
    "        step_train_loss = sink_process()\n",
    "\n",
    "    # set model to eval mode\n",
    "    model.set_train(False)\n",
    "\n",
    "    if epoch % summary_config[\"validate_interval_epochs\"] == 0:\n",
    "        # current epoch loss\n",
    "        print(\n",
    "            f\"epoch: {epoch} train loss: {step_train_loss} epoch time: {(time.time() - time_beg) * 1000 :.3f}ms\")\n",
    "        evaluate(model, inputs, label, config)\n",
    "\n",
    "    # save checkpoint\n",
    "    if epoch % summary_config[\"save_checkpoint_epochs\"] == 0:\n",
    "        ckpt_name = f\"{case_name}_nn-{epoch + 1}.ckpt\"\n",
    "        save_checkpoint(model, os.path.join(\n",
    "            summary_config[\"save_checkpoint_epochs\"], ckpt_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate metadata\n",
    "\n",
    "The following metadata is generated under the current network according to the automatic differentiation mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load successfully\n",
      "Meta data produced successfully\n"
     ]
    }
   ],
   "source": [
    "# produce meta data\n",
    "produce_meta_data(case_name, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Genetic algorithm\n",
    "\n",
    "The next iteration of the genetic algorithm based on the metadata will output the right term of the prediction equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best one:  [[2], [0, 0], [2], [1]]The best coef:  [[ 1.29570466e-03][-2.43374822e-01][-5.22589630e-12][-3.62047983e-09]]The best MSE 0.46980834457278564left is:     [[0, 0], [0, 1], [0, 3], [0, 3]][0.46980834457278564, 0.6880833403821989, 0.7075666639554047, 0.7578930294986997, 0.7578930294986997, 0.7616924071536476, 0.7616924071536476, 0.7616924071536476, 0.765759259310983, 0.772105611239328, 0.7773331999849253, 0.7777517230742845, 0.7778460331634339, 0.7778913996376994, 0.7973053049490681, 0.8044220450718776, 0.8044642001995869, 0.8045025579988737, 0.8144802626401659, 0.8178883580454595, 0.8266673719127372, 0.8334237654374858, 0.8342321627765292, 0.837488567287119, 0.8442066546480582, 0.844267867848322, 0.8444561950449638, 0.8490651610921612, 0.85701691824813, 0.85701691824813, 0.85701691824813, 0.85701691824813, 0.85701691824813, 0.85701691824813, 0.85701691824813, 0.85701691824813, 0.85701691824813, 0.85701691824813, 0.85701691824813, 0.8572462772894673, 0.8572462772894673, 0.8572462772894673, 0.8572462772894673, 0.8572462772894673, 0.8572462772894673, 0.8572462772894673, 0.8572462772894673, 0.8572462772894673, 0.8572462772894673, 0.8573864841433405, 0.858647120628148, 0.858647120628148, 0.858647120628148, 0.858647120628148, 0.858647120628148, 0.858647120628148, 0.858647120628148, 0.858647120628148, 0.8586611493208939, 0.8586611493208939, 0.8586611493208939, 0.8586611493208939, 0.8586611493208939, 0.8586611493208939, 0.8586611493208939, 0.8586611493208939, 0.8586611493208939, 0.8586611493208939, 0.8586611493208939, 0.8586611493208939, 0.8586611493208939, 0.8586757975579109, 0.8586757975579109, 0.8586757975579109, 0.858767970909533, 0.858767970909533, 0.858767970909533, 0.858767970909533, 0.858767970909533, 0.858767970909533, 0.858767970909533, 0.8587859441864671, 0.8587859441864671, 0.8587859441864671, 0.8587859441864671, 0.8587859441864671, 0.8587859441864671, 0.8587859441864671, 0.8587859441864671, 0.8587859441864671, 0.8587859441864671, 0.8588008496047409, 0.8588029131356413, 0.8588029131356413, 0.8588029131356413, 0.8588029131356413, 0.8588029131356413, 0.8588029131356413, 0.8588029131356413, 0.8588107474715687, 0.8588107474715687, 0.8588107474715687, 0.8588107474715687, 0.8588107474715687, 0.8588107474715687, 0.8588352165261267, 0.8588353561808134, 0.8588353561808134, 0.8588353561808134, 0.8588353561808134, 0.8588353561808134, 0.8588353561808134, 0.8588356589317749, 0.8588356589317749, 0.8588356589317749, 0.8588356589317749, 0.8588356589317749, 0.8588356589317749, 0.8588356589317749, 0.8588356589317749, 0.8588356589317749, 0.8588356589317749, 0.8588356589317749, 0.8588356589317749, 0.8588356589317749, 0.8588356589317749, 0.8588357850084336, 0.8588357850084336, 0.8588357850084336, 0.8588357850084336, 0.8588357850084336, 0.8588357850084336, 0.8588357850084336, 0.8588357850084336, 0.8655826652362133, 0.8655871338540321, 0.8655871338540321, 0.8656027194124504, 0.8663200839659956, 0.8668431850438014, 0.8669818885242198, 0.8669931122201172, 0.8669931122201172, 0.8670165899751281, 0.8670166989759319, 0.867134344839717, 0.867134344839717, 0.8671802977393409, 0.8672089353320759, 0.8672211465560474, 0.8672458549744931, 0.8672458549744931, 0.8672458549744931, 0.8683729260078408, 0.8684454693002447, 0.8684729334630744, 0.8685436622775905, 0.8685436622775905, 0.8685436622775905, 0.8685583684420037, 0.8685583684420037, 0.8685583684420037, 0.8685932420264372, 0.8686277041524009, 0.8686277041524009, 0.8686356954337672, 0.8686466756725788, 0.8686469013559269, 0.8686469013559269, 0.8686470090051567, 0.8686470090051567, 0.8686610722886546, 0.8686738993701075, 0.868675086239328, 0.8686755495743819, 0.8686757043623482, 0.8686757043623482, 0.8686757043623482, 0.8687222415642999, 0.8687222415642999, 0.8687222415642999, 0.868752497704878, 0.868752497704878, 0.8687579370783721, 0.8687579370783721, 0.8687579370783721, 0.8687606854559351, 0.8687606854559351, 0.8687859441864672, 0.8687862980637027, 0.8687862980637027, 0.8687862980637027, 0.8687992686307083, 0.8688018174178694, 0.8688018174178694, 0.8688062887677095, 0.8688062887677095, 0.8688189068734582, 0.8688189068734582, 0.8688189068734582, 0.868819408822107, 0.868819408822107, 0.868819408822107, 0.868819408822107, 0.868819408822107, 0.8688223167090112, 0.8688223167090112, 0.8688223167090112, 0.8688223167090112, 0.8688256983969952, 0.8688256983969952, 0.8688256983969952, 0.8688256983969952, 0.8688314950983652, 0.8688325915924631, 0.8688325915924631, 0.8688345200596361, 0.8688350798148172, 0.8688350798148172, 0.8688354655828938, 0.8688354655828938, 0.8688354655828938, 0.8688356298127021, 0.8688356484186538, 0.8688356875898768, 0.8688356875898768, 0.8688356875898768, 0.8688356875898768, 0.8688356875898768, 0.8688358593416531, 0.8688358593416531, 0.8688358665515187, 0.8688358665515187, 0.8688358665515187, 0.868835874540708, 0.8688358768032148, 0.8688358768032148, 0.8741397489464056, 0.8742953738818185, 0.8766802562135716, 0.8768437597413684, 0.8768915678818876, 0.8768915678818876, 0.876917340152207, 0.876917340152207, 0.8771501655468429, 0.8771945514053833, 0.8772455798744646, 0.8772459459490907, 0.8785227849696272, 0.8785509550988885, 0.8785771791007404, 0.8786303781560759, 0.8786311128453247, 0.8786366116881565, 0.878660766697423, 0.8786682821189385, 0.8786683415596199, 0.8786730892941353, 0.8786736183286297, 0.878673894632055, 0.8786746757085881, 0.8787401763901967, 0.8787514624242463, 0.8787528468081035, 0.8787574308117513, 0.8787593984423, 0.8787717210705105, 0.8787823045310134, 0.8787855322554153, 0.8787858495555774, 0.8787858495555774, 0.8787858540714556, 0.8787858540714556, 0.8787959581934283, 0.8787984495868041, 0.8788021593276605, 0.8788025378860962, 0.8788096460592698, 0.8788096460592698, 0.8788096460592698, 0.8788096460592698, 0.8788151842712857, 0.8788151842712857, 0.8788166579623953, 0.8788246704433039, 0.8788256010241108, 0.8788256983969952, 0.8788268730306489, 0.8788271723979968, 0.8788288097298069, 0.8788288097298069, 0.8788295823183984, 0.8788297453511951, 0.8788297765542112, 0.8788301319645663, 0.8788301319645663, 0.8788307590520305, 0.87883092219831, 0.8788323716711612, 0.8788329964120769, 0.8788330409248005, 0.8788330409248005, 0.8788332749278543, 0.8788337708479879, 0.8788337708479879, 0.8788338694125932, 0.8788347028070849, 0.8788348782209803, 0.8788350194520009, 0.8788352018834994, 0.8788352263782375, 0.8788352905759952, 0.8788352905759952, 0.8788353333329625, 0.8788353504055992, 0.8788353504055992, 0.8788353504055992, 0.8788355605313654, 0.8788356327473822, 0.878835717901455, 0.878835717901455, 0.878835717901455, 0.8788358268291131, 0.878835838545208, 0.878835838545208, 0.878835838545208, 0.8788358520188378, 0.8852492870968267, 0.8855826639014519, 0.885766182077388, 0.8867426193977956, 0.8869284026726274, 0.8870033415081403, 0.8870064976596731, 0.8870156318578722, 0.887030379876908, 0.8871852392988798, 0.8872311332768459, 0.8872451958423493, 0.8883384612119264, 0.8884097426920318, 0.8884324636715181, 0.8885409435443564, 0.8885813909830348, 0.8886167614386938, 0.888643757064351, 0.8886454903084535, 0.8886523386109899, 0.8886567755433017, 0.8886580920040648, 0.8886580920040648, 0.8886825028570653, 0.8887512879851075, 0.8887717531848243, 0.8887858770938352, 0.8887861074495834, 0.8887861074495834, 0.8887903009690415, 0.8887903009690415, 0.8887918073179295, 0.8887971394811542, 0.8888018075258741, 0.8888031486165775, 0.8888058518790876, 0.8888059282743661, 0.8888061670147845, 0.8888061670147845, 0.8888113023117205, 0.8888119571016107, 0.8888128299254209, 0.8888187898331196, 0.8888266429576093, 0.8888294881647375, 0.8888297453511951, 0.888830056742524, 0.8888312327682681, 0.8888312327682681, 0.8888328999515446, 0.8888329846433288, 0.8888330409248005, 0.8888334324637382, 0.8888334324637382, 0.8888337383264587, 0.8888338742584134, 0.8888339983376917, 0.8888348128541917, 0.8888348811789315, 0.8888349712039522, 0.8888350117553583, 0.8888350901446178, 0.8888355528758412, 0.888835582144898, 0.888835679329919, 0.888835679329919, 0.888835679329919, 0.8888357263021183, 0.888835838545208, 0.8888358665515187, 0.8942529898352117]-----------------------------...\n",
      "The best one:  [[0, 2], [0, 0], [1], [1], [0, 1], [0]]The best coef:  [[-4.17574314e-10][-4.47832843e-01][-2.10390744e-01][ 4.20791641e-07][-2.77357888e-05][ 2.13367508e-01]]The best MSE 0.18538161139860898left is:     [[0, 3], [0, 1], [0, 1], [0, 3], [0, 2], [0, 2]][0.18538161139860898, 0.18884818785804564, 0.18884818785827118, 0.18946420763075006, 0.195800188067421, 0.19660996127779015, 0.19946079757561197, 0.199460797575612, 0.199460797575612, 0.20254702594665236, 0.2037008235533585, 0.2037123546739095, 0.20467435760113756, 0.20470247470884798, 0.20482169163814135, 0.204850314228757, 0.20539829804577114, 0.20575883662851896, 0.20582172353473532, 0.20609943676694042, 0.2076029766123138, 0.20839964009638307, 0.20914489474872644, 0.2101359498329033, 0.21033932412482773, 0.21125257947290735, 0.21163680276222138, 0.2118511682745236, 0.21208418095542048, 0.21247664998158067, 0.21254221306481502, 0.21285446771404543, 0.21285446771404543, 0.21292214405706367, 0.21299531175120445, 0.21363926571009645, 0.21378983164782955, 0.21449049817733423, 0.2148207837408802, 0.21487421261818063, 0.21487421261818065, 0.21487421261818065, 0.21487421261818068, 0.21490850879276246, 0.2151620238469464, 0.21576983096169888, 0.2158739099643755, 0.21612861191606225, 0.21629504042311007, 0.21649748136685049, 0.2170862657566576, 0.21848771340678821, 0.21860497147824645, 0.21864978577379568, 0.21931850783731754, 0.2198689343496461, 0.22012604520246176, 0.22078909039750627, 0.22082162242617104, 0.22102129173066176, 0.22118203292566907, 0.22167242496482745, 0.22167242496482745, 0.22179576370362858, 0.22223958018437912, 0.22225250330158755, 0.2223895254664147, 0.22242137338816909, 0.22243255174518456, 0.2225331039561425, 0.22294639233109179, 0.22294639233109179, 0.22321419917523733, 0.22374230843567094, 0.22407115582378212, 0.22407115582378212, 0.22408392426310175, 0.22408392426310175, 0.2242192132875349, 0.22441464977359793, 0.2245005552380264, 0.22475600504267715, 0.22480033213826423, 0.2248692220341939, 0.22581859098774065, 0.22581963932430882, 0.22599977330790794, 0.22617157668763427, 0.22620984237585334, 0.22633662225888998, 0.2268268577373667, 0.22711266714133424, 0.227146828115694, 0.2272919135263456, 0.2275967622958727, 0.22759676229587272, 0.22810228177361522, 0.22815612691005593, 0.22844448971351455, 0.2286007447053759, 0.2287055220466508, 0.2289590443288309, 0.2289590443288309, 0.22909036405757932, 0.2290994435394673, 0.2290994435394673, 0.22912790968254282, 0.22943831325926098, 0.22976959420721105, 0.22990452792205118, 0.23064870263354853, 0.23081148674134508, 0.23081148674134513, 0.23083317889042482, 0.2313019936572045, 0.2313216357335848, 0.23144728099432604, 0.23155691800872596, 0.23156160951616056, 0.231661712381105, 0.23245908629620443, 0.23256982061004705, 0.23264926024703708, 0.2326840097817378, 0.2326840097817378, 0.23288119965391443, 0.23312519843111373, 0.23318888714385327, 0.23345815139656795, 0.23360492624955054, 0.23370678949405482, 0.23372129221012533, 0.23403765509162228, 0.2340376550916226, 0.2340376550916226, 0.2341298697413238, 0.23427550990586998, 0.2342842803298121, 0.23439810029490127, 0.2345568863976361, 0.23483998851785326, 0.23486246998793753, 0.23522368428190626, 0.23526176069759736, 0.23532198689366657, 0.23552109582800657, 0.23563677219309104, 0.23579298229951728, 0.23587332582161902, 0.23598525508424692, 0.2360412800827258, 0.2361721595798466, 0.23633509884875437, 0.23635734735144714, 0.23649038646620257, 0.23649038646620257, 0.2365223435619578, 0.23664174285212855, 0.23680304505214037, 0.23708542454178916, 0.23724372079242612, 0.23748294308221396, 0.23789287170884527, 0.23793190197127959, 0.2384625524494482, 0.23848414838401633, 0.23883604394297475, 0.239077198732817, 0.23920463914506124, 0.2392067796055013, 0.23946853535800838, 0.2395463848603357, 0.23955269176837313, 0.2395830504287068, 0.2395830504287068, 0.2398436826300827, 0.24013177412731745, 0.24034286542209643, 0.24049922948249172, 0.2405762269020451, 0.24058489098803434, 0.24091817281054775, 0.2412477659016219, 0.24131790905973738, 0.24158016876311406, 0.2418225926358358, 0.24186792273967805, 0.24188684534077862, 0.24199462044208253, 0.2421071020304997, 0.2421349766143403, 0.24217786824499593, 0.2421968318765559, 0.24223559419088198, 0.2422797115559504, 0.24232485220611155, 0.24236483401658887, 0.24270702868162902, 0.24270909528209483, 0.242866398374324, 0.24302340389151691, 0.24330618667967036, 0.24347329490407973, 0.24355084666900637, 0.24355084666900642, 0.24355650533859008, 0.24360451528000887, 0.2436361526491418, 0.24367520220935845, 0.24369979687125037, 0.24424700781678693, 0.24425563692269703, 0.24425563692269703, 0.24429688136477584, 0.24438357347191098, 0.2445326417615884, 0.2445634183232511, 0.2445634183232511, 0.24482794408858766, 0.24497744850396821, 0.24507451865889554, 0.24515748700753248, 0.24527307806491894, 0.24545740269734762, 0.2454628886928624, 0.24552248463741203, 0.24553764529675773, 0.24558613056145154, 0.2456989288751607, 0.2458396727950194, 0.24595068052898816, 0.246141947172027, 0.24616651715607127, 0.2462508754737476, 0.2463294928150051, 0.24654224410748116, 0.24654224410748116, 0.2465422441074812, 0.2465422441074812, 0.2470183164786239, 0.2470394559649832, 0.24723745025495858, 0.2472672206233509, 0.24761902044337192, 0.24769088690744148, 0.24778180778659054, 0.24799713560146877, 0.24825065029535237, 0.24827398026295472, 0.24848368091560472, 0.24848529330065972, 0.24858007891781925, 0.24858007891781955, 0.24858007891781983, 0.24862050221426124, 0.24865860295893585, 0.24992600854604358, 0.25005018336685914, 0.2501091711011693, 0.2501588968756398, 0.2501588968756398, 0.25035046080092177, 0.2503672176573072, 0.2505096822942625, 0.2507180724893219, 0.2507273291923333, 0.2508060796614524, 0.2510214589626187, 0.2512631307576136, 0.25136741512450045, 0.2518483197783328, 0.2519705624991423, 0.2521855158273696, 0.2522087302614206, 0.2524695380547892, 0.25252094705004885, 0.2526660393177787, 0.2526984688184463, 0.25301562798773836, 0.25319033731022395, 0.2535091021964393, 0.25351964525293685, 0.25364400744134236, 0.25367668939001203, 0.2536766893900121, 0.2539587459167074, 0.2541205049526041, 0.2544971567701142, 0.25466278811035925, 0.25467674217879654, 0.254956192839217, 0.2550720373265972, 0.2551176473063827, 0.255213493120804, 0.25546125787864415, 0.25553124531632104, 0.2556410580844662, 0.25567659455642033, 0.2558559354493562, 0.2563936201981173, 0.25662407500141293, 0.2567505873226208, 0.25703935456889027, 0.25703935456889027, 0.257242598150235, 0.25735937318791535, 0.25758608980124087, 0.25764724247464854, 0.257864409668201, 0.25791655343265885, 0.25792436808724595, 0.25847384887561065, 0.25847384887561065, 0.25859846198498093, 0.25864649771941906, 0.25867653185092904, 0.25907717981180933, 0.2590996471457174, 0.2590996471457174, 0.2590996471457174, 0.2591669299157906, 0.2593284077944942, 0.25970176213919116, 0.2598607021424588, 0.2605175844059231, 0.2610428540025688, 0.2610776276068698, 0.261301746678651, 0.2617208996152584, 0.26189203131882455, 0.2622454535588496, 0.26241839597978805, 0.2624231268767725, 0.2625256758584535, 0.2627231178943697, 0.2629389108205865, 0.2629643505986237, 0.26328513673706583, 0.26350498364851893, 0.2636897043215641, 0.26373868976747444, 0.26410487572244057, 0.2642494545762112, 0.26450456762462626, 0.2646499892457984, 0.26476246984259516, 0.26487808540940955, 0.2652730267926925, 0.2652730267926925, 0.2652730267926925, 0.2653020798939005, 0.26549707191514477, 0.2659171130494615, 0.26595308104903626, 0.2659853565000428, 0.2659853565000428, 0.26608316153548783, 0.26643369869340905, 0.26656544917517266, 0.2666732294258279, 0.26692878368374295, 0.2669362753944404, 0.2672658777702748, 0.2674727798255206, 0.2676548317922668, 0.2680206913957739, 0.2680206913957739, 0.26854222251881754, 0.26876249130717944, 0.26935794055452433, 0.27006902209729633, 0.2702074827404496, 0.2704243808415874, 0.27042438084158743, 0.27051757097074325, 0.27068338156581034, 0.2712712433470865, 0.27144347530751445, 0.27144347530751445, 0.271633217498353, 0.271633217498353, 0.271907287833235, 0.2719449608245337, 0.2721230400427858, 0.2727685607080027, 0.2727685607080027, 0.27284030272541837, 0.27437214000863364, 0.27439340290285763, 0.27457452274220207, 0.2749756970186396, 0.275398527835158, 0.275398527835158, 0.2757847835953725, 0.2758020673041009, 0.27618669834287096, 0.2764416582360582, 0.2767561679387055, 0.2767561679387055, 0.27714271850327155]"
     ]
    }
   ],
   "source": [
    "gene_algorithm(case_name, config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
