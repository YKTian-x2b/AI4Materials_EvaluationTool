{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# meta_pinns算法\n",
                "\n",
                "## 环境安装\n",
                "\n",
                "本案例要求 **MindSpore >= 2.0.0** 版本以调用如下接口: *mindspore.jit, mindspore.jit_class, mindspore.data_sink*。具体请查看[MindSpore安装](https://www.mindspore.cn/install)。\n",
                "\n",
                "此外，你需要安装 **MindFlow >=0.1.0** 版本。如果当前环境还没有安装，请按照下列方式选择后端和版本进行安装。"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "mindflow_version = \"0.1.0\"  # update if needed\n",
                "# GPU Comment out the following code if you are using NPU.\n",
                "!pip uninstall -y mindflow-gpu\n",
                "!pip install mindflow-gpu==$mindflow_version\n",
                "\n",
                "# NPU Uncomment if needed.\n",
                "# !pip uninstall -y mindflow-ascend\n",
                "# !pip install mindflow-ascend==$mindflow_version"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 概述\n",
                "\n",
                "在不同的物理应用场景下，选取合适的PINNs 损失函数目前仍主要依赖于经验和手工设计。为了解决上述问题，Apostolos F Psarosa等人提出了Meta-PINNs 算法，该算法通过在训练中以梯度下降的方式更新作用于损失函数的超参，从而训练出一组适用于一类同族偏微分方程的超参数。\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 技术路径\n",
                "\n",
                "求解该问题的具体流程如下：\n",
                "\n",
                "1. 创建数据集\n",
                "2. 定义模型和优化器\n",
                "3. 定义内循环（inner loop）和外循环（outer loop）的前向传播函数和梯度函数\n",
                "4. 定义内循环和外循环的训练步骤\n",
                "5. 设置训练参数，如学习率、迭代次数\n",
                "6. 进行训练，并在每个外循环间隔上进行模型评估"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 引入代码包"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import argparse\n",
                "import os\n",
                "import time\n",
                "\n",
                "import numpy as np\n",
                "\n",
                "from mindspore import context, nn, get_seed, set_seed, data_sink\n",
                "\n",
                "from mindflow.cell import MultiScaleFCSequential\n",
                "from mindflow.utils import load_yaml_config"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "下述`src`包可以在[research/meta_pinns/src](https://gitee.com/mindspore/mindscience/tree/master/MindFlow/applications/research/meta_pinns/src)下载。"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from src import create_train_dataset, create_problem, create_trainer, create_normal_params\n",
                "from src import re_initialize_model, evaluate, plot_l2_error, plot_l2_comparison_error\n",
                "from src import WorkspaceConfig, TrainerInfo\n",
                "\n",
                "set_seed(0)\n",
                "np.random.seed(0)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "进行参数配置，其中--case可选\"burgers\", \"l_burgers\", \"cylinder_flow\", \"periodic_hill\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "parser = argparse.ArgumentParser(description=\"meta-pinns\")\n",
                "parser.add_argument(\"--case\", type=str, default=\"burgers\", choices=[\"burgers\", \"l_burgers\", \"periodic_hill\", \"cylinder_flow\"],\n",
                "                    help=\"choose burgers\")\n",
                "parser.add_argument(\"--mode\", type=str, default=\"GRAPH\", choices=[\"GRAPH\", \"PYNATIVE\"],\n",
                "                    help=\"Running in GRAPH_MODE OR PYNATIVE_MODE\")\n",
                "parser.add_argument(\"--device_target\", type=str, default=\"Ascend\", choices=[\"GPU\", \"Ascend\"],\n",
                "                    help=\"The target device to run, support 'Ascend', 'GPU'\")\n",
                "parser.add_argument(\"--device_id\", type=int, default=0,\n",
                "                    help=\"ID of the target device\")\n",
                "parser.add_argument(\"--config_file_path\", type=str,\n",
                "                    default=\"./configs/burgers.yaml\")\n",
                "input_args = parser.parse_args()\n",
                "\n",
                "context.set_context(mode=context.GRAPH_MODE if input_args.mode.upper().startswith(\"GRAPH\")\n",
                "                    else context.PYNATIVE_MODE,\n",
                "                    save_graphs=input_args.save_graphs,\n",
                "                    save_graphs_path=input_args.save_graphs_path,\n",
                "                    device_target=input_args.device_target,\n",
                "                    device_id=input_args.device_id)\n",
                "print(\n",
                "    f\"Running in {input_args.mode.upper()} mode, using device id: {input_args.device_id}.\")\n",
                "use_ascend = context.get_context(attr_key='device_target') == \"Ascend\"\n",
                "print(use_ascend)\n",
                "print(\"pid:\", os.getpid())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "确定要训练的方程的、进行yaml文件的加载"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# load configurations\n",
                "case_name = input_args.case\n",
                "config = load_yaml_config(input_args.config_file_path)\n",
                "model_config = config[\"model\"]\n",
                "test_config = config[\"meta_test\"]\n",
                "summary_config = config[\"summary\"]\n",
                "lamda_config = config[\"lamda\"]\n",
                "meta_train_config = config[\"meta_train\"]\n",
                "initial_lr = config[\"optimizer\"][\"initial_lr\"]"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 创建数据集\n",
                "\n",
                "在案例训练中未使用方程真实值，仅对方程内部点、边界点、初始点进行采样即可。\n",
                "\n",
                "inner_train_dataset用于内部循环训练，outer_train_dataset用于外部循环训练。"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# create dataset\n",
                "inner_train_dataset = create_train_dataset(\n",
                "    case_name, config, get_seed() + 1)\n",
                "outer_train_dataset = create_train_dataset(\n",
                "    case_name, config, get_seed() + 2)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 构建模型\n",
                "\n",
                "本例使用简单的全连接网络，网络形状由yaml文件决定。"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# define models and optimizers\n",
                "model = MultiScaleFCSequential(in_channels=config[\"model\"][\"in_channels\"],\n",
                "                               out_channels=config[\"model\"][\"out_channels\"],\n",
                "                               layers=config[\"model\"][\"layers\"],\n",
                "                               neurons=config[\"model\"][\"neurons\"],\n",
                "                               residual=config[\"model\"][\"residual\"],\n",
                "                               act=config[\"model\"][\"activation\"],\n",
                "                               num_scales=1)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 模型训练\n",
                "\n",
                "使用**MindSpore >= 2.0.0**的版本，可以使用函数式编程范式训练神经网络。\n",
                "\n",
                "此处采用元学习算法学习PINN训练过程中损失函数中项的权重。\n",
                "\n",
                "并在每个外循环间隔上在未见过的方程上进行模型评估。"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "epoch: 1 loss: [0.86779684] epoch time: 23640.816ms\n",
                        "epoch: 2 loss: [0.9413994] epoch time: 1189.008ms\n",
                        "epoch: 3 loss: [0.9189246] epoch time: 1087.861ms\n",
                        "epoch: 4 loss: [0.9275309] epoch time: 1123.993ms\n",
                        "...\n",
                        "epoch: 50 l2_error: 1.0040625699565209 epoch time: 8788.293ms\n",
                        "...\n",
                        "epoch: 1000 loss: [0.9453384] epoch time: 1885.815ms\n",
                        "learned params are: pde weight=[1.865994] ic weight=[1.8658787] bc_weight=[1.8648634]\n",
                        "...\n",
                        "-------------------------------------start meta testing-------------------------------------\n",
                        "epoch: 0 l2_error: 1.0029983755797225 epoch time: 12976.881ms\n",
                        "epoch: 100 l2_error: 0.8387432039920448 epoch time: 111.595ms\n",
                        "epoch: 200 l2_error: 0.5762822379368656 epoch time: 111.311ms\n",
                        "...\n",
                        "epoch: 19800 l2_error: 0.033477298861820166 epoch time: 76.523ms\n",
                        "epoch: 19900 l2_error: 0.027350145606692883 epoch time: 79.059ms\n",
                        "-------------------------------------end meta testing-------------------------------------\n",
                        "-------------------------------------start normal training-------------------------------------\n",
                        "epoch: 0 l2_error: 1.0000106844369898 epoch time: 17964.145ms\n",
                        "epoch: 100 l2_error: 0.883998340190983 epoch time: 163.618ms\n",
                        "epoch: 200 l2_error: 0.559024364575979 epoch time: 113.474ms\n",
                        "...\n",
                        "epoch: 19800 l2_error: 0.004723342759751621 epoch time: 93.854ms\n",
                        "epoch: 19900 l2_error: 0.004713620036952725 epoch time: 100.113ms\n",
                        "-------------------------------------end normal training-------------------------------------\n"
                    ]
                }
            ],
            "source": [
                "lamda = lamda_config[\"initial_lamda\"]\n",
                "problem = create_problem(lamda, case_name, model, config)\n",
                "inner_optimizer = nn.SGD(model.trainable_params(),\n",
                "                         initial_lr)\n",
                "outer_optimizer = nn.Adam(problem.get_params(),\n",
                "                          initial_lr)\n",
                "\n",
                "if use_ascend:\n",
                "    from mindspore.amp import DynamicLossScaler, auto_mixed_precision\n",
                "    loss_scaler = DynamicLossScaler(1024, 2, 100)\n",
                "    auto_mixed_precision(model, 'O1')\n",
                "else:\n",
                "    loss_scaler = None\n",
                "\n",
                "inner_trainer = create_trainer(TrainerInfo(case_name, model, inner_optimizer, problem,\n",
                "                                           use_ascend, loss_scaler, config, False, True))\n",
                "outer_trainer = create_trainer(TrainerInfo(case_name, model, outer_optimizer,\n",
                "                                           problem, use_ascend, loss_scaler, config, True, True))\n",
                "\n",
                "inner_train_step = inner_trainer.train_step\n",
                "outer_train_step = outer_trainer.train_step\n",
                "\n",
                "iteration_str = \"iterations\"\n",
                "inner_iters = meta_train_config[\"inner_loop\"][iteration_str]\n",
                "outer_iters = meta_train_config[\"outer_loop\"][iteration_str]\n",
                "\n",
                "steps_per_epochs = inner_train_dataset.get_dataset_size()\n",
                "inner_sink_process = data_sink(\n",
                "    inner_train_step, inner_train_dataset, sink_size=1)\n",
                "outer_sink_process = data_sink(\n",
                "    outer_train_step, outer_train_dataset, sink_size=1)\n",
                "\n",
                "lamda_min = lamda_config[\"lamda_min\"]\n",
                "lamda_max = lamda_config[\"lamda_max\"]\n",
                "\n",
                "used_lamda = [lamda_config[\"eva_lamda\"]]\n",
                "best_params = problem.get_params()\n",
                "best_l2 = 1e10\n",
                "\n",
                "# starting meta training\n",
                "eva_l2_errors = []\n",
                "for epoch in range(1, 1 + outer_iters):\n",
                "    # train\n",
                "    lamda = np.random.uniform(lamda_min, lamda_max)\n",
                "    if lamda not in used_lamda:\n",
                "        used_lamda.append(lamda)\n",
                "    time_beg = time.time()\n",
                "    model.set_train(True)\n",
                "\n",
                "    if epoch % meta_train_config[\"reinit_lamda\"] == 0:\n",
                "        problem.lamda = lamda\n",
                "    if epoch % meta_train_config[\"reinit_epoch\"] == 0:\n",
                "        re_initialize_model(model, epoch)\n",
                "\n",
                "    for _ in range(1, 1 + inner_iters):\n",
                "        for _ in range(steps_per_epochs):\n",
                "            inner_sink_process()\n",
                "    for _ in range(steps_per_epochs):\n",
                "        cur_loss = outer_sink_process()\n",
                "\n",
                "    print(\"epoch: %s loss: %s epoch time: %.3fms\",\n",
                "          epoch, cur_loss, (time.time() - time_beg) * 1000)\n",
                "\n",
                "    if epoch % meta_train_config[\"eva_interval_outer\"] == 0:\n",
                "        # evaluate current model on unseen lamda\n",
                "        print(\"learned params are: %s\",\n",
                "              problem.get_params(if_value=True))\n",
                "        eva_iter = meta_train_config[\"eva_loop\"][iteration_str]\n",
                "        eva_l2_error = evaluate(WorkspaceConfig(epoch, case_name, config, problem.get_params(), eva_iter,\n",
                "                                                eva_iter, use_ascend, loss_scaler, False, True, False, None))\n",
                "        eva_l2_errors.append(eva_l2_error[0])\n",
                "        if eva_l2_error[0] < best_l2:\n",
                "            best_l2 = eva_l2_error[0]\n",
                "            best_params = problem.get_params()\n",
                "\n",
                "print(best_l2)\n",
                "for param in best_params:\n",
                "    print(param.asnumpy())\n",
                "\n",
                "plot_l2_error(case_name, summary_config[\"visual_dir\"],\n",
                "              meta_train_config[\"eva_interval_outer\"], eva_l2_errors)\n",
                "\n",
                "# start comparing\n",
                "test_iter = test_config[iteration_str]\n",
                "test_interval = test_config[\"cal_l2_interval\"]\n",
                "\n",
                "# start meta training\n",
                "meta_l2_errors = evaluate(WorkspaceConfig(None, case_name, config, best_params,\n",
                "                                          test_iter, test_interval,\n",
                "                                          use_ascend, loss_scaler, False, True,\n",
                "                                          True, f\"{case_name}_meta_testing\"))\n",
                "# end meta training\n",
                "\n",
                "# start normal training\n",
                "normal_params = create_normal_params(case_name)\n",
                "\n",
                "normal_l2_errors = evaluate(WorkspaceConfig(None, case_name, config, normal_params,\n",
                "                                            test_iter, test_interval,\n",
                "                                            use_ascend, loss_scaler, False, False,\n",
                "                                            True, f\"{case_name}_normal_training\"))\n",
                "# end normal training\n",
                "\n",
                "plot_l2_comparison_error(\n",
                "    case_name, summary_config[\"visual_dir\"], test_interval, meta_l2_errors, normal_l2_errors)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "![burgers_l2](./images/burgers_l2.png)"
            ]
        }
    ],
    "metadata": {
        "language_info": {
            "name": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
