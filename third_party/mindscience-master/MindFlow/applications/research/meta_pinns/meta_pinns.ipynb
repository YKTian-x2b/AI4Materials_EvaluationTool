{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Meta_PINNs algorithm\n",
                "\n",
                "## Environment Setup\n",
                "\n",
                "This notebook requires **MindSpore version >= 2.0.0** to support new APIs including: *mindspore.jit, mindspore.jit_class, mindspore.data_sink*. Please check [MindSpore Installation](https://www.mindspore.cn/install/en) for details.\n",
                "\n",
                "In addition, **MindFlow version >=0.1.0** is also required. If it has not been installed in your environment, please select the right version and hardware, then install it as follows."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "mindflow_version = \"0.1.0\"  # update if needed\n",
                "# GPU Comment out the following code if you are using NPU.\n",
                "!pip uninstall -y mindflow-gpu\n",
                "!pip install mindflow-gpu==$mindflow_version\n",
                "\n",
                "# NPU Uncomment if needed.\n",
                "# !pip uninstall -y mindflow-ascend\n",
                "# !pip install mindflow-ascend==$mindflow_version"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Overview\n",
                "\n",
                "In different physical application scenarios, the selection of appropriate PINNs loss functions still relies mainly on experience and manual design. In order to address the above issues, Apostolos F. Psarosa and others proposed the Meta-PINNs algorithm, which updates hyperparameters acting on the loss function during training through gradient descent, thus training a set of hyperparameters applicable to a class of related partial differential equations.\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Technical Path\n",
                "\n",
                "The specific process for solving this problem is as follows:\n",
                "\n",
                "1. Create the dataset.\n",
                "2. Define the model and optimizer.\n",
                "3. Define the forward propagation function and gradient function for the inner loop and outer loop.\n",
                "4. Define the training steps for the inner loop and outer loop.\n",
                "5. Set training parameters, such as learning rate and number of iterations.\n",
                "6. Conduct training and evaluate the model at intervals between each outer loop."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Importing Code Packages"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import argparse\n",
                "import os\n",
                "import time\n",
                "\n",
                "import numpy as np\n",
                "\n",
                "from mindspore import context, nn, get_seed, set_seed, data_sink\n",
                "\n",
                "from mindflow.cell import MultiScaleFCSequential\n",
                "from mindflow.utils import load_yaml_config"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The following `src` package can be downloaded at [research/meta_pinns/src](https://gitee.com/mindspore/mindscience/tree/master/MindFlow/applications/research/meta_pinns/src)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from src import create_train_dataset, create_problem, create_trainer, create_normal_params\n",
                "from src import re_initialize_model, evaluate, plot_l2_error, plot_l2_comparison_error\n",
                "from src import WorkspaceConfig, TrainerInfo\n",
                "\n",
                "set_seed(0)\n",
                "np.random.seed(0)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Configure the parameters, where --case is optional and can be set to \"burgers\", \"l_burgers\", \"periodic_hill\" or \"cylinder_flow\"."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "parser = argparse.ArgumentParser(description=\"meta-pinns\")\n",
                "parser.add_argument(\"--case\", type=str, default=\"burgers\", choices=[\"burgers\", \"l_burgers\", \"cylinder_flow\", \"periodic_hill\"],\n",
                "                    help=\"choose burgers\")\n",
                "parser.add_argument(\"--mode\", type=str, default=\"GRAPH\", choices=[\"GRAPH\", \"PYNATIVE\"],\n",
                "                    help=\"Running in GRAPH_MODE OR PYNATIVE_MODE\")\n",
                "parser.add_argument(\"--device_target\", type=str, default=\"Ascend\", choices=[\"GPU\", \"Ascend\"],\n",
                "                    help=\"The target device to run, support 'Ascend', 'GPU'\")\n",
                "parser.add_argument(\"--device_id\", type=int, default=0,\n",
                "                    help=\"ID of the target device\")\n",
                "parser.add_argument(\"--config_file_path\", type=str,\n",
                "                    default=\"./configs/burgers.yaml\")\n",
                "input_args = parser.parse_args()\n",
                "\n",
                "context.set_context(mode=context.GRAPH_MODE if input_args.mode.upper().startswith(\"GRAPH\")\n",
                "                    else context.PYNATIVE_MODE,\n",
                "                    save_graphs=input_args.save_graphs,\n",
                "                    save_graphs_path=input_args.save_graphs_path,\n",
                "                    device_target=input_args.device_target,\n",
                "                    device_id=input_args.device_id)\n",
                "print(\n",
                "    f\"Running in {input_args.mode.upper()} mode, using device id: {input_args.device_id}.\")\n",
                "use_ascend = context.get_context(attr_key='device_target') == \"Ascend\"\n",
                "print(use_ascend)\n",
                "print(\"pid:\", os.getpid())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Determine the equation to be trained and load the YAML file."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# load configurations\n",
                "case_name = input_args.case\n",
                "config = load_yaml_config(input_args.config_file_path)\n",
                "model_config = config[\"model\"]\n",
                "test_config = config[\"meta_test\"]\n",
                "summary_config = config[\"summary\"]\n",
                "lamda_config = config[\"lamda\"]\n",
                "meta_train_config = config[\"meta_train\"]\n",
                "initial_lr = config[\"optimizer\"][\"initial_lr\"]"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Create Dataset\n",
                "\n",
                "In the case training, actual equation values are not used. Sampling of internal points, boundary points, and initial points within the equation is sufficient.\n",
                "\n",
                "inner_train_dataset is used for training in the inner loop, and outer_train_dataset is used for training in the outer loop."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# create dataset\n",
                "inner_train_dataset = create_train_dataset(\n",
                "    case_name, config, get_seed() + 1)\n",
                "outer_train_dataset = create_train_dataset(\n",
                "    case_name, config, get_seed() + 2)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Build Model\n",
                "\n",
                "In this case, a simple fully connected network is used, and the network shape is determined by the YAML file."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "model = MultiScaleFCSequential(in_channels=model_config[\"in_channels\"],\n",
                "                               out_channels=model_config[\"out_channels\"],\n",
                "                               layers=model_config[\"layers\"],\n",
                "                               neurons=model_config[\"neurons\"],\n",
                "                               residual=model_config[\"residual\"],\n",
                "                               act=model_config[\"activation\"],\n",
                "                               num_scales=1)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Model Training\n",
                "\n",
                "For **MindSpore >= 2.0.0**, the functional programming paradigm can be used to train neural networks.\n",
                "\n",
                "In this case, meta-learning algorithms are employed to learn the weights of terms in the loss function during PINN training.\n",
                "\n",
                "Additionally, model evaluation on unseen equations is performed at intervals between each outer loop."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "epoch: 1 loss: [0.86779684] epoch time: 23640.816ms\n",
                        "epoch: 2 loss: [0.9413994] epoch time: 1189.008ms\n",
                        "epoch: 3 loss: [0.9189246] epoch time: 1087.861ms\n",
                        "epoch: 4 loss: [0.9275309] epoch time: 1123.993ms\n",
                        "...\n",
                        "epoch: 50 l2_error: 1.0040625699565209 epoch time: 8788.293ms\n",
                        "...\n",
                        "epoch: 1000 loss: [0.9453384] epoch time: 1885.815ms\n",
                        "learned params are: pde weight=[1.865994] ic weight=[1.8658787] bc_weight=[1.8648634]\n",
                        "...\n",
                        "-------------------------------------start meta testing-------------------------------------\n",
                        "epoch: 0 l2_error: 1.0029983755797225 epoch time: 12976.881ms\n",
                        "epoch: 100 l2_error: 0.8387432039920448 epoch time: 111.595ms\n",
                        "epoch: 200 l2_error: 0.5762822379368656 epoch time: 111.311ms\n",
                        "...\n",
                        "epoch: 19800 l2_error: 0.033477298861820166 epoch time: 76.523ms\n",
                        "epoch: 19900 l2_error: 0.027350145606692883 epoch time: 79.059ms\n",
                        "-------------------------------------end meta testing-------------------------------------\n",
                        "-------------------------------------start normal training-------------------------------------\n",
                        "epoch: 0 l2_error: 1.0000106844369898 epoch time: 17964.145ms\n",
                        "epoch: 100 l2_error: 0.883998340190983 epoch time: 163.618ms\n",
                        "epoch: 200 l2_error: 0.559024364575979 epoch time: 113.474ms\n",
                        "...\n",
                        "epoch: 19800 l2_error: 0.004723342759751621 epoch time: 93.854ms\n",
                        "epoch: 19900 l2_error: 0.004713620036952725 epoch time: 100.113ms\n",
                        "-------------------------------------end normal training-------------------------------------\n"
                    ]
                }
            ],
            "source": [
                "lamda = lamda_config[\"initial_lamda\"]\n",
                "problem = create_problem(lamda, case_name, model, config)\n",
                "inner_optimizer = nn.SGD(model.trainable_params(),\n",
                "                         initial_lr)\n",
                "outer_optimizer = nn.Adam(problem.get_params(),\n",
                "                          initial_lr)\n",
                "\n",
                "if use_ascend:\n",
                "    from mindspore.amp import DynamicLossScaler, auto_mixed_precision\n",
                "    loss_scaler = DynamicLossScaler(1024, 2, 100)\n",
                "    auto_mixed_precision(model, 'O1')\n",
                "else:\n",
                "    loss_scaler = None\n",
                "\n",
                "inner_trainer = create_trainer(TrainerInfo(case_name, model, inner_optimizer, problem,\n",
                "                                           use_ascend, loss_scaler, config, False, True))\n",
                "outer_trainer = create_trainer(TrainerInfo(case_name, model, outer_optimizer,\n",
                "                                           problem, use_ascend, loss_scaler, config, True, True))\n",
                "\n",
                "inner_train_step = inner_trainer.train_step\n",
                "outer_train_step = outer_trainer.train_step\n",
                "\n",
                "iteration_str = \"iterations\"\n",
                "inner_iters = meta_train_config[\"inner_loop\"][iteration_str]\n",
                "outer_iters = meta_train_config[\"outer_loop\"][iteration_str]\n",
                "\n",
                "steps_per_epochs = inner_train_dataset.get_dataset_size()\n",
                "inner_sink_process = data_sink(\n",
                "    inner_train_step, inner_train_dataset, sink_size=1)\n",
                "outer_sink_process = data_sink(\n",
                "    outer_train_step, outer_train_dataset, sink_size=1)\n",
                "\n",
                "lamda_min = lamda_config[\"lamda_min\"]\n",
                "lamda_max = lamda_config[\"lamda_max\"]\n",
                "\n",
                "used_lamda = [lamda_config[\"eva_lamda\"]]\n",
                "best_params = problem.get_params()\n",
                "best_l2 = 1e10\n",
                "\n",
                "# starting meta training\n",
                "eva_l2_errors = []\n",
                "for epoch in range(1, 1 + outer_iters):\n",
                "    # train\n",
                "    lamda = np.random.uniform(lamda_min, lamda_max)\n",
                "    if lamda not in used_lamda:\n",
                "        used_lamda.append(lamda)\n",
                "    time_beg = time.time()\n",
                "    model.set_train(True)\n",
                "\n",
                "    if epoch % meta_train_config[\"reinit_lamda\"] == 0:\n",
                "        problem.lamda = lamda\n",
                "    if epoch % meta_train_config[\"reinit_epoch\"] == 0:\n",
                "        re_initialize_model(model, epoch)\n",
                "\n",
                "    for _ in range(1, 1 + inner_iters):\n",
                "        for _ in range(steps_per_epochs):\n",
                "            inner_sink_process()\n",
                "    for _ in range(steps_per_epochs):\n",
                "        cur_loss = outer_sink_process()\n",
                "\n",
                "    print(\"epoch: %s loss: %s epoch time: %.3fms\",\n",
                "          epoch, cur_loss, (time.time() - time_beg) * 1000)\n",
                "\n",
                "    if epoch % meta_train_config[\"eva_interval_outer\"] == 0:\n",
                "        # evaluate current model on unseen lamda\n",
                "        print(\"learned params are: %s\",\n",
                "              problem.get_params(if_value=True))\n",
                "        eva_iter = meta_train_config[\"eva_loop\"][iteration_str]\n",
                "        eva_l2_error = evaluate(WorkspaceConfig(epoch, case_name, config, problem.get_params(), eva_iter,\n",
                "                                                eva_iter, use_ascend, loss_scaler, False, True, False, None))\n",
                "        eva_l2_errors.append(eva_l2_error[0])\n",
                "        if eva_l2_error[0] < best_l2:\n",
                "            best_l2 = eva_l2_error[0]\n",
                "            best_params = problem.get_params()\n",
                "\n",
                "print(best_l2)\n",
                "for param in best_params:\n",
                "    print(param.asnumpy())\n",
                "\n",
                "plot_l2_error(case_name, summary_config[\"visual_dir\"],\n",
                "              meta_train_config[\"eva_interval_outer\"], eva_l2_errors)\n",
                "\n",
                "# start comparing\n",
                "test_iter = test_config[iteration_str]\n",
                "test_interval = test_config[\"cal_l2_interval\"]\n",
                "\n",
                "# start meta training\n",
                "meta_l2_errors = evaluate(WorkspaceConfig(None, case_name, config, best_params,\n",
                "                                          test_iter, test_interval,\n",
                "                                          use_ascend, loss_scaler, False, True,\n",
                "                                          True, f\"{case_name}_meta_testing\"))\n",
                "# end meta training\n",
                "\n",
                "# start normal training\n",
                "normal_params = create_normal_params(case_name)\n",
                "\n",
                "normal_l2_errors = evaluate(WorkspaceConfig(None, case_name, config, normal_params,\n",
                "                                            test_iter, test_interval,\n",
                "                                            use_ascend, loss_scaler, False, False,\n",
                "                                            True, f\"{case_name}_normal_training\"))\n",
                "# end normal training\n",
                "\n",
                "plot_l2_comparison_error(\n",
                "    case_name, summary_config[\"visual_dir\"], test_interval, meta_l2_errors, normal_l2_errors)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "![burgers_l2](./images/burgers_l2.png)"
            ]
        }
    ],
    "metadata": {
        "language_info": {
            "name": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
