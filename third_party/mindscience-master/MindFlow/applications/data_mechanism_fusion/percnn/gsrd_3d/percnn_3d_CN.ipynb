{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89a1fe73",
   "metadata": {},
   "source": [
    "# PeRCNN求解3D 反应扩散方程"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8627dd49",
   "metadata": {},
   "source": [
    "## 概述\n",
    "\n",
    "近日，华为与中国人民大学孙浩教授团队合作，基于昇腾AI基础软硬件平台与昇思\n",
    "MindSpore AI框架提出了一种[物理编码递归卷积神经网络（Physics-encoded Recurrent Convolutional Neural Network， PeRCNN）](https://www.nature.com/articles/s42256-023-00685-7)。相较于物理信息神经网络、ConvLSTM、PDE-NET等方法，模型泛化性和抗噪性明显提升，长期推理精度提升了\n",
    "10倍以上，在航空航天、船舶制造、气象预报等领域拥有广阔的应用前景，目前该成果已在 nature machine intelligence 上发表。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed20f19c",
   "metadata": {},
   "source": [
    "## 问题描述"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b71838",
   "metadata": {},
   "source": [
    "反应扩散方程（reaction-diffusion equation）是非常重要且应用广泛的一类偏微分方程，它描述了物理学中的种种现象，也在化学反应中被广泛使用。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4174fdee",
   "metadata": {},
   "source": [
    "## 控制方程\n",
    "\n",
    "在本研究中，反应扩散方程的形式为：\n",
    "\n",
    "$$\n",
    "u_t = \\mu_u \\Delta u - u{v*2} + F(1-v)\n",
    "$$\n",
    "\n",
    "$$\n",
    "v_t = \\mu_v \\Delta v + u{v*2} + (F+\\kappa)v\n",
    "$$\n",
    "\n",
    "其中，\n",
    "\n",
    "$$\n",
    "\\mu_v = 0.1, \\mu_u = 0.2, F = 0.025, \\kappa = 0.055\n",
    "$$\n",
    "\n",
    "在本案例中，拟在$ \\Omega \\times \\tau = {[-50,50]}^3 \\times [0,500] $ 的物理域中求解100个时间步的流场演化（时间步长为0.5s），初始条件经历了高斯加噪，采取周期性边界条件。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd99f3dc",
   "metadata": {},
   "source": [
    "## 技术路径\n",
    "\n",
    "MindFlow求解该问题的具体流程如下：\n",
    "\n",
    "1. 优化器\n",
    "2. 构建模型\n",
    "3. 模型训练\n",
    "4. 模型推理及可视化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27fdf4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c65a682d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mindspore import context, jit, nn, ops, save_checkpoint, set_seed\n",
    "import mindspore.common.dtype as mstype\n",
    "from mindflow.utils import load_yaml_config, print_log\n",
    "from src import RecurrentCnn, post_process, Trainer, UpScaler, count_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b0edc43",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(123456)\n",
    "np.random.seed(123456)\n",
    "context.set_context(mode=context.GRAPH_MODE, device_target=\"GPU\", device_id=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f12211",
   "metadata": {},
   "source": [
    "## 优化器和单步训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe6ae474",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_stage(trainer, stage, config, ckpt_dir, use_ascend):\n",
    "    \"\"\"train stage\"\"\"\n",
    "    if use_ascend:\n",
    "        from mindspore.amp import DynamicLossScaler, all_finite\n",
    "        loss_scaler = DynamicLossScaler(2**10, 2, 100)\n",
    "\n",
    "    if 'milestone_num' in config.keys():\n",
    "        milestone = list([(config['epochs']//config['milestone_num'])*(i + 1)\n",
    "                          for i in range(config['milestone_num'])])\n",
    "        learning_rate = config['learning_rate']\n",
    "        lr = float(config['learning_rate'])*np.array(list([config['gamma']\n",
    "                                                           ** i for i in range(config['milestone_num'])]))\n",
    "        learning_rate = nn.piecewise_constant_lr(milestone, list(lr))\n",
    "    else:\n",
    "        learning_rate = config['learning_rate']\n",
    "\n",
    "    if stage == 'pretrain':\n",
    "        params = trainer.upconv.trainable_params()\n",
    "    else:\n",
    "        params = trainer.upconv.trainable_params() + trainer.recurrent_cnn.trainable_params()\n",
    "\n",
    "    optimizer = nn.Adam(params, learning_rate=learning_rate)\n",
    "\n",
    "    def forward_fn():\n",
    "        if stage == 'pretrain':\n",
    "            loss = trainer.get_ic_loss()\n",
    "        else:\n",
    "            loss = trainer.get_loss()\n",
    "        if use_ascend:\n",
    "            loss = loss_scaler.scale(loss)\n",
    "        return loss\n",
    "\n",
    "    if stage == 'pretrain':\n",
    "        grad_fn = ops.value_and_grad(forward_fn, None, params, has_aux=False)\n",
    "    else:\n",
    "        grad_fn = ops.value_and_grad(forward_fn, None, params, has_aux=True)\n",
    "\n",
    "    @jit\n",
    "    def train_step():\n",
    "        loss, grads = grad_fn()\n",
    "        if use_ascend:\n",
    "            loss = loss_scaler.unscale(loss)\n",
    "            is_finite = all_finite(grads)\n",
    "            if is_finite:\n",
    "                grads = loss_scaler.unscale(grads)\n",
    "                loss = ops.depend(loss, optimizer(grads))\n",
    "            loss_scaler.adjust(is_finite)\n",
    "        else:\n",
    "            loss = ops.depend(loss, optimizer(grads))\n",
    "        return loss\n",
    "\n",
    "    best_loss = sys.maxsize\n",
    "    for epoch in range(1, 1 + config['epochs']):\n",
    "        time_beg = time.time()\n",
    "        trainer.upconv.set_train(True)\n",
    "        trainer.recurrent_cnn.set_train(True)\n",
    "        if stage == 'pretrain':\n",
    "            step_train_loss = train_step()\n",
    "            print_log(\n",
    "                f\"epoch: {epoch} train loss: {step_train_loss} epoch time: {(time.time() - time_beg) :.3f} s\")\n",
    "        else:\n",
    "            if epoch == 3800:\n",
    "                break\n",
    "            epoch_loss, loss_data, loss_ic, loss_phy, loss_valid = train_step()\n",
    "            print_log(f\"epoch: {epoch} train loss: {epoch_loss} ic_loss: {loss_ic} data_loss: {loss_data} \\\n",
    " phy_loss: {loss_phy}  valid_loss: {loss_valid} epoch time: {(time.time() - time_beg): .3f} s\")\n",
    "            if epoch_loss < best_loss:\n",
    "                best_loss = epoch_loss\n",
    "                print_log('best loss', best_loss, 'save model')\n",
    "                save_checkpoint(trainer.upconv, os.path.join(ckpt_dir, \"train_upconv.ckpt\"))\n",
    "                save_checkpoint(trainer.recurrent_cnn,\n",
    "                                os.path.join(ckpt_dir, \"train_recurrent_cnn.ckpt\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933c4152",
   "metadata": {},
   "source": [
    "## 构建模型\n",
    "\n",
    "PeRCNN要构建两个网络，一个是做上采样的UpSclaer，一个是作为主体的recurrent CNN。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d41734e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    \"\"\"train\"\"\"\n",
    "    rd_config = load_yaml_config('./configs/percnn_3d_rd.yaml')\n",
    "    data_config = rd_config['data']\n",
    "    optim_config = rd_config['optimizer']\n",
    "    summary_config = rd_config['summary']\n",
    "    model_config = rd_config['model']\n",
    "\n",
    "    use_ascend = context.get_context(attr_key='device_target') == \"Ascend\"\n",
    "    print_log(f\"use_ascend: {use_ascend}\")\n",
    "\n",
    "    if use_ascend:\n",
    "        compute_dtype = mstype.float16\n",
    "    else:\n",
    "        compute_dtype = mstype.float32\n",
    "\n",
    "    upconv_config = model_config['upconv']\n",
    "    upconv = UpScaler(in_channels=upconv_config['in_channel'],\n",
    "                      out_channels=upconv_config['out_channel'],\n",
    "                      hidden_channels=upconv_config['hidden_channel'],\n",
    "                      kernel_size=upconv_config['kernel_size'],\n",
    "                      stride=upconv_config['stride'],\n",
    "                      has_bais=True)\n",
    "\n",
    "    if use_ascend:\n",
    "        from mindspore.amp import auto_mixed_precision\n",
    "        auto_mixed_precision(upconv, 'O1')\n",
    "\n",
    "    rcnn_config = model_config['rcnn']\n",
    "    recurrent_cnn = RecurrentCnn(input_channels=rcnn_config['in_channel'],\n",
    "                                 hidden_channels=rcnn_config['hidden_channel'],\n",
    "                                 kernel_size=rcnn_config['kernel_size'],\n",
    "                                 stride=rcnn_config['stride'],\n",
    "                                 compute_dtype=compute_dtype)\n",
    "\n",
    "    percnn_trainer = Trainer(upconv=upconv,\n",
    "                             recurrent_cnn=recurrent_cnn,\n",
    "                             timesteps_for_train=data_config['rollout_steps'],\n",
    "                             dx=data_config['dx'],\n",
    "                             grid_size=data_config['grid_size'],\n",
    "                             dt=data_config['dt'],\n",
    "                             mu=data_config['mu'],\n",
    "                             data_path=data_config['data_path'],\n",
    "                             compute_dtype=compute_dtype)\n",
    "\n",
    "    total_params = int(count_params(upconv.trainable_params()) +\n",
    "                       count_params(recurrent_cnn.trainable_params()))\n",
    "    print(f\"There are {total_params} parameters\")\n",
    "\n",
    "    ckpt_dir = summary_config[\"ckpt_dir\"]\n",
    "    fig_path = summary_config[\"fig_save_path\"]\n",
    "    if not os.path.exists(ckpt_dir):\n",
    "        os.makedirs(ckpt_dir)\n",
    "\n",
    "    train_stage(percnn_trainer, 'pretrain',\n",
    "                optim_config['pretrain'], ckpt_dir, use_ascend)\n",
    "    train_stage(percnn_trainer, 'finetune',\n",
    "                optim_config['finetune'], ckpt_dir, use_ascend)\n",
    "\n",
    "    output = percnn_trainer.get_output(100).asnumpy()\n",
    "    output = np.transpose(output, (1, 0, 2, 3, 4))[:, :-1:10]\n",
    "\n",
    "    print('output shape is ', output.shape)\n",
    "    for i in range(0, 10, 2):\n",
    "        post_process(output[0, i], fig_path, is_u=True, num=i)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb93ab1",
   "metadata": {},
   "source": [
    "## 模型训练\n",
    "\n",
    "使用**MindSpore >= 2.0.0**的版本，可以使用函数式编程范式训练神经网络。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d79bc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use_ascend: False\n",
      "shape of uv is  (3001, 2, 48, 48, 48)\n",
      "shape of ic is  (1, 2, 48, 48, 48)\n",
      "shape of init_state_low is  (1, 2, 24, 24, 24)\n",
      "There are 10078 parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ERROR] CORE(60439,7ff577089200,python):2023-10-19-18:58:20.403.131 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_60439/47002643.py]\n",
      "[ERROR] CORE(60439,7ff577089200,python):2023-10-19-18:58:20.403.227 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_60439/47002643.py]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 train loss: 0.160835 epoch time: 5.545 s\n",
      "epoch: 2 train loss: 104.36749 epoch time: 0.010 s\n",
      "epoch: 3 train loss: 4.3207517 epoch time: 0.009 s\n",
      "epoch: 4 train loss: 8.491383 epoch time: 0.009 s\n",
      "epoch: 5 train loss: 23.683647 epoch time: 0.009 s\n",
      "epoch: 6 train loss: 23.857117 epoch time: 0.010 s\n",
      "epoch: 7 train loss: 16.037672 epoch time: 0.010 s\n",
      "epoch: 8 train loss: 8.406443 epoch time: 0.009 s\n",
      "epoch: 9 train loss: 3.527469 epoch time: 0.020 s\n",
      "epoch: 10 train loss: 1.0823832 epoch time: 0.009 s\n",
      "epoch: 11 train loss: 0.18675002 epoch time: 0.018 s\n",
      "epoch: 12 train loss: 0.13879554 epoch time: 0.019 s\n",
      "epoch: 13 train loss: 0.4895127 epoch time: 0.013 s\n",
      "epoch: 14 train loss: 0.9677096 epoch time: 0.008 s\n",
      "epoch: 15 train loss: 1.4145753 epoch time: 0.012 s\n",
      "epoch: 16 train loss: 1.7454404 epoch time: 0.019 s\n",
      "epoch: 17 train loss: 1.9276694 epoch time: 0.022 s\n",
      "epoch: 18 train loss: 1.9647502 epoch time: 0.009 s\n",
      "epoch: 19 train loss: 1.8823279 epoch time: 0.020 s\n",
      "epoch: 20 train loss: 1.7159437 epoch time: 0.021 s\n",
      "epoch: 21 train loss: 1.5015931 epoch time: 0.017 s\n",
      "epoch: 22 train loss: 1.2698298 epoch time: 0.013 s\n",
      "epoch: 23 train loss: 1.0432097 epoch time: 0.008 s\n",
      "epoch: 24 train loss: 0.8361859 epoch time: 0.012 s\n",
      "epoch: 25 train loss: 0.6563898 epoch time: 0.008 s\n",
      "epoch: 26 train loss: 0.50641394 epoch time: 0.011 s\n",
      "epoch: 27 train loss: 0.38553846 epoch time: 0.024 s\n",
      "epoch: 28 train loss: 0.29113472 epoch time: 0.011 s\n",
      "epoch: 29 train loss: 0.21967764 epoch time: 0.017 s\n",
      "epoch: 30 train loss: 0.16740894 epoch time: 0.014 s\n",
      "epoch: 31 train loss: 0.13072886 epoch time: 0.017 s\n",
      "epoch: 32 train loss: 0.10639836 epoch time: 0.009 s\n",
      "epoch: 33 train loss: 0.09161945 epoch time: 0.009 s\n",
      "epoch: 34 train loss: 0.08404361 epoch time: 0.009 s\n",
      "epoch: 35 train loss: 0.08174121 epoch time: 0.009 s\n",
      "epoch: 36 train loss: 0.08315387 epoch time: 0.011 s\n",
      "epoch: 37 train loss: 0.087041795 epoch time: 0.008 s\n",
      "epoch: 38 train loss: 0.09243279 epoch time: 0.009 s\n",
      "epoch: 39 train loss: 0.09857648 epoch time: 0.011 s\n",
      "epoch: 40 train loss: 0.104904406 epoch time: 0.010 s\n",
      "epoch: 41 train loss: 0.11099631 epoch time: 0.009 s\n",
      "epoch: 42 train loss: 0.116552 epoch time: 0.009 s\n",
      "epoch: 43 train loss: 0.12136779 epoch time: 0.009 s\n",
      "epoch: 44 train loss: 0.12531736 epoch time: 0.009 s\n",
      "epoch: 45 train loss: 0.12833557 epoch time: 0.009 s\n",
      "epoch: 46 train loss: 0.13040529 epoch time: 0.008 s\n",
      "epoch: 47 train loss: 0.13154633 epoch time: 0.009 s\n",
      "epoch: 48 train loss: 0.13180636 epoch time: 0.009 s\n",
      "epoch: 49 train loss: 0.13125326 epoch time: 0.009 s\n",
      "epoch: 50 train loss: 0.12996882 epoch time: 0.010 s\n",
      "epoch: 51 train loss: 0.1280437 epoch time: 0.009 s\n",
      "epoch: 52 train loss: 0.12557296 epoch time: 0.008 s\n",
      "epoch: 53 train loss: 0.12265288 epoch time: 0.009 s\n",
      "epoch: 54 train loss: 0.11937802 epoch time: 0.008 s\n",
      "epoch: 55 train loss: 0.11583916 epoch time: 0.009 s\n",
      "epoch: 56 train loss: 0.11212168 epoch time: 0.008 s\n",
      "epoch: 57 train loss: 0.10830422 epoch time: 0.008 s\n",
      "epoch: 58 train loss: 0.10445792 epoch time: 0.009 s\n",
      "epoch: 59 train loss: 0.10064584 epoch time: 0.009 s\n",
      "epoch: 60 train loss: 0.096922554 epoch time: 0.009 s\n",
      "epoch: 61 train loss: 0.0933342 epoch time: 0.009 s\n",
      "epoch: 62 train loss: 0.089918405 epoch time: 0.009 s\n",
      "epoch: 63 train loss: 0.08670464 epoch time: 0.009 s\n",
      "epoch: 64 train loss: 0.0837146 epoch time: 0.008 s\n",
      "epoch: 65 train loss: 0.08096256 epoch time: 0.009 s\n",
      "epoch: 66 train loss: 0.07845608 epoch time: 0.009 s\n",
      "epoch: 67 train loss: 0.07619659 epoch time: 0.009 s\n",
      "epoch: 68 train loss: 0.07418011 epoch time: 0.009 s\n",
      "epoch: 69 train loss: 0.072398014 epoch time: 0.008 s\n",
      "epoch: 70 train loss: 0.07083781 epoch time: 0.008 s\n",
      "epoch: 71 train loss: 0.06948393 epoch time: 0.009 s\n",
      "epoch: 72 train loss: 0.06831847 epoch time: 0.009 s\n",
      "epoch: 73 train loss: 0.06732196 epoch time: 0.009 s\n",
      "epoch: 74 train loss: 0.06647409 epoch time: 0.009 s\n",
      "epoch: 75 train loss: 0.06575425 epoch time: 0.009 s\n",
      "epoch: 76 train loss: 0.06514216 epoch time: 0.008 s\n",
      "epoch: 77 train loss: 0.06461836 epoch time: 0.008 s\n",
      "epoch: 78 train loss: 0.06416454 epoch time: 0.009 s\n",
      "epoch: 79 train loss: 0.063763924 epoch time: 0.009 s\n",
      "epoch: 80 train loss: 0.06340143 epoch time: 0.009 s\n",
      "epoch: 81 train loss: 0.06306383 epoch time: 0.009 s\n",
      "epoch: 82 train loss: 0.062739834 epoch time: 0.008 s\n",
      "epoch: 83 train loss: 0.062420063 epoch time: 0.008 s\n",
      "epoch: 84 train loss: 0.06209702 epoch time: 0.009 s\n",
      "epoch: 85 train loss: 0.061764974 epoch time: 0.009 s\n",
      "epoch: 86 train loss: 0.06141983 epoch time: 0.009 s\n",
      "epoch: 87 train loss: 0.061058972 epoch time: 0.008 s\n",
      "epoch: 88 train loss: 0.06068106 epoch time: 0.009 s\n",
      "epoch: 89 train loss: 0.060285877 epoch time: 0.008 s\n",
      "epoch: 90 train loss: 0.059874102 epoch time: 0.009 s\n",
      "epoch: 91 train loss: 0.059447125 epoch time: 0.008 s\n",
      "epoch: 92 train loss: 0.059006896 epoch time: 0.013 s\n",
      "epoch: 93 train loss: 0.058555692 epoch time: 0.009 s\n",
      "epoch: 94 train loss: 0.058096036 epoch time: 0.009 s\n",
      "epoch: 95 train loss: 0.057630505 epoch time: 0.009 s\n",
      "epoch: 96 train loss: 0.05716162 epoch time: 0.009 s\n",
      "epoch: 97 train loss: 0.05669179 epoch time: 0.008 s\n",
      "epoch: 98 train loss: 0.0562232 epoch time: 0.009 s\n",
      "epoch: 99 train loss: 0.055757735 epoch time: 0.009 s\n",
      "epoch: 100 train loss: 0.055297032 epoch time: 0.008 s\n",
      "epoch: 101 train loss: 0.054842368 epoch time: 0.009 s\n",
      "epoch: 102 train loss: 0.05440359 epoch time: 0.009 s\n",
      "epoch: 103 train loss: 0.053972173 epoch time: 0.009 s\n",
      "epoch: 104 train loss: 0.0535485 epoch time: 0.009 s\n",
      "epoch: 105 train loss: 0.053132694 epoch time: 0.009 s\n",
      "epoch: 106 train loss: 0.052724693 epoch time: 0.008 s\n",
      "epoch: 107 train loss: 0.05232426 epoch time: 0.009 s\n",
      "epoch: 108 train loss: 0.05193102 epoch time: 0.008 s\n",
      "epoch: 109 train loss: 0.051544502 epoch time: 0.009 s\n",
      "epoch: 110 train loss: 0.051164184 epoch time: 0.008 s\n",
      "epoch: 111 train loss: 0.05078949 epoch time: 0.008 s\n",
      "epoch: 112 train loss: 0.050419834 epoch time: 0.009 s\n",
      "epoch: 113 train loss: 0.050054647 epoch time: 0.009 s\n",
      "epoch: 114 train loss: 0.04969338 epoch time: 0.009 s\n",
      "epoch: 115 train loss: 0.049335532 epoch time: 0.009 s\n",
      "epoch: 116 train loss: 0.04898065 epoch time: 0.009 s\n",
      "epoch: 117 train loss: 0.04862833 epoch time: 0.009 s\n",
      "epoch: 118 train loss: 0.04827823 epoch time: 0.009 s\n",
      "epoch: 119 train loss: 0.047930077 epoch time: 0.036 s\n",
      "epoch: 120 train loss: 0.04758365 epoch time: 0.008 s\n",
      "epoch: 121 train loss: 0.047238782 epoch time: 0.009 s\n",
      "epoch: 122 train loss: 0.04689537 epoch time: 0.009 s\n",
      "epoch: 123 train loss: 0.04655334 epoch time: 0.008 s\n",
      "epoch: 124 train loss: 0.046212673 epoch time: 0.007 s\n",
      "epoch: 125 train loss: 0.04587337 epoch time: 0.007 s\n",
      "epoch: 126 train loss: 0.045535468 epoch time: 0.008 s\n",
      "epoch: 127 train loss: 0.04519902 epoch time: 0.007 s\n",
      "epoch: 128 train loss: 0.0448641 epoch time: 0.007 s\n",
      "epoch: 129 train loss: 0.04453077 epoch time: 0.007 s\n",
      "epoch: 130 train loss: 0.04419911 epoch time: 0.007 s\n",
      "epoch: 131 train loss: 0.043869186 epoch time: 0.008 s\n",
      "epoch: 132 train loss: 0.043541066 epoch time: 0.009 s\n",
      "epoch: 133 train loss: 0.043214805 epoch time: 0.009 s\n",
      "epoch: 134 train loss: 0.042890463 epoch time: 0.008 s\n",
      "epoch: 135 train loss: 0.042568065 epoch time: 0.009 s\n",
      "epoch: 136 train loss: 0.042247646 epoch time: 0.008 s\n",
      "epoch: 137 train loss: 0.04192921 epoch time: 0.009 s\n",
      "epoch: 138 train loss: 0.041612785 epoch time: 0.008 s\n",
      "epoch: 139 train loss: 0.041298337 epoch time: 0.008 s\n",
      "epoch: 140 train loss: 0.040985867 epoch time: 0.008 s\n",
      "epoch: 141 train loss: 0.04067536 epoch time: 0.009 s\n",
      "epoch: 142 train loss: 0.040366802 epoch time: 0.008 s\n",
      "epoch: 143 train loss: 0.04006016 epoch time: 0.008 s\n",
      "epoch: 144 train loss: 0.03975539 epoch time: 0.008 s\n",
      "epoch: 145 train loss: 0.03945248 epoch time: 0.008 s\n",
      "epoch: 146 train loss: 0.039151385 epoch time: 0.008 s\n",
      "epoch: 147 train loss: 0.03885208 epoch time: 0.008 s\n",
      "epoch: 148 train loss: 0.038554545 epoch time: 0.008 s\n",
      "epoch: 149 train loss: 0.03825874 epoch time: 0.008 s\n",
      "epoch: 150 train loss: 0.03796464 epoch time: 0.009 s\n",
      "epoch: 151 train loss: 0.03767223 epoch time: 0.008 s\n",
      "epoch: 152 train loss: 0.03738148 epoch time: 0.008 s\n",
      "epoch: 153 train loss: 0.037092384 epoch time: 0.008 s\n",
      "epoch: 154 train loss: 0.036804926 epoch time: 0.008 s\n",
      "epoch: 155 train loss: 0.036519088 epoch time: 0.008 s\n",
      "epoch: 156 train loss: 0.036234863 epoch time: 0.007 s\n",
      "epoch: 157 train loss: 0.035952244 epoch time: 0.008 s\n",
      "epoch: 158 train loss: 0.03567122 epoch time: 0.008 s\n",
      "epoch: 159 train loss: 0.035391793 epoch time: 0.007 s\n",
      "epoch: 160 train loss: 0.035113953 epoch time: 0.007 s\n",
      "epoch: 161 train loss: 0.034837693 epoch time: 0.008 s\n",
      "epoch: 162 train loss: 0.034563016 epoch time: 0.009 s\n",
      "epoch: 163 train loss: 0.034289915 epoch time: 0.009 s\n",
      "epoch: 164 train loss: 0.03401839 epoch time: 0.009 s\n",
      "epoch: 165 train loss: 0.03374843 epoch time: 0.009 s\n",
      "epoch: 166 train loss: 0.033480033 epoch time: 0.009 s\n",
      "epoch: 167 train loss: 0.033213194 epoch time: 0.008 s\n",
      "epoch: 168 train loss: 0.032947917 epoch time: 0.008 s\n",
      "epoch: 169 train loss: 0.03268419 epoch time: 0.009 s\n",
      "epoch: 170 train loss: 0.03242201 epoch time: 0.008 s\n",
      "epoch: 171 train loss: 0.032161362 epoch time: 0.009 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 9977 train loss: 8.792636e-05 epoch time: 0.007 s\n",
      "epoch: 9978 train loss: 8.811904e-05 epoch time: 0.008 s\n",
      "epoch: 9979 train loss: 8.8046014e-05 epoch time: 0.008 s\n",
      "epoch: 9980 train loss: 8.781863e-05 epoch time: 0.007 s\n",
      "epoch: 9981 train loss: 8.778041e-05 epoch time: 0.007 s\n",
      "epoch: 9982 train loss: 8.790506e-05 epoch time: 0.007 s\n",
      "epoch: 9983 train loss: 8.791076e-05 epoch time: 0.008 s\n",
      "epoch: 9984 train loss: 8.77632e-05 epoch time: 0.008 s\n",
      "epoch: 9985 train loss: 8.768207e-05 epoch time: 0.008 s\n",
      "epoch: 9986 train loss: 8.7740336e-05 epoch time: 0.008 s\n",
      "epoch: 9987 train loss: 8.777662e-05 epoch time: 0.008 s\n",
      "epoch: 9988 train loss: 8.769672e-05 epoch time: 0.008 s\n",
      "epoch: 9989 train loss: 8.760943e-05 epoch time: 0.008 s\n",
      "epoch: 9990 train loss: 8.7615306e-05 epoch time: 0.008 s\n",
      "epoch: 9991 train loss: 8.76504e-05 epoch time: 0.008 s\n",
      "epoch: 9992 train loss: 8.761823e-05 epoch time: 0.008 s\n",
      "epoch: 9993 train loss: 8.7546505e-05 epoch time: 0.008 s\n",
      "epoch: 9994 train loss: 8.7519744e-05 epoch time: 0.008 s\n",
      "epoch: 9995 train loss: 8.753734e-05 epoch time: 0.008 s\n",
      "epoch: 9996 train loss: 8.753101e-05 epoch time: 0.008 s\n",
      "epoch: 9997 train loss: 8.748294e-05 epoch time: 0.008 s\n",
      "epoch: 9998 train loss: 8.7443106e-05 epoch time: 0.008 s\n",
      "epoch: 9999 train loss: 8.743979e-05 epoch time: 0.008 s\n",
      "epoch: 10000 train loss: 8.744074e-05 epoch time: 0.008 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 train loss: 61.754555 ic_loss: 8.7413886e-05 data_loss: 6.1754117  phy_loss: 2.6047118  valid_loss: 7.221066 truth_loss: 2.7125626 epoch time:  138.495 s\n",
      "best loss 61.754555 save model\n",
      "epoch: 2 train loss: 54.79151 ic_loss: 0.32984126 data_loss: 5.3142304  phy_loss: 52.50226  valid_loss: 6.812231 truth_loss: 2.7744124 epoch time:  1.342 s\n",
      "best loss 54.79151 save model\n",
      "epoch: 3 train loss: 46.904842 ic_loss: 0.12049961 data_loss: 4.6302347  phy_loss: 32.494545  valid_loss: 5.953037 truth_loss: 2.579268 epoch time:  1.262 s\n",
      "best loss 46.904842 save model\n",
      "epoch: 4 train loss: 40.674736 ic_loss: 0.031907484 data_loss: 4.05152  phy_loss: 11.360751  valid_loss: 5.08032 truth_loss: 2.3503494 epoch time:  1.233 s\n",
      "best loss 40.674736 save model\n",
      "epoch: 5 train loss: 36.910408 ic_loss: 0.10554239 data_loss: 3.6382694  phy_loss: 3.5776496  valid_loss: 4.4271708 truth_loss: 2.1671412 epoch time:  1.315 s\n",
      "best loss 36.910408 save model\n",
      "epoch: 6 train loss: 33.767193 ic_loss: 0.14396289 data_loss: 3.304738  phy_loss: 1.4308721  valid_loss: 3.954126 truth_loss: 2.0307255 epoch time:  1.322 s\n",
      "best loss 33.767193 save model\n",
      "epoch: 7 train loss: 30.495178 ic_loss: 0.09850004 data_loss: 3.0002677  phy_loss: 0.8241035  valid_loss: 3.586939 truth_loss: 1.9244627 epoch time:  1.178 s\n",
      "best loss 30.495178 save model\n",
      "epoch: 8 train loss: 27.448381 ic_loss: 0.03362463 data_loss: 2.728026  phy_loss: 0.6343211  valid_loss: 3.286183 truth_loss: 1.8369334 epoch time:  1.271 s\n",
      "best loss 27.448381 save model\n",
      "epoch: 9 train loss: 24.990078 ic_loss: 0.0024543565 data_loss: 2.4977806  phy_loss: 0.5740176  valid_loss: 3.0332325 truth_loss: 1.7619449 epoch time:  1.573 s\n",
      "best loss 24.990078 save model\n",
      "epoch: 10 train loss: 23.15583 ic_loss: 0.014634784 data_loss: 2.3082657  phy_loss: 0.5407104  valid_loss: 2.8156128 truth_loss: 1.6955423 epoch time:  1.351 s\n",
      "best loss 23.15583 save model\n",
      "epoch: 11 train loss: 21.735628 ic_loss: 0.047814455 data_loss: 2.1496556  phy_loss: 0.48163098  valid_loss: 2.6234283 truth_loss: 1.6348788 epoch time:  1.201 s\n",
      "best loss 21.735628 save model\n",
      "epoch: 12 train loss: 20.475292 ic_loss: 0.072982915 data_loss: 2.0110378  phy_loss: 0.38152087  valid_loss: 2.4501312 truth_loss: 1.5782493 epoch time:  1.139 s\n",
      "best loss 20.475292 save model\n",
      "epoch: 13 train loss: 19.234226 ic_loss: 0.075263694 data_loss: 1.8857908  phy_loss: 0.26258236  valid_loss: 2.2930105 truth_loss: 1.5251731 epoch time:  1.204 s\n",
      "best loss 19.234226 save model\n",
      "epoch: 14 train loss: 18.010792 ic_loss: 0.057861533 data_loss: 1.7721483  phy_loss: 0.1595172  valid_loss: 2.1518373 truth_loss: 1.4759724 epoch time:  1.237 s\n",
      "best loss 18.010792 save model\n",
      "epoch: 15 train loss: 16.871925 ic_loss: 0.033282857 data_loss: 1.6705511  phy_loss: 0.09075087  valid_loss: 2.0266707 truth_loss: 1.4310707 epoch time:  1.228 s\n",
      "best loss 16.871925 save model\n",
      "epoch: 16 train loss: 15.876991 ic_loss: 0.013340431 data_loss: 1.5810289  phy_loss: 0.05365573  valid_loss: 1.9164674 truth_loss: 1.3904977 epoch time:  1.318 s\n",
      "best loss 15.876991 save model\n",
      "epoch: 17 train loss: 15.041311 ic_loss: 0.0038963158 data_loss: 1.502183  phy_loss: 0.03669169  valid_loss: 1.8189894 truth_loss: 1.3538085 epoch time:  1.310 s\n",
      "best loss 15.041311 save model\n",
      "epoch: 18 train loss: 14.33803 ic_loss: 0.004466851 data_loss: 1.4315696  phy_loss: 0.029765908  valid_loss: 1.7315247 truth_loss: 1.3202969 epoch time:  1.374 s\n",
      "best loss 14.33803 save model\n",
      "epoch: 19 train loss: 13.720046 ic_loss: 0.010658737 data_loss: 1.3666753  phy_loss: 0.027048446  valid_loss: 1.6517344 truth_loss: 1.2892944 epoch time:  1.370 s\n",
      "best loss 13.720046 save model\n",
      "epoch: 20 train loss: 13.144362 ic_loss: 0.017341504 data_loss: 1.3057655  phy_loss: 0.0259407  valid_loss: 1.5781578 truth_loss: 1.2603784 epoch time:  1.321 s\n",
      "best loss 13.144362 save model\n",
      "epoch: 21 train loss: 12.587399 ic_loss: 0.021058846 data_loss: 1.2482104  phy_loss: 0.025655862  valid_loss: 1.5102282 truth_loss: 1.2334127 epoch time:  1.184 s\n",
      "best loss 12.587399 save model\n",
      "epoch: 22 train loss: 12.046999 ic_loss: 0.020924855 data_loss: 1.1942375  phy_loss: 0.026263908  valid_loss: 1.4478996 truth_loss: 1.2084355 epoch time:  1.200 s\n",
      "best loss 12.046999 save model\n",
      "epoch: 23 train loss: 11.53398 ic_loss: 0.018084219 data_loss: 1.144356  phy_loss: 0.028034285  valid_loss: 1.3911395 truth_loss: 1.1854722 epoch time:  1.196 s\n",
      "best loss 11.53398 save model\n",
      "epoch: 24 train loss: 11.060363 ic_loss: 0.014451177 data_loss: 1.0988107  phy_loss: 0.030886661  valid_loss: 1.3395449 truth_loss: 1.1643757 epoch time:  1.329 s\n",
      "best loss 11.060363 save model\n",
      "epoch: 25 train loss: 10.630998 ic_loss: 0.011555473 data_loss: 1.057322  phy_loss: 0.033994675  valid_loss: 1.2922314 truth_loss: 1.1447669 epoch time:  1.276 s\n",
      "best loss 10.630998 save model\n",
      "epoch: 26 train loss: 10.241822 ic_loss: 0.009986807 data_loss: 1.0191888  phy_loss: 0.035903756  valid_loss: 1.2480327 truth_loss: 1.1261127 epoch time:  1.205 s\n",
      "best loss 10.241822 save model\n",
      "epoch: 27 train loss: 9.883552 ic_loss: 0.009476765 data_loss: 0.98361677  phy_loss: 0.03534786  valid_loss: 1.20588 truth_loss: 1.1079 epoch time:  1.239 s\n",
      "best loss 9.883552 save model\n",
      "epoch: 28 train loss: 9.547181 ic_loss: 0.00935055 data_loss: 0.95004284  phy_loss: 0.032170422  valid_loss: 1.165143 truth_loss: 1.0898125 epoch time:  1.259 s\n",
      "best loss 9.547181 save model\n",
      "epoch: 29 train loss: 9.227694 ic_loss: 0.009004757 data_loss: 0.91826695  phy_loss: 0.02744086  valid_loss: 1.1257396 truth_loss: 1.0718101 epoch time:  1.247 s\n",
      "best loss 9.227694 save model\n",
      "epoch: 30 train loss: 8.924402 ic_loss: 0.008175709 data_loss: 0.8883524  phy_loss: 0.022667732  valid_loss: 1.0879731 truth_loss: 1.0540737 epoch time:  1.230 s\n",
      "best loss 8.924402 save model\n",
      "epoch: 31 train loss: 8.638719 ic_loss: 0.0069448506 data_loss: 0.8603994  phy_loss: 0.018913466  valid_loss: 1.0522348 truth_loss: 1.0368662 epoch time:  1.245 s\n",
      "best loss 8.638719 save model\n",
      "epoch: 32 train loss: 8.371384 ic_loss: 0.00557371 data_loss: 0.83435154  phy_loss: 0.01649121  valid_loss: 1.0187497 truth_loss: 1.0203989 epoch time:  1.263 s\n",
      "best loss 8.371384 save model\n",
      "epoch: 33 train loss: 8.120908 ic_loss: 0.004312452 data_loss: 0.80993456  phy_loss: 0.015186624  valid_loss: 0.98748696 truth_loss: 1.0047648 epoch time:  1.271 s\n",
      "best loss 8.120908 save model\n",
      "epoch: 34 train loss: 7.8838377 ic_loss: 0.0032896025 data_loss: 0.786739  phy_loss: 0.014595456  valid_loss: 0.9582258 truth_loss: 0.9899538 epoch time:  1.197 s\n",
      "best loss 7.8838377 save model\n",
      "epoch: 35 train loss: 7.6563325 ic_loss: 0.002512321 data_loss: 0.76437706  phy_loss: 0.0143464245  valid_loss: 0.9307019 truth_loss: 0.97591347 epoch time:  1.240 s\n",
      "best loss 7.6563325 save model\n",
      "epoch: 36 train loss: 7.435952 ic_loss: 0.0019378862 data_loss: 0.7426263  phy_loss: 0.01420884  valid_loss: 0.90473443 truth_loss: 0.9626097 epoch time:  1.256 s\n",
      "best loss 7.435952 save model\n",
      "epoch: 37 train loss: 7.2225933 ic_loss: 0.0015484956 data_loss: 0.7214851  phy_loss: 0.014122354  valid_loss: 0.88026917 truth_loss: 0.95004666 epoch time:  1.195 s\n",
      "best loss 7.2225933 save model\n",
      "epoch: 38 train loss: 7.0181174 ic_loss: 0.001376933 data_loss: 0.7011233  phy_loss: 0.014153889  valid_loss: 0.85732114 truth_loss: 0.9382366 epoch time:  1.302 s\n",
      "best loss 7.0181174 save model\n",
      "epoch: 39 train loss: 6.8248954 ic_loss: 0.001471745 data_loss: 0.6817537  phy_loss: 0.014397057  valid_loss: 0.8358557 truth_loss: 0.92713416 epoch time:  1.302 s\n",
      "best loss 6.8248954 save model\n",
      "epoch: 40 train loss: 6.644097 ic_loss: 0.0018317336 data_loss: 0.6634938  phy_loss: 0.014855212  valid_loss: 0.81567574 truth_loss: 0.9165742 epoch time:  1.214 s\n",
      "best loss 6.644097 save model\n",
      "epoch: 41 train loss: 6.474652 ic_loss: 0.0023596808 data_loss: 0.64628536  phy_loss: 0.015363368  valid_loss: 0.796389 truth_loss: 0.9062592 epoch time:  1.208 s\n",
      "best loss 6.474652 save model\n",
      "epoch: 42 train loss: 6.3135366 ic_loss: 0.002874115 data_loss: 0.6299166  phy_loss: 0.015615393  valid_loss: 0.7774975 truth_loss: 0.89582586 epoch time:  1.276 s\n",
      "best loss 6.3135366 save model\n",
      "epoch: 43 train loss: 6.1572404 ic_loss: 0.0031803595 data_loss: 0.6141339  phy_loss: 0.015325737  valid_loss: 0.75857925 truth_loss: 0.884974 epoch time:  1.239 s\n",
      "best loss 6.1572404 save model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1640 train loss: 0.094870105 ic_loss: 0.0006555757 data_loss: 0.009159223  phy_loss: 0.000713372  valid_loss: 0.012182931 truth_loss: 0.16177362 epoch time:  1.289 s\n",
      "best loss 0.094870105 save model\n",
      "epoch: 1641 train loss: 0.09474868 ic_loss: 0.00065547746 data_loss: 0.00914713  phy_loss: 0.00071231654  valid_loss: 0.01216803 truth_loss: 0.16169967 epoch time:  1.259 s\n",
      "best loss 0.09474868 save model\n",
      "epoch: 1642 train loss: 0.09462735 ic_loss: 0.0006553787 data_loss: 0.009135046  phy_loss: 0.00071125705  valid_loss: 0.012153144 truth_loss: 0.16162594 epoch time:  1.310 s\n",
      "best loss 0.09462735 save model\n",
      "epoch: 1643 train loss: 0.094506115 ic_loss: 0.000655279 data_loss: 0.009122972  phy_loss: 0.00071020663  valid_loss: 0.01213827 truth_loss: 0.16155209 epoch time:  1.379 s\n",
      "best loss 0.094506115 save model\n",
      "epoch: 1644 train loss: 0.094384976 ic_loss: 0.0006551788 data_loss: 0.009110908  phy_loss: 0.0007091502  valid_loss: 0.012123411 truth_loss: 0.16147849 epoch time:  1.375 s\n",
      "best loss 0.094384976 save model\n",
      "epoch: 1645 train loss: 0.094263926 ic_loss: 0.0006550779 data_loss: 0.009098854  phy_loss: 0.0007081007  valid_loss: 0.012108564 truth_loss: 0.16140485 epoch time:  1.354 s\n",
      "best loss 0.094263926 save model\n",
      "epoch: 1646 train loss: 0.09414298 ic_loss: 0.0006549765 data_loss: 0.00908681  phy_loss: 0.00070705137  valid_loss: 0.012093734 truth_loss: 0.16133131 epoch time:  1.332 s\n",
      "best loss 0.09414298 save model\n",
      "epoch: 1647 train loss: 0.09402215 ic_loss: 0.0006548743 data_loss: 0.009074777  phy_loss: 0.0007060007  valid_loss: 0.012078916 truth_loss: 0.16125791 epoch time:  1.435 s\n",
      "best loss 0.09402215 save model\n",
      "epoch: 1648 train loss: 0.09390141 ic_loss: 0.0006547714 data_loss: 0.009062755  phy_loss: 0.00070495723  valid_loss: 0.012064112 truth_loss: 0.16118445 epoch time:  1.402 s\n",
      "best loss 0.09390141 save model\n",
      "epoch: 1649 train loss: 0.09378076 ic_loss: 0.00065466797 data_loss: 0.009050743  phy_loss: 0.0007039088  valid_loss: 0.012049323 truth_loss: 0.1611112 epoch time:  1.284 s\n",
      "best loss 0.09378076 save model\n",
      "epoch: 1650 train loss: 0.09366022 ic_loss: 0.00065456395 data_loss: 0.009038741  phy_loss: 0.00070286694  valid_loss: 0.0120345475 truth_loss: 0.16103792 epoch time:  1.502 s\n",
      "best loss 0.09366022 save model\n",
      "epoch: 1651 train loss: 0.093539774 ic_loss: 0.0006544591 data_loss: 0.009026748  phy_loss: 0.0007018241  valid_loss: 0.012019787 truth_loss: 0.16096477 epoch time:  1.274 s\n",
      "best loss 0.093539774 save model\n",
      "epoch: 1652 train loss: 0.093419425 ic_loss: 0.0006543536 data_loss: 0.009014766  phy_loss: 0.00070078264  valid_loss: 0.012005039 truth_loss: 0.16089168 epoch time:  1.456 s\n",
      "best loss 0.093419425 save model\n",
      "epoch: 1653 train loss: 0.09329918 ic_loss: 0.00065424765 data_loss: 0.0090027945  phy_loss: 0.00069974473  valid_loss: 0.011990305 truth_loss: 0.16081864 epoch time:  1.203 s\n",
      "best loss 0.09329918 save model\n",
      "epoch: 1654 train loss: 0.09317903 ic_loss: 0.00065414095 data_loss: 0.008990833  phy_loss: 0.000698706  valid_loss: 0.011975586 truth_loss: 0.16074573 epoch time:  1.285 s\n",
      "best loss 0.09317903 save model\n",
      "epoch: 1655 train loss: 0.09305898 ic_loss: 0.0006540336 data_loss: 0.008978881  phy_loss: 0.0006976697  valid_loss: 0.011960882 truth_loss: 0.16067289 epoch time:  1.159 s\n",
      "best loss 0.09305898 save model\n",
      "epoch: 1656 train loss: 0.092939034 ic_loss: 0.0006539258 data_loss: 0.00896694  phy_loss: 0.00069663546  valid_loss: 0.01194619 truth_loss: 0.16060013 epoch time:  1.274 s\n",
      "best loss 0.092939034 save model\n",
      "epoch: 1657 train loss: 0.092819184 ic_loss: 0.00065381714 data_loss: 0.00895501  phy_loss: 0.00069560105  valid_loss: 0.011931514 truth_loss: 0.16052744 epoch time:  1.174 s\n",
      "best loss 0.092819184 save model\n",
      "epoch: 1658 train loss: 0.09269943 ic_loss: 0.0006537079 data_loss: 0.008943089  phy_loss: 0.0006945693  valid_loss: 0.011916851 truth_loss: 0.1604548 epoch time:  1.296 s\n",
      "best loss 0.09269943 save model\n",
      "epoch: 1659 train loss: 0.092579775 ic_loss: 0.00065359805 data_loss: 0.008931179  phy_loss: 0.0006935386  valid_loss: 0.0119022 truth_loss: 0.16038223 epoch time:  1.426 s\n",
      "best loss 0.092579775 save model\n",
      "epoch: 1660 train loss: 0.09246021 ic_loss: 0.0006534874 data_loss: 0.008919277  phy_loss: 0.00069250836  valid_loss: 0.011887563 truth_loss: 0.16030973 epoch time:  1.389 s\n",
      "best loss 0.09246021 save model\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a90b495f",
   "metadata": {},
   "source": [
    "## 模型推理及可视化\n",
    "\n",
    "完成训练后，下图展示了预测结果和真实标签的对比情况。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f41ffe",
   "metadata": {},
   "source": [
    "![](./images/result.jpg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.14 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.14"
  },
  "vscode": {
   "interpreter": {
    "hash": "fd69f43f58546b570e94fd7eba7b65e6bcc7a5bbc4eab0408017d18902915d69"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
