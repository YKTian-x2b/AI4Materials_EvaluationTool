{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Solving 2D burgers Equation using PeRCNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Overview\n",
    "\n",
    "PDE equations occupy an important position in the modeling of physical systems. But many underlying PDEs have not yet been fully explored in epidemiology, meteorological science, fluid mechanics, and biology. However, for those known PDE equations, such as Naiver-Stokes equations, the exact numerical calculation of these equations requires huge computing power, which hinders the application of numerical simulation in large-scale systems. Recently, advances in machine learning provide a new way for PDE solution and inversion.\n",
    "\n",
    "Recently, Huawei and Professor Sun Hao's team from Renmin University of China proposed Physics-encoded Recurrent Convolutional Neural Network, PeRCNN(https://www.nature.com/articles/s42256-023-00685-7) based on Ascend platform and MindSpore. Compared with physical information neural network, ConvLSTM, PDE-NET and other methods, generalization and noise resistance of PeRCNN are significantly improved. The long-term prediction accuracy is improved by more than 10 times. This method has broad application prospects in aerospace, shipbuilding, weather forecasting and other fields. The results have been published in nature machine intelligence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Description\n",
    "\n",
    "Burgers' equation is a nonlinear partial differential equation that simulates the propagation and reflection of shock waves. It is widely used in the fields of fluid mechanics, nonlinear acoustics, gas dynamics et al. It is named after Johannes Martins Hamburg (1895-1981). In this case, the 2D Burgers' equation with viscosity is solved based on PeRCNN method.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Governing Equation\n",
    "\n",
    "In this research, Burgers Equation is formulted as follow:\n",
    "\n",
    "$$\n",
    "u_{t} = \\nu \\Delta u - (uu_{x} + vu_{y})\n",
    "$$\n",
    "\n",
    "$$\n",
    "v_{t} = \\nu \\Delta v - (uv_{x} + vv_{y})\n",
    "$$\n",
    "\n",
    "where,\n",
    "$\\nu = 0.005$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Technology Path\n",
    "\n",
    "MindFlow solves the problem as follows:\n",
    "\n",
    "1. Optimizer and One-step Training\n",
    "2. Model Construction\n",
    "3. Model training\n",
    "4. Model Evaluation and Visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mindspore import context, jit, nn, ops, save_checkpoint, set_seed\n",
    "import mindspore.common.dtype as mstype\n",
    "from mindflow.utils import load_yaml_config, print_log\n",
    "from src import RecurrentCNNCell, RecurrentCNNCellBurgers, Trainer, UpScaler, post_process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "set_seed(123456)\n",
    "np.random.seed(123456)\n",
    "context.set_context(mode=context.GRAPH_MODE, device_target=\"GPU\", device_id=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load configuration yaml\n",
    "config = load_yaml_config('./configs/data_driven_percnn_burgers.yaml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Optimizer and One-step Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train_stage(trainer, stage, pattern, config, ckpt_dir, use_ascend):\n",
    "    \"\"\"train stage\"\"\"\n",
    "    if use_ascend:\n",
    "        from mindspore.amp import DynamicLossScaler, all_finite\n",
    "        loss_scaler = DynamicLossScaler(2**10, 2, 100)\n",
    "\n",
    "    if 'milestone_num' in config.keys():\n",
    "        milestone = list([(config['epochs']//config['milestone_num'])*(i + 1)\n",
    "                          for i in range(config['milestone_num'])])\n",
    "        learning_rate = config['learning_rate']\n",
    "        lr = float(config['learning_rate'])*np.array(list([config['gamma']\n",
    "                                                           ** i for i in range(config['milestone_num'])]))\n",
    "        learning_rate = nn.piecewise_constant_lr(milestone, list(lr))\n",
    "    else:\n",
    "        learning_rate = config['learning_rate']\n",
    "\n",
    "    if stage == 'pretrain':\n",
    "        params = trainer.upconv.trainable_params()\n",
    "    else:\n",
    "        params = trainer.upconv.trainable_params() + trainer.recurrent_cnn.trainable_params()\n",
    "\n",
    "    optimizer = nn.Adam(params, learning_rate=learning_rate)\n",
    "\n",
    "    def forward_fn():\n",
    "        if stage == 'pretrain':\n",
    "            loss = trainer.get_ic_loss()\n",
    "        else:\n",
    "            loss = trainer.get_loss()\n",
    "        if use_ascend:\n",
    "            loss = loss_scaler.scale(loss)\n",
    "        return loss\n",
    "\n",
    "    if stage == 'pretrain':\n",
    "        grad_fn = ops.value_and_grad(forward_fn, None, params, has_aux=False)\n",
    "    else:\n",
    "        grad_fn = ops.value_and_grad(forward_fn, None, params, has_aux=True)\n",
    "\n",
    "    @jit\n",
    "    def train_step():\n",
    "        loss, grads = grad_fn()\n",
    "        if use_ascend:\n",
    "            loss = loss_scaler.unscale(loss)\n",
    "            is_finite = all_finite(grads)\n",
    "            if is_finite:\n",
    "                grads = loss_scaler.unscale(grads)\n",
    "                loss = ops.depend(loss, optimizer(grads))\n",
    "            loss_scaler.adjust(is_finite)\n",
    "        else:\n",
    "            loss = ops.depend(loss, optimizer(grads))\n",
    "        return loss\n",
    "\n",
    "    best_loss = 100000\n",
    "    for epoch in range(1, 1 + config['epochs']):\n",
    "        time_beg = time.time()\n",
    "        trainer.upconv.set_train(True)\n",
    "        trainer.recurrent_cnn.set_train(True)\n",
    "        if stage == 'pretrain':\n",
    "            step_train_loss = train_step()\n",
    "            print_log(\n",
    "                f\"epoch: {epoch} train loss: {step_train_loss} \\\n",
    "                    epoch time: {(time.time() - time_beg)*1000 :5.3f}ms \\\n",
    "                    step time: {(time.time() - time_beg)*1000 :5.3f}ms\")\n",
    "        else:\n",
    "            step_train_loss, loss_data, loss_ic, loss_phy, loss_valid = train_step()\n",
    "            print_log(f\"epoch: {epoch} train loss: {step_train_loss} ic_loss: {loss_ic} data_loss: {loss_data}\"\n",
    "                      f\"val_loss: {loss_valid} phy_loss: {loss_phy}\"\n",
    "                      f\"epoch time: {(time.time() - time_beg)*1000 :5.3f}ms\"\n",
    "                      f\"step time: {(time.time() - time_beg)*1000 :5.3f}ms\")\n",
    "            if step_train_loss < best_loss:\n",
    "                best_loss = step_train_loss\n",
    "                print_log('best loss', best_loss, 'save model')\n",
    "                save_checkpoint(trainer.upconv, os.path.join(ckpt_dir, f\"{pattern}_{config['name']}_upconv.ckpt\"))\n",
    "                save_checkpoint(trainer.recurrent_cnn,\n",
    "                                os.path.join(ckpt_dir, f\"{pattern}_{config['name']}_recurrent_cnn.ckpt\"))\n",
    "    if pattern == 'physics_driven':\n",
    "        trainer.recurrent_cnn.show_coef()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Model Construction\n",
    "\n",
    "PeRCNN is composed of two networks which are UpSclaer for upscaling and recurrent CNN as a backbone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train():\n",
    "    \"\"\"train\"\"\"\n",
    "    burgers_config = config\n",
    "\n",
    "    use_ascend = context.get_context(attr_key='device_target') == \"Ascend\"\n",
    "    print_log(f\"use_ascend: {use_ascend}\")\n",
    "\n",
    "    if use_ascend:\n",
    "        compute_dtype = mstype.float16\n",
    "    else:\n",
    "        compute_dtype = mstype.float32\n",
    "\n",
    "    data_config = burgers_config['data']\n",
    "    optimizer_config = burgers_config['optimizer']\n",
    "    model_config = burgers_config['model']\n",
    "    summary_config = burgers_config['summary']\n",
    "\n",
    "    upconv = UpScaler(in_channels=model_config['in_channels'],\n",
    "                      out_channels=model_config['out_channels'],\n",
    "                      hidden_channels=model_config['upscaler_hidden_channels'],\n",
    "                      kernel_size=model_config['kernel_size'],\n",
    "                      stride=model_config['stride'],\n",
    "                      has_bais=True)\n",
    "\n",
    "    if use_ascend:\n",
    "        from mindspore.amp import auto_mixed_precision\n",
    "        auto_mixed_precision(upconv, 'O1')\n",
    "\n",
    "    pattern = data_config['pattern']\n",
    "    if pattern == 'data_driven':\n",
    "        recurrent_cnn = RecurrentCNNCell(input_channels=model_config['in_channels'],\n",
    "                                         hidden_channels=model_config['rcnn_hidden_channels'],\n",
    "                                         kernel_size=model_config['kernel_size'],\n",
    "                                         compute_dtype=compute_dtype)\n",
    "    else:\n",
    "        recurrent_cnn = RecurrentCNNCellBurgers(kernel_size=model_config['kernel_size'],\n",
    "                                                init_coef=model_config['init_coef'],\n",
    "                                                compute_dtype=compute_dtype)\n",
    "\n",
    "    percnn_trainer = Trainer(upconv=upconv,\n",
    "                             recurrent_cnn=recurrent_cnn,\n",
    "                             timesteps_for_train=data_config['rollout_steps'],\n",
    "                             dx=data_config['dx'],\n",
    "                             dt=data_config['dy'],\n",
    "                             nu=data_config['nu'],\n",
    "                             data_path=os.path.join(data_config['root_dir'], data_config['file_name']),\n",
    "                             compute_dtype=compute_dtype)\n",
    "\n",
    "    ckpt_dir = os.path.join(summary_config[\"root_dir\"], summary_config['ckpt_dir'])\n",
    "    if not os.path.exists(ckpt_dir):\n",
    "        os.makedirs(ckpt_dir)\n",
    "\n",
    "    train_stage(percnn_trainer, 'pretrain', pattern, optimizer_config['pretrain'], ckpt_dir, use_ascend)\n",
    "    train_stage(percnn_trainer, 'finetune', pattern, optimizer_config['finetune'], ckpt_dir, use_ascend)\n",
    "    post_process(percnn_trainer, pattern)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "\n",
    "With **MindSpore version >= 2.0.0**, we can use the functional programming for training neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use_ascend: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ERROR] CORE(76358,7f2ea97a6200,python):2023-10-19-09:00:56.455.675 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_76358/972768667.py]\n",
      "[ERROR] CORE(76358,7f2ea97a6200,python):2023-10-19-09:00:56.455.714 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_76358/972768667.py]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 train loss: 1.5724593 epoch time: 0.867 s\n",
      "epoch: 2 train loss: 1.5299724 epoch time: 0.002 s\n",
      "epoch: 3 train loss: 1.4901378 epoch time: 0.002 s\n",
      "epoch: 4 train loss: 1.449844 epoch time: 0.002 s\n",
      "epoch: 5 train loss: 1.4070688 epoch time: 0.002 s\n",
      "epoch: 6 train loss: 1.3605155 epoch time: 0.002 s\n",
      "epoch: 7 train loss: 1.3093143 epoch time: 0.002 s\n",
      "epoch: 8 train loss: 1.253143 epoch time: 0.002 s\n",
      "epoch: 9 train loss: 1.1923409 epoch time: 0.002 s\n",
      "epoch: 10 train loss: 1.1278089 epoch time: 0.002 s\n",
      "epoch: 11 train loss: 1.0608313 epoch time: 0.001 s\n",
      "epoch: 12 train loss: 0.99293244 epoch time: 0.001 s\n",
      "epoch: 13 train loss: 0.9257586 epoch time: 0.001 s\n",
      "epoch: 14 train loss: 0.8609383 epoch time: 0.001 s\n",
      "epoch: 15 train loss: 0.79991245 epoch time: 0.001 s\n",
      "epoch: 16 train loss: 0.7437653 epoch time: 0.002 s\n",
      "epoch: 17 train loss: 0.69311035 epoch time: 0.002 s\n",
      "epoch: 18 train loss: 0.64807683 epoch time: 0.002 s\n",
      "epoch: 19 train loss: 0.6083956 epoch time: 0.002 s\n",
      "epoch: 20 train loss: 0.57353437 epoch time: 0.002 s\n",
      "epoch: 21 train loss: 0.54282933 epoch time: 0.002 s\n",
      "epoch: 22 train loss: 0.51557946 epoch time: 0.002 s\n",
      "epoch: 23 train loss: 0.49110448 epoch time: 0.002 s\n",
      "epoch: 24 train loss: 0.46878013 epoch time: 0.002 s\n",
      "epoch: 25 train loss: 0.44806084 epoch time: 0.002 s\n",
      "epoch: 26 train loss: 0.42849916 epoch time: 0.002 s\n",
      "epoch: 27 train loss: 0.40976158 epoch time: 0.002 s\n",
      "epoch: 28 train loss: 0.39164323 epoch time: 0.002 s\n",
      "epoch: 29 train loss: 0.3740813 epoch time: 0.002 s\n",
      "epoch: 30 train loss: 0.357165 epoch time: 0.007 s\n",
      "epoch: 31 train loss: 0.3411376 epoch time: 0.003 s\n",
      "epoch: 32 train loss: 0.32637975 epoch time: 0.002 s\n",
      "epoch: 33 train loss: 0.3133565 epoch time: 0.002 s\n",
      "epoch: 34 train loss: 0.3025042 epoch time: 0.002 s\n",
      "epoch: 35 train loss: 0.2940462 epoch time: 0.002 s\n",
      "epoch: 36 train loss: 0.2877709 epoch time: 0.001 s\n",
      "epoch: 37 train loss: 0.28290102 epoch time: 0.001 s\n",
      "epoch: 38 train loss: 0.27824152 epoch time: 0.001 s\n",
      "epoch: 39 train loss: 0.2726494 epoch time: 0.001 s\n",
      "epoch: 40 train loss: 0.26554668 epoch time: 0.001 s\n",
      "epoch: 41 train loss: 0.25710905 epoch time: 0.002 s\n",
      "epoch: 42 train loss: 0.24805647 epoch time: 0.002 s\n",
      "epoch: 43 train loss: 0.23925379 epoch time: 0.002 s\n",
      "epoch: 44 train loss: 0.23135479 epoch time: 0.002 s\n",
      "epoch: 45 train loss: 0.22461845 epoch time: 0.002 s\n",
      "epoch: 46 train loss: 0.21891639 epoch time: 0.002 s\n",
      "epoch: 47 train loss: 0.21387422 epoch time: 0.002 s\n",
      "epoch: 48 train loss: 0.20905314 epoch time: 0.002 s\n",
      "epoch: 49 train loss: 0.20409666 epoch time: 0.002 s\n",
      "epoch: 50 train loss: 0.19881195 epoch time: 0.002 s\n",
      "epoch: 51 train loss: 0.1931911 epoch time: 0.002 s\n",
      "epoch: 52 train loss: 0.18738899 epoch time: 0.002 s\n",
      "epoch: 53 train loss: 0.18166895 epoch time: 0.002 s\n",
      "epoch: 54 train loss: 0.17632408 epoch time: 0.002 s\n",
      "epoch: 55 train loss: 0.17158397 epoch time: 0.002 s\n",
      "epoch: 56 train loss: 0.16753128 epoch time: 0.002 s\n",
      "epoch: 57 train loss: 0.16406536 epoch time: 0.002 s\n",
      "epoch: 58 train loss: 0.16094452 epoch time: 0.002 s\n",
      "epoch: 59 train loss: 0.15790188 epoch time: 0.002 s\n",
      "epoch: 60 train loss: 0.15477598 epoch time: 0.002 s\n",
      "epoch: 61 train loss: 0.15157954 epoch time: 0.002 s\n",
      "epoch: 62 train loss: 0.14846873 epoch time: 0.002 s\n",
      "epoch: 63 train loss: 0.145642 epoch time: 0.002 s\n",
      "epoch: 64 train loss: 0.14323543 epoch time: 0.002 s\n",
      "epoch: 65 train loss: 0.14126863 epoch time: 0.002 s\n",
      "epoch: 66 train loss: 0.13965559 epoch time: 0.002 s\n",
      "epoch: 67 train loss: 0.13825975 epoch time: 0.002 s\n",
      "epoch: 68 train loss: 0.13695695 epoch time: 0.002 s\n",
      "epoch: 69 train loss: 0.13567837 epoch time: 0.002 s\n",
      "epoch: 70 train loss: 0.13442115 epoch time: 0.002 s\n",
      "epoch: 71 train loss: 0.13322943 epoch time: 0.001 s\n",
      "epoch: 72 train loss: 0.13215823 epoch time: 0.002 s\n",
      "epoch: 73 train loss: 0.1312373 epoch time: 0.001 s\n",
      "epoch: 74 train loss: 0.13045141 epoch time: 0.001 s\n",
      "epoch: 75 train loss: 0.12974598 epoch time: 0.001 s\n",
      "epoch: 76 train loss: 0.12905371 epoch time: 0.001 s\n",
      "epoch: 77 train loss: 0.12832664 epoch time: 0.001 s\n",
      "epoch: 78 train loss: 0.12755427 epoch time: 0.001 s\n",
      "epoch: 79 train loss: 0.12675957 epoch time: 0.001 s\n",
      "epoch: 80 train loss: 0.12597847 epoch time: 0.001 s\n",
      "epoch: 81 train loss: 0.12523702 epoch time: 0.001 s\n",
      "epoch: 82 train loss: 0.124539256 epoch time: 0.001 s\n",
      "epoch: 83 train loss: 0.123869605 epoch time: 0.001 s\n",
      "epoch: 84 train loss: 0.123204984 epoch time: 0.001 s\n",
      "epoch: 85 train loss: 0.1225287 epoch time: 0.001 s\n",
      "epoch: 86 train loss: 0.12183832 epoch time: 0.001 s\n",
      "epoch: 87 train loss: 0.12114503 epoch time: 0.001 s\n",
      "epoch: 88 train loss: 0.12046631 epoch time: 0.001 s\n",
      "epoch: 89 train loss: 0.119816385 epoch time: 0.001 s\n",
      "epoch: 90 train loss: 0.11919978 epoch time: 0.001 s\n",
      "epoch: 91 train loss: 0.11861051 epoch time: 0.001 s\n",
      "epoch: 92 train loss: 0.11803673 epoch time: 0.001 s\n",
      "epoch: 93 train loss: 0.11746741 epoch time: 0.001 s\n",
      "epoch: 94 train loss: 0.116897196 epoch time: 0.001 s\n",
      "epoch: 95 train loss: 0.116327345 epoch time: 0.001 s\n",
      "epoch: 96 train loss: 0.115762696 epoch time: 0.001 s\n",
      "epoch: 97 train loss: 0.11520728 epoch time: 0.001 s\n",
      "epoch: 98 train loss: 0.11466124 epoch time: 0.001 s\n",
      "epoch: 99 train loss: 0.11412031 epoch time: 0.001 s\n",
      "epoch: 100 train loss: 0.113577634 epoch time: 0.001 s\n",
      "epoch: 101 train loss: 0.11302667 epoch time: 0.001 s\n",
      "epoch: 102 train loss: 0.11246343 epoch time: 0.001 s\n",
      "epoch: 103 train loss: 0.111887 epoch time: 0.001 s\n",
      "epoch: 104 train loss: 0.1112987 epoch time: 0.001 s\n",
      "epoch: 105 train loss: 0.11070024 epoch time: 0.001 s\n",
      "epoch: 106 train loss: 0.110092245 epoch time: 0.001 s\n",
      "epoch: 107 train loss: 0.109473646 epoch time: 0.001 s\n",
      "epoch: 108 train loss: 0.1088422 epoch time: 0.001 s\n",
      "epoch: 109 train loss: 0.10819558 epoch time: 0.001 s\n",
      "epoch: 110 train loss: 0.10753235 epoch time: 0.001 s\n",
      "epoch: 111 train loss: 0.10685242 epoch time: 0.001 s\n",
      "epoch: 112 train loss: 0.106156625 epoch time: 0.001 s\n",
      "epoch: 113 train loss: 0.10544601 epoch time: 0.001 s\n",
      "epoch: 114 train loss: 0.104721054 epoch time: 0.001 s\n",
      "epoch: 115 train loss: 0.10398147 epoch time: 0.001 s\n",
      "epoch: 116 train loss: 0.103226334 epoch time: 0.001 s\n",
      "epoch: 117 train loss: 0.10245458 epoch time: 0.001 s\n",
      "epoch: 118 train loss: 0.10166553 epoch time: 0.001 s\n",
      "epoch: 119 train loss: 0.10085907 epoch time: 0.001 s\n",
      "epoch: 120 train loss: 0.10003561 epoch time: 0.001 s\n",
      "epoch: 121 train loss: 0.09919575 epoch time: 0.001 s\n",
      "epoch: 122 train loss: 0.09834001 epoch time: 0.001 s\n",
      "epoch: 123 train loss: 0.09746871 epoch time: 0.001 s\n",
      "epoch: 124 train loss: 0.09658209 epoch time: 0.001 s\n",
      "epoch: 125 train loss: 0.09568048 epoch time: 0.002 s\n",
      "epoch: 126 train loss: 0.09476456 epoch time: 0.001 s\n",
      "epoch: 127 train loss: 0.09383546 epoch time: 0.001 s\n",
      "epoch: 128 train loss: 0.09289474 epoch time: 0.001 s\n",
      "epoch: 129 train loss: 0.091944225 epoch time: 0.001 s\n",
      "epoch: 130 train loss: 0.09098588 epoch time: 0.001 s\n",
      "epoch: 131 train loss: 0.09002173 epoch time: 0.001 s\n",
      "epoch: 132 train loss: 0.08905391 epoch time: 0.001 s\n",
      "epoch: 133 train loss: 0.08808468 epoch time: 0.001 s\n",
      "epoch: 134 train loss: 0.08711653 epoch time: 0.001 s\n",
      "epoch: 135 train loss: 0.08615225 epoch time: 0.001 s\n",
      "epoch: 136 train loss: 0.0851947 epoch time: 0.001 s\n",
      "epoch: 137 train loss: 0.08424695 epoch time: 0.001 s\n",
      "epoch: 138 train loss: 0.08331201 epoch time: 0.001 s\n",
      "epoch: 139 train loss: 0.082392834 epoch time: 0.001 s\n",
      "epoch: 140 train loss: 0.081492215 epoch time: 0.001 s\n",
      "epoch: 141 train loss: 0.080612816 epoch time: 0.001 s\n",
      "epoch: 142 train loss: 0.079757065 epoch time: 0.001 s\n",
      "epoch: 143 train loss: 0.07892716 epoch time: 0.001 s\n",
      "epoch: 144 train loss: 0.07812495 epoch time: 0.001 s\n",
      "epoch: 145 train loss: 0.07735187 epoch time: 0.001 s\n",
      "epoch: 146 train loss: 0.07660883 epoch time: 0.001 s\n",
      "epoch: 147 train loss: 0.07589619 epoch time: 0.001 s\n",
      "epoch: 148 train loss: 0.075213626 epoch time: 0.001 s\n",
      "epoch: 149 train loss: 0.07456027 epoch time: 0.001 s\n",
      "epoch: 150 train loss: 0.07393459 epoch time: 0.001 s\n",
      "epoch: 151 train loss: 0.073334605 epoch time: 0.001 s\n",
      "epoch: 152 train loss: 0.07275779 epoch time: 0.001 s\n",
      "epoch: 153 train loss: 0.07220135 epoch time: 0.001 s\n",
      "epoch: 154 train loss: 0.071662195 epoch time: 0.001 s\n",
      "epoch: 155 train loss: 0.07113721 epoch time: 0.001 s\n",
      "epoch: 156 train loss: 0.07062334 epoch time: 0.001 s\n",
      "epoch: 157 train loss: 0.07011775 epoch time: 0.001 s\n",
      "epoch: 158 train loss: 0.069617964 epoch time: 0.001 s\n",
      "epoch: 159 train loss: 0.06912199 epoch time: 0.001 s\n",
      "epoch: 160 train loss: 0.06862836 epoch time: 0.001 s\n",
      "epoch: 161 train loss: 0.06813616 epoch time: 0.001 s\n",
      "epoch: 162 train loss: 0.067644976 epoch time: 0.001 s\n",
      "epoch: 163 train loss: 0.06715485 epoch time: 0.001 s\n",
      "epoch: 164 train loss: 0.066666156 epoch time: 0.001 s\n",
      "epoch: 165 train loss: 0.06617953 epoch time: 0.001 s\n",
      "epoch: 166 train loss: 0.0656957 epoch time: 0.001 s\n",
      "epoch: 167 train loss: 0.06521546 epoch time: 0.001 s\n",
      "epoch: 168 train loss: 0.06473949 epoch time: 0.001 s\n",
      "epoch: 169 train loss: 0.064268366 epoch time: 0.001 s\n",
      "epoch: 170 train loss: 0.06380246 epoch time: 0.001 s\n",
      "epoch: 171 train loss: 0.06334197 epoch time: 0.001 s\n",
      "epoch: 172 train loss: 0.062886916 epoch time: 0.001 s\n",
      "epoch: 173 train loss: 0.06243714 epoch time: 0.001 s\n",
      "epoch: 174 train loss: 0.06199239 epoch time: 0.001 s\n",
      "epoch: 175 train loss: 0.061552312 epoch time: 0.001 s\n",
      "epoch: 176 train loss: 0.061116554 epoch time: 0.001 s\n",
      "epoch: 177 train loss: 0.06068474 epoch time: 0.001 s\n",
      "epoch: 178 train loss: 0.060256563 epoch time: 0.001 s\n",
      "epoch: 179 train loss: 0.059831765 epoch time: 0.001 s\n",
      "epoch: 180 train loss: 0.05941017 epoch time: 0.001 s\n",
      "epoch: 181 train loss: 0.05899168 epoch time: 0.001 s\n",
      "epoch: 182 train loss: 0.058576282 epoch time: 0.001 s\n",
      "epoch: 183 train loss: 0.058164008 epoch time: 0.001 s\n",
      "epoch: 184 train loss: 0.05775493 epoch time: 0.001 s\n",
      "epoch: 185 train loss: 0.057349183 epoch time: 0.001 s\n",
      "epoch: 186 train loss: 0.05694685 epoch time: 0.001 s\n",
      "epoch: 187 train loss: 0.05654805 epoch time: 0.001 s\n",
      "epoch: 188 train loss: 0.05615285 epoch time: 0.001 s\n",
      "epoch: 189 train loss: 0.055761278 epoch time: 0.001 s\n",
      "epoch: 190 train loss: 0.05537336 epoch time: 0.001 s\n",
      "epoch: 191 train loss: 0.05498904 epoch time: 0.001 s\n",
      "epoch: 192 train loss: 0.05460827 epoch time: 0.001 s\n",
      "epoch: 193 train loss: 0.054230977 epoch time: 0.001 s\n",
      "epoch: 194 train loss: 0.053857055 epoch time: 0.001 s\n",
      "epoch: 195 train loss: 0.053486414 epoch time: 0.001 s\n",
      "epoch: 196 train loss: 0.05311897 epoch time: 0.001 s\n",
      "epoch: 197 train loss: 0.052754644 epoch time: 0.001 s\n",
      "epoch: 198 train loss: 0.052393373 epoch time: 0.001 s\n",
      "epoch: 199 train loss: 0.05203512 epoch time: 0.001 s\n",
      "epoch: 200 train loss: 0.051679853 epoch time: 0.002 s\n",
      "epoch: 201 train loss: 0.051327553 epoch time: 0.002 s\n",
      "epoch: 202 train loss: 0.050978187 epoch time: 0.002 s\n",
      "epoch: 203 train loss: 0.05063177 epoch time: 0.002 s\n",
      "epoch: 204 train loss: 0.05028826 epoch time: 0.002 s\n",
      "epoch: 205 train loss: 0.049947653 epoch time: 0.002 s\n",
      "epoch: 206 train loss: 0.04960991 epoch time: 0.002 s\n",
      "epoch: 207 train loss: 0.049275022 epoch time: 0.002 s\n",
      "epoch: 208 train loss: 0.048942946 epoch time: 0.002 s\n",
      "epoch: 209 train loss: 0.04861365 epoch time: 0.001 s\n",
      "epoch: 210 train loss: 0.048287127 epoch time: 0.001 s\n",
      "epoch: 211 train loss: 0.04796335 epoch time: 0.001 s\n",
      "epoch: 212 train loss: 0.047642317 epoch time: 0.001 s\n",
      "epoch: 213 train loss: 0.047324028 epoch time: 0.001 s\n",
      "epoch: 214 train loss: 0.04700849 epoch time: 0.001 s\n",
      "epoch: 215 train loss: 0.046695717 epoch time: 0.001 s\n",
      "epoch: 216 train loss: 0.04638573 epoch time: 0.001 s\n",
      "epoch: 217 train loss: 0.046078544 epoch time: 0.001 s\n",
      "epoch: 218 train loss: 0.04577417 epoch time: 0.002 s\n",
      "epoch: 219 train loss: 0.04547262 epoch time: 0.002 s\n",
      "epoch: 220 train loss: 0.045173906 epoch time: 0.002 s\n",
      "epoch: 221 train loss: 0.044878013 epoch time: 0.001 s\n",
      "epoch: 222 train loss: 0.044584945 epoch time: 0.001 s\n",
      "epoch: 223 train loss: 0.044294685 epoch time: 0.001 s\n",
      "epoch: 224 train loss: 0.044007223 epoch time: 0.001 s\n",
      "epoch: 225 train loss: 0.04372256 epoch time: 0.001 s\n",
      "epoch: 226 train loss: 0.04344067 epoch time: 0.001 s\n",
      "epoch: 227 train loss: 0.043161564 epoch time: 0.001 s\n",
      "epoch: 228 train loss: 0.04288522 epoch time: 0.001 s\n",
      "epoch: 229 train loss: 0.042611662 epoch time: 0.001 s\n",
      "epoch: 230 train loss: 0.04234087 epoch time: 0.001 s\n",
      "epoch: 231 train loss: 0.042072866 epoch time: 0.001 s\n",
      "epoch: 232 train loss: 0.04180764 epoch time: 0.001 s\n",
      "epoch: 233 train loss: 0.04154521 epoch time: 0.001 s\n",
      "epoch: 234 train loss: 0.041285552 epoch time: 0.001 s\n",
      "epoch: 235 train loss: 0.0410287 epoch time: 0.001 s\n",
      "epoch: 236 train loss: 0.040774632 epoch time: 0.001 s\n",
      "epoch: 237 train loss: 0.04052334 epoch time: 0.001 s\n",
      "epoch: 238 train loss: 0.040274836 epoch time: 0.001 s\n",
      "epoch: 239 train loss: 0.040029097 epoch time: 0.001 s\n",
      "epoch: 240 train loss: 0.039786126 epoch time: 0.001 s\n",
      "epoch: 241 train loss: 0.039545923 epoch time: 0.001 s\n",
      "epoch: 242 train loss: 0.03930846 epoch time: 0.001 s\n",
      "epoch: 243 train loss: 0.039073743 epoch time: 0.001 s\n",
      "epoch: 244 train loss: 0.038841773 epoch time: 0.001 s\n",
      "epoch: 245 train loss: 0.038612526 epoch time: 0.001 s\n",
      "epoch: 246 train loss: 0.038386 epoch time: 0.001 s\n",
      "epoch: 247 train loss: 0.038162183 epoch time: 0.001 s\n",
      "epoch: 248 train loss: 0.03794107 epoch time: 0.001 s\n",
      "epoch: 249 train loss: 0.037722632 epoch time: 0.001 s\n",
      "epoch: 250 train loss: 0.037506875 epoch time: 0.001 s\n",
      "epoch: 251 train loss: 0.037293766 epoch time: 0.001 s\n",
      "epoch: 252 train loss: 0.037083294 epoch time: 0.001 s\n",
      "epoch: 253 train loss: 0.036875453 epoch time: 0.001 s\n",
      "epoch: 254 train loss: 0.03667021 epoch time: 0.001 s\n",
      "epoch: 255 train loss: 0.036467556 epoch time: 0.001 s\n",
      "epoch: 256 train loss: 0.036267467 epoch time: 0.001 s\n",
      "epoch: 257 train loss: 0.036069922 epoch time: 0.001 s\n",
      "epoch: 258 train loss: 0.035874907 epoch time: 0.001 s\n",
      "epoch: 259 train loss: 0.03568239 epoch time: 0.001 s\n",
      "epoch: 260 train loss: 0.035492357 epoch time: 0.001 s\n",
      "epoch: 261 train loss: 0.035304774 epoch time: 0.001 s\n",
      "epoch: 262 train loss: 0.03511962 epoch time: 0.001 s\n",
      "epoch: 263 train loss: 0.034936868 epoch time: 0.001 s\n",
      "epoch: 264 train loss: 0.034756493 epoch time: 0.001 s\n",
      "epoch: 265 train loss: 0.03457846 epoch time: 0.001 s\n",
      "epoch: 266 train loss: 0.034402747 epoch time: 0.001 s\n",
      "epoch: 267 train loss: 0.034229323 epoch time: 0.001 s\n",
      "epoch: 268 train loss: 0.03405816 epoch time: 0.001 s\n",
      "epoch: 269 train loss: 0.033889223 epoch time: 0.001 s\n",
      "epoch: 270 train loss: 0.03372247 epoch time: 0.001 s\n",
      "epoch: 271 train loss: 0.033557884 epoch time: 0.001 s\n",
      "epoch: 272 train loss: 0.03339543 epoch time: 0.001 s\n",
      "epoch: 273 train loss: 0.03323506 epoch time: 0.001 s\n",
      "epoch: 274 train loss: 0.033076752 epoch time: 0.001 s\n",
      "epoch: 275 train loss: 0.03292047 epoch time: 0.001 s\n",
      "epoch: 276 train loss: 0.03276617 epoch time: 0.001 s\n",
      "epoch: 277 train loss: 0.032613825 epoch time: 0.001 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5862 train loss: 0.00017873458 epoch time: 0.001 s\n",
      "epoch: 5863 train loss: 0.00017868946 epoch time: 0.001 s\n",
      "epoch: 5864 train loss: 0.00017864427 epoch time: 0.001 s\n",
      "epoch: 5865 train loss: 0.00017859896 epoch time: 0.001 s\n",
      "epoch: 5866 train loss: 0.00017855378 epoch time: 0.001 s\n",
      "epoch: 5867 train loss: 0.00017850856 epoch time: 0.001 s\n",
      "epoch: 5868 train loss: 0.00017846328 epoch time: 0.001 s\n",
      "epoch: 5869 train loss: 0.00017841818 epoch time: 0.001 s\n",
      "epoch: 5870 train loss: 0.000178373 epoch time: 0.001 s\n",
      "epoch: 5871 train loss: 0.0001783278 epoch time: 0.001 s\n",
      "epoch: 5872 train loss: 0.00017828264 epoch time: 0.001 s\n",
      "epoch: 5873 train loss: 0.00017823742 epoch time: 0.001 s\n",
      "epoch: 5874 train loss: 0.0001781923 epoch time: 0.001 s\n",
      "epoch: 5875 train loss: 0.00017814718 epoch time: 0.001 s\n",
      "epoch: 5876 train loss: 0.00017810194 epoch time: 0.001 s\n",
      "epoch: 5877 train loss: 0.00017805678 epoch time: 0.001 s\n",
      "epoch: 5878 train loss: 0.00017801175 epoch time: 0.001 s\n",
      "epoch: 5879 train loss: 0.00017796655 epoch time: 0.001 s\n",
      "epoch: 5880 train loss: 0.0001779214 epoch time: 0.001 s\n",
      "epoch: 5881 train loss: 0.0001778763 epoch time: 0.001 s\n",
      "epoch: 5882 train loss: 0.00017783123 epoch time: 0.001 s\n",
      "epoch: 5883 train loss: 0.00017778616 epoch time: 0.001 s\n",
      "epoch: 5884 train loss: 0.00017774105 epoch time: 0.001 s\n",
      "epoch: 5885 train loss: 0.00017769601 epoch time: 0.001 s\n",
      "epoch: 5886 train loss: 0.00017765095 epoch time: 0.001 s\n",
      "epoch: 5887 train loss: 0.00017760605 epoch time: 0.001 s\n",
      "epoch: 5888 train loss: 0.00017756136 epoch time: 0.001 s\n",
      "epoch: 5889 train loss: 0.00017751653 epoch time: 0.001 s\n",
      "epoch: 5890 train loss: 0.00017747216 epoch time: 0.001 s\n",
      "epoch: 5891 train loss: 0.00017742805 epoch time: 0.001 s\n",
      "epoch: 5892 train loss: 0.00017738467 epoch time: 0.001 s\n",
      "epoch: 5893 train loss: 0.00017734226 epoch time: 0.001 s\n",
      "epoch: 5894 train loss: 0.00017730152 epoch time: 0.001 s\n",
      "epoch: 5895 train loss: 0.00017726362 epoch time: 0.001 s\n",
      "epoch: 5896 train loss: 0.00017723037 epoch time: 0.001 s\n",
      "epoch: 5897 train loss: 0.00017720491 epoch time: 0.001 s\n",
      "epoch: 5898 train loss: 0.00017719281 epoch time: 0.001 s\n",
      "epoch: 5899 train loss: 0.00017720344 epoch time: 0.001 s\n",
      "epoch: 5900 train loss: 0.00017725298 epoch time: 0.001 s\n",
      "epoch: 5901 train loss: 0.00017736998 epoch time: 0.001 s\n",
      "epoch: 5902 train loss: 0.00017760496 epoch time: 0.001 s\n",
      "epoch: 5903 train loss: 0.00017804513 epoch time: 0.001 s\n",
      "epoch: 5904 train loss: 0.00017885077 epoch time: 0.001 s\n",
      "epoch: 5905 train loss: 0.00018029695 epoch time: 0.001 s\n",
      "epoch: 5906 train loss: 0.00018290235 epoch time: 0.001 s\n",
      "epoch: 5907 train loss: 0.00018754211 epoch time: 0.001 s\n",
      "epoch: 5908 train loss: 0.00019592045 epoch time: 0.001 s\n",
      "epoch: 5909 train loss: 0.0002107625 epoch time: 0.001 s\n",
      "epoch: 5910 train loss: 0.00023757138 epoch time: 0.001 s\n",
      "epoch: 5911 train loss: 0.00028389142 epoch time: 0.001 s\n",
      "epoch: 5912 train loss: 0.0003652773 epoch time: 0.001 s\n",
      "epoch: 5913 train loss: 0.00049293425 epoch time: 0.001 s\n",
      "epoch: 5914 train loss: 0.0006892403 epoch time: 0.001 s\n",
      "epoch: 5915 train loss: 0.0009036317 epoch time: 0.001 s\n",
      "epoch: 5916 train loss: 0.0010791365 epoch time: 0.001 s\n",
      "epoch: 5917 train loss: 0.0009868235 epoch time: 0.001 s\n",
      "epoch: 5918 train loss: 0.00066104677 epoch time: 0.001 s\n",
      "epoch: 5919 train loss: 0.0002852385 epoch time: 0.001 s\n",
      "epoch: 5920 train loss: 0.00018740365 epoch time: 0.001 s\n",
      "epoch: 5921 train loss: 0.0003762292 epoch time: 0.001 s\n",
      "epoch: 5922 train loss: 0.0005760202 epoch time: 0.001 s\n",
      "epoch: 5923 train loss: 0.000558066 epoch time: 0.001 s\n",
      "epoch: 5924 train loss: 0.00033339724 epoch time: 0.001 s\n",
      "epoch: 5925 train loss: 0.00018118932 epoch time: 0.001 s\n",
      "epoch: 5926 train loss: 0.00024368358 epoch time: 0.001 s\n",
      "epoch: 5927 train loss: 0.00038230172 epoch time: 0.001 s\n",
      "epoch: 5928 train loss: 0.00039467216 epoch time: 0.001 s\n",
      "epoch: 5929 train loss: 0.00026476063 epoch time: 0.001 s\n",
      "epoch: 5930 train loss: 0.00017756656 epoch time: 0.001 s\n",
      "epoch: 5931 train loss: 0.00022597761 epoch time: 0.001 s\n",
      "epoch: 5932 train loss: 0.00030594956 epoch time: 0.001 s\n",
      "epoch: 5933 train loss: 0.0002931421 epoch time: 0.001 s\n",
      "epoch: 5934 train loss: 0.00020983303 epoch time: 0.001 s\n",
      "epoch: 5935 train loss: 0.00017779617 epoch time: 0.001 s\n",
      "epoch: 5936 train loss: 0.00022356462 epoch time: 0.001 s\n",
      "epoch: 5937 train loss: 0.0002600992 epoch time: 0.001 s\n",
      "epoch: 5938 train loss: 0.00023202146 epoch time: 0.001 s\n",
      "epoch: 5939 train loss: 0.00018401966 epoch time: 0.001 s\n",
      "epoch: 5940 train loss: 0.00018259157 epoch time: 0.001 s\n",
      "epoch: 5941 train loss: 0.00021608724 epoch time: 0.001 s\n",
      "epoch: 5942 train loss: 0.00022612284 epoch time: 0.001 s\n",
      "epoch: 5943 train loss: 0.00019940684 epoch time: 0.001 s\n",
      "epoch: 5944 train loss: 0.00017642637 epoch time: 0.001 s\n",
      "epoch: 5945 train loss: 0.00018583192 epoch time: 0.001 s\n",
      "epoch: 5946 train loss: 0.0002053247 epoch time: 0.001 s\n",
      "epoch: 5947 train loss: 0.00020258316 epoch time: 0.001 s\n",
      "epoch: 5948 train loss: 0.0001835046 epoch time: 0.001 s\n",
      "epoch: 5949 train loss: 0.00017577168 epoch time: 0.001 s\n",
      "epoch: 5950 train loss: 0.00018622554 epoch time: 0.001 s\n",
      "epoch: 5951 train loss: 0.00019509612 epoch time: 0.001 s\n",
      "epoch: 5952 train loss: 0.00018861495 epoch time: 0.001 s\n",
      "epoch: 5953 train loss: 0.00017738927 epoch time: 0.001 s\n",
      "epoch: 5954 train loss: 0.00017653534 epoch time: 0.001 s\n",
      "epoch: 5955 train loss: 0.00018412332 epoch time: 0.001 s\n",
      "epoch: 5956 train loss: 0.00018711686 epoch time: 0.001 s\n",
      "epoch: 5957 train loss: 0.000181274 epoch time: 0.001 s\n",
      "epoch: 5958 train loss: 0.00017544796 epoch time: 0.001 s\n",
      "epoch: 5959 train loss: 0.00017678422 epoch time: 0.001 s\n",
      "epoch: 5960 train loss: 0.00018139958 epoch time: 0.001 s\n",
      "epoch: 5961 train loss: 0.0001818038 epoch time: 0.001 s\n",
      "epoch: 5962 train loss: 0.0001776499 epoch time: 0.001 s\n",
      "epoch: 5963 train loss: 0.00017489426 epoch time: 0.001 s\n",
      "epoch: 5964 train loss: 0.00017648055 epoch time: 0.001 s\n",
      "epoch: 5965 train loss: 0.00017903089 epoch time: 0.001 s\n",
      "epoch: 5966 train loss: 0.00017859435 epoch time: 0.001 s\n",
      "epoch: 5967 train loss: 0.00017594424 epoch time: 0.001 s\n",
      "epoch: 5968 train loss: 0.00017467099 epoch time: 0.001 s\n",
      "epoch: 5969 train loss: 0.00017587883 epoch time: 0.001 s\n",
      "epoch: 5970 train loss: 0.00017724505 epoch time: 0.001 s\n",
      "epoch: 5971 train loss: 0.00017672038 epoch time: 0.001 s\n",
      "epoch: 5972 train loss: 0.0001751023 epoch time: 0.001 s\n",
      "epoch: 5973 train loss: 0.00017446878 epoch time: 0.001 s\n",
      "epoch: 5974 train loss: 0.00017524564 epoch time: 0.001 s\n",
      "epoch: 5975 train loss: 0.00017599051 epoch time: 0.001 s\n",
      "epoch: 5976 train loss: 0.00017559691 epoch time: 0.001 s\n",
      "epoch: 5977 train loss: 0.00017461884 epoch time: 0.001 s\n",
      "epoch: 5978 train loss: 0.00017425422 epoch time: 0.001 s\n",
      "epoch: 5979 train loss: 0.00017470632 epoch time: 0.001 s\n",
      "epoch: 5980 train loss: 0.00017512836 epoch time: 0.001 s\n",
      "epoch: 5981 train loss: 0.00017488474 epoch time: 0.001 s\n",
      "epoch: 5982 train loss: 0.00017428873 epoch time: 0.001 s\n",
      "epoch: 5983 train loss: 0.00017403346 epoch time: 0.001 s\n",
      "epoch: 5984 train loss: 0.00017426563 epoch time: 0.001 s\n",
      "epoch: 5985 train loss: 0.00017451524 epoch time: 0.001 s\n",
      "epoch: 5986 train loss: 0.00017438717 epoch time: 0.001 s\n",
      "epoch: 5987 train loss: 0.00017402103 epoch time: 0.001 s\n",
      "epoch: 5988 train loss: 0.00017381944 epoch time: 0.001 s\n",
      "epoch: 5989 train loss: 0.00017391308 epoch time: 0.001 s\n",
      "epoch: 5990 train loss: 0.00017405982 epoch time: 0.001 s\n",
      "epoch: 5991 train loss: 0.00017400463 epoch time: 0.001 s\n",
      "epoch: 5992 train loss: 0.00017378097 epoch time: 0.001 s\n",
      "epoch: 5993 train loss: 0.00017361519 epoch time: 0.001 s\n",
      "epoch: 5994 train loss: 0.00017362367 epoch time: 0.001 s\n",
      "epoch: 5995 train loss: 0.00017370074 epoch time: 0.001 s\n",
      "epoch: 5996 train loss: 0.00017368408 epoch time: 0.001 s\n",
      "epoch: 5997 train loss: 0.00017355102 epoch time: 0.001 s\n",
      "epoch: 5998 train loss: 0.00017341717 epoch time: 0.001 s\n",
      "epoch: 5999 train loss: 0.0001733772 epoch time: 0.001 s\n",
      "epoch: 6000 train loss: 0.00017340294 epoch time: 0.001 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 train loss: 0.0040010856 ic_loss: 0.00017339904 data_loss: 0.0036542874 val_loss: 0.034989584 phy_loss: 385.93723 epoch time:  14.898 s\n",
      "best loss 0.0040010856 save model\n",
      "epoch: 2 train loss: 0.023029208 ic_loss: 0.0069433 data_loss: 0.009142607 val_loss: 0.035638213 phy_loss: 416.80725 epoch time:  0.247 s\n",
      "epoch: 3 train loss: 0.09626201 ic_loss: 0.030940203 data_loss: 0.0343816 val_loss: 0.05810566 phy_loss: 221.00093 epoch time:  0.162 s\n",
      "epoch: 4 train loss: 0.01788263 ic_loss: 0.0053461124 data_loss: 0.0071904045 val_loss: 0.03353381 phy_loss: 301.05966 epoch time:  0.147 s\n",
      "epoch: 5 train loss: 0.029557336 ic_loss: 0.0091625415 data_loss: 0.011232254 val_loss: 0.038305752 phy_loss: 449.9107 epoch time:  0.152 s\n",
      "epoch: 6 train loss: 0.052337468 ic_loss: 0.016626468 data_loss: 0.019084534 val_loss: 0.046096146 phy_loss: 497.9761 epoch time:  0.214 s\n",
      "epoch: 7 train loss: 0.014262615 ic_loss: 0.004195284 data_loss: 0.005872047 val_loss: 0.03377932 phy_loss: 430.3675 epoch time:  0.151 s\n",
      "epoch: 8 train loss: 0.00919872 ic_loss: 0.0025033113 data_loss: 0.0041920976 val_loss: 0.031886213 phy_loss: 344.02713 epoch time:  0.181 s\n",
      "epoch: 9 train loss: 0.032457784 ic_loss: 0.010022995 data_loss: 0.012411795 val_loss: 0.039276786 phy_loss: 301.3161 epoch time:  0.168 s\n",
      "epoch: 10 train loss: 0.027750801 ic_loss: 0.008489873 data_loss: 0.010771056 val_loss: 0.037965972 phy_loss: 310.4488 epoch time:  0.159 s\n",
      "epoch: 11 train loss: 0.0076310486 ic_loss: 0.0019796316 data_loss: 0.0036717854 val_loss: 0.031909 phy_loss: 358.9909 epoch time:  0.130 s\n",
      "epoch: 12 train loss: 0.008330074 ic_loss: 0.0022419605 data_loss: 0.0038461532 val_loss: 0.03257039 phy_loss: 424.85175 epoch time:  0.153 s\n",
      "epoch: 13 train loss: 0.021797765 ic_loss: 0.006647556 data_loss: 0.008502652 val_loss: 0.037142586 phy_loss: 466.17737 epoch time:  0.154 s\n",
      "epoch: 14 train loss: 0.018005246 ic_loss: 0.0054102875 data_loss: 0.0071846712 val_loss: 0.035914406 phy_loss: 455.007 epoch time:  0.155 s\n",
      "epoch: 15 train loss: 0.0054027643 ic_loss: 0.0012915538 data_loss: 0.0028196569 val_loss: 0.03169038 phy_loss: 406.5189 epoch time:  0.150 s\n",
      "epoch: 16 train loss: 0.006027311 ic_loss: 0.0014778372 data_loss: 0.003071637 val_loss: 0.03168514 phy_loss: 354.81702 epoch time:  0.151 s\n",
      "epoch: 17 train loss: 0.014524633 ic_loss: 0.00422331 data_loss: 0.0060780137 val_loss: 0.034304276 phy_loss: 325.54895 epoch time:  0.146 s\n",
      "epoch: 18 train loss: 0.013527118 ic_loss: 0.0038862156 data_loss: 0.005754687 val_loss: 0.034022197 phy_loss: 326.37933 epoch time:  0.153 s\n",
      "epoch: 19 train loss: 0.0053326674 ic_loss: 0.0012240786 data_loss: 0.00288451 val_loss: 0.031566396 phy_loss: 353.00153 epoch time:  0.154 s\n",
      "epoch: 20 train loss: 0.004021447 ic_loss: 0.0008175871 data_loss: 0.0023862731 val_loss: 0.031394474 phy_loss: 392.56976 epoch time:  0.160 s\n",
      "epoch: 21 train loss: 0.009884438 ic_loss: 0.0027478226 data_loss: 0.004388793 val_loss: 0.03349047 phy_loss: 423.32056 epoch time:  0.293 s\n",
      "epoch: 22 train loss: 0.010467967 ic_loss: 0.002937854 data_loss: 0.0045922594 val_loss: 0.03377234 phy_loss: 426.57697 epoch time:  0.217 s\n",
      "epoch: 23 train loss: 0.0049000396 ic_loss: 0.0011069634 data_loss: 0.002686113 val_loss: 0.031970598 phy_loss: 403.31744 epoch time:  0.154 s\n",
      "epoch: 24 train loss: 0.0033944962 ic_loss: 0.0006046509 data_loss: 0.0021851943 val_loss: 0.031444885 phy_loss: 371.23462 epoch time:  0.227 s\n",
      "best loss 0.0033944962 save model\n",
      "epoch: 25 train loss: 0.007136262 ic_loss: 0.0018252442 data_loss: 0.0034857735 val_loss: 0.032656632 phy_loss: 349.60138 epoch time:  0.185 s\n",
      "epoch: 26 train loss: 0.007900774 ic_loss: 0.002080823 data_loss: 0.0037391274 val_loss: 0.032990582 phy_loss: 348.5257 epoch time:  0.272 s\n",
      "epoch: 27 train loss: 0.004348184 ic_loss: 0.00092142256 data_loss: 0.0025053388 val_loss: 0.031991713 phy_loss: 367.4348 epoch time:  0.148 s\n",
      "epoch: 28 train loss: 0.003181572 ic_loss: 0.0005281044 data_loss: 0.0021253633 val_loss: 0.031767208 phy_loss: 396.6914 epoch time:  0.159 s\n",
      "best loss 0.003181572 save model\n",
      "epoch: 29 train loss: 0.005702177 ic_loss: 0.0013381036 data_loss: 0.00302597 val_loss: 0.032687984 phy_loss: 419.9187 epoch time:  0.153 s\n",
      "epoch: 30 train loss: 0.0060964604 ic_loss: 0.0014690985 data_loss: 0.0031582634 val_loss: 0.032856096 phy_loss: 423.2553 epoch time:  0.185 s\n",
      "epoch: 31 train loss: 0.003608352 ic_loss: 0.00067884946 data_loss: 0.0022506532 val_loss: 0.032029238 phy_loss: 406.5824 epoch time:  0.155 s\n",
      "epoch: 32 train loss: 0.0031469613 ic_loss: 0.00056265324 data_loss: 0.0020216547 val_loss: 0.03181699 phy_loss: 382.65485 epoch time:  0.146 s\n",
      "best loss 0.0031469613 save model\n",
      "epoch: 33 train loss: 0.004910022 ic_loss: 0.0011656027 data_loss: 0.0025788166 val_loss: 0.03232329 phy_loss: 366.28693 epoch time:  0.137 s\n",
      "epoch: 34 train loss: 0.0048236772 ic_loss: 0.001140143 data_loss: 0.002543391 val_loss: 0.032260366 phy_loss: 365.35724 epoch time:  0.146 s\n",
      "epoch: 35 train loss: 0.0030512572 ic_loss: 0.0005429255 data_loss: 0.0019654061 val_loss: 0.031684183 phy_loss: 378.6032 epoch time:  0.144 s\n",
      "best loss 0.0030512572 save model\n",
      "epoch: 36 train loss: 0.003021317 ic_loss: 0.0005148646 data_loss: 0.001991588 val_loss: 0.03168495 phy_loss: 397.12442 epoch time:  0.152 s\n",
      "best loss 0.003021317 save model\n",
      "epoch: 37 train loss: 0.004276075 ic_loss: 0.00092229684 data_loss: 0.0024314814 val_loss: 0.032086346 phy_loss: 408.54224 epoch time:  0.155 s\n",
      "epoch: 38 train loss: 0.0039014593 ic_loss: 0.00081061193 data_loss: 0.0022802355 val_loss: 0.03192441 phy_loss: 405.2475 epoch time:  0.161 s\n",
      "epoch: 39 train loss: 0.0027258128 ic_loss: 0.00044021662 data_loss: 0.0018453796 val_loss: 0.031466797 phy_loss: 390.3824 epoch time:  0.159 s\n",
      "best loss 0.0027258128 save model\n",
      "epoch: 40 train loss: 0.003046881 ic_loss: 0.00055471604 data_loss: 0.0019374489 val_loss: 0.031485453 phy_loss: 374.5586 epoch time:  0.171 s\n",
      "epoch: 41 train loss: 0.003790486 ic_loss: 0.0008015522 data_loss: 0.0021873815 val_loss: 0.031675965 phy_loss: 367.56708 epoch time:  0.160 s\n",
      "epoch: 42 train loss: 0.0032505365 ic_loss: 0.00062844064 data_loss: 0.0019936552 val_loss: 0.03151096 phy_loss: 372.80597 epoch time:  0.153 s\n",
      "epoch: 43 train loss: 0.0026114883 ic_loss: 0.00042857818 data_loss: 0.001754332 val_loss: 0.03135915 phy_loss: 386.375 epoch time:  0.190 s\n",
      "best loss 0.0026114883 save model\n",
      "epoch: 44 train loss: 0.0030608212 ic_loss: 0.00058760797 data_loss: 0.0018856053 val_loss: 0.031559538 phy_loss: 399.3157 epoch time:  0.140 s\n",
      "epoch: 45 train loss: 0.0033346456 ic_loss: 0.00068342924 data_loss: 0.0019677873 val_loss: 0.031661417 phy_loss: 403.10516 epoch time:  0.149 s\n",
      "epoch: 46 train loss: 0.0027668823 ic_loss: 0.0004955834 data_loss: 0.0017757155 val_loss: 0.031441465 phy_loss: 396.10785 epoch time:  0.155 s\n",
      "epoch: 47 train loss: 0.002610017 ic_loss: 0.0004405342 data_loss: 0.0017289486 val_loss: 0.031332597 phy_loss: 384.58118 epoch time:  0.183 s\n",
      "best loss 0.002610017 save model\n",
      "epoch: 48 train loss: 0.0030249408 ic_loss: 0.0005780913 data_loss: 0.0018687584 val_loss: 0.03142715 phy_loss: 377.17737 epoch time:  0.158 s\n",
      "epoch: 49 train loss: 0.0029174434 ic_loss: 0.00055095344 data_loss: 0.0018155366 val_loss: 0.031393096 phy_loss: 378.74072 epoch time:  0.154 s\n",
      "epoch: 50 train loss: 0.0025217962 ic_loss: 0.0004311717 data_loss: 0.0016594528 val_loss: 0.031294495 phy_loss: 387.73975 epoch time:  0.165 s\n",
      "best loss 0.0025217962 save model\n",
      "epoch: 51 train loss: 0.0026807818 ic_loss: 0.00049031724 data_loss: 0.0017001473 val_loss: 0.03137029 phy_loss: 397.54248 epoch time:  0.197 s\n",
      "epoch: 52 train loss: 0.0028790259 ic_loss: 0.0005595933 data_loss: 0.0017598392 val_loss: 0.03142535 phy_loss: 400.95398 epoch time:  0.187 s\n",
      "epoch: 53 train loss: 0.0025953043 ic_loss: 0.0004708952 data_loss: 0.0016535139 val_loss: 0.031287212 phy_loss: 395.81772 epoch time:  0.176 s\n",
      "epoch: 54 train loss: 0.0024823146 ic_loss: 0.0004391241 data_loss: 0.0016040665 val_loss: 0.031186981 phy_loss: 386.58408 epoch time:  0.167 s\n",
      "best loss 0.0024823146 save model\n",
      "epoch: 55 train loss: 0.0027060837 ic_loss: 0.0005175455 data_loss: 0.0016709927 val_loss: 0.031207217 phy_loss: 380.23993 epoch time:  0.159 s\n",
      "epoch: 56 train loss: 0.0026487852 ic_loss: 0.0005031622 data_loss: 0.0016424608 val_loss: 0.03116662 phy_loss: 380.92755 epoch time:  0.172 s\n",
      "epoch: 57 train loss: 0.0024394803 ic_loss: 0.00043916394 data_loss: 0.0015611524 val_loss: 0.031103278 phy_loss: 387.44763 epoch time:  0.151 s\n",
      "best loss 0.0024394803 save model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 14913 train loss: 0.0012426777 ic_loss: 0.0004164279 data_loss: 0.00040982183 val_loss: 0.031901624 phy_loss: 394.97473 epoch time:  0.151 s\n",
      "best loss 0.0012426777 save model\n",
      "epoch: 14914 train loss: 0.0012426733 ic_loss: 0.0004164262 data_loss: 0.00040982093 val_loss: 0.031901624 phy_loss: 394.97485 epoch time:  0.152 s\n",
      "best loss 0.0012426733 save model\n",
      "epoch: 14915 train loss: 0.0012426688 ic_loss: 0.00041642456 data_loss: 0.00040981974 val_loss: 0.03190162 phy_loss: 394.97388 epoch time:  0.153 s\n",
      "best loss 0.0012426688 save model\n",
      "epoch: 14916 train loss: 0.0012426645 ic_loss: 0.0004164233 data_loss: 0.00040981793 val_loss: 0.031901624 phy_loss: 394.9753 epoch time:  0.140 s\n",
      "best loss 0.0012426645 save model\n",
      "epoch: 14917 train loss: 0.0012426602 ic_loss: 0.00041642186 data_loss: 0.00040981654 val_loss: 0.03190162 phy_loss: 394.97372 epoch time:  0.132 s\n",
      "best loss 0.0012426602 save model\n",
      "epoch: 14918 train loss: 0.0012426558 ic_loss: 0.00041642037 data_loss: 0.0004098151 val_loss: 0.031901628 phy_loss: 394.9754 epoch time:  0.143 s\n",
      "best loss 0.0012426558 save model\n",
      "epoch: 14919 train loss: 0.0012426515 ic_loss: 0.00041641865 data_loss: 0.0004098142 val_loss: 0.031901628 phy_loss: 394.97382 epoch time:  0.184 s\n",
      "best loss 0.0012426515 save model\n",
      "epoch: 14920 train loss: 0.0012426472 ic_loss: 0.0004164174 data_loss: 0.00040981235 val_loss: 0.03190163 phy_loss: 394.97528 epoch time:  0.180 s\n",
      "best loss 0.0012426472 save model\n",
      "epoch: 14921 train loss: 0.0012426428 ic_loss: 0.0004164156 data_loss: 0.00040981153 val_loss: 0.031901628 phy_loss: 394.9741 epoch time:  0.154 s\n",
      "best loss 0.0012426428 save model\n",
      "epoch: 14922 train loss: 0.0012426383 ic_loss: 0.00041641426 data_loss: 0.00040980987 val_loss: 0.031901628 phy_loss: 394.9748 epoch time:  0.158 s\n",
      "best loss 0.0012426383 save model\n",
      "epoch: 14923 train loss: 0.001242634 ic_loss: 0.0004164126 data_loss: 0.00040980882 val_loss: 0.03190163 phy_loss: 394.97455 epoch time:  0.137 s\n",
      "best loss 0.001242634 save model\n",
      "epoch: 14924 train loss: 0.0012426297 ic_loss: 0.00041641126 data_loss: 0.0004098072 val_loss: 0.03190163 phy_loss: 394.97433 epoch time:  0.248 s\n",
      "best loss 0.0012426297 save model\n",
      "epoch: 14925 train loss: 0.0012426252 ic_loss: 0.00041640963 data_loss: 0.0004098059 val_loss: 0.03190163 phy_loss: 394.9748 epoch time:  0.141 s\n",
      "best loss 0.0012426252 save model\n",
      "epoch: 14926 train loss: 0.0012426209 ic_loss: 0.00041640812 data_loss: 0.0004098047 val_loss: 0.031901635 phy_loss: 394.97424 epoch time:  0.150 s\n",
      "best loss 0.0012426209 save model\n",
      "epoch: 14927 train loss: 0.0012426166 ic_loss: 0.00041640655 data_loss: 0.0004098035 val_loss: 0.03190164 phy_loss: 394.97485 epoch time:  0.155 s\n",
      "best loss 0.0012426166 save model\n",
      "epoch: 14928 train loss: 0.0012426123 ic_loss: 0.000416405 data_loss: 0.0004098022 val_loss: 0.031901635 phy_loss: 394.9742 epoch time:  0.172 s\n",
      "best loss 0.0012426123 save model\n",
      "epoch: 14929 train loss: 0.0012426078 ic_loss: 0.00041640358 data_loss: 0.00040980068 val_loss: 0.03190164 phy_loss: 394.97495 epoch time:  0.195 s\n",
      "best loss 0.0012426078 save model\n",
      "epoch: 14930 train loss: 0.0012426036 ic_loss: 0.0004164021 data_loss: 0.0004097994 val_loss: 0.03190164 phy_loss: 394.97418 epoch time:  0.136 s\n",
      "best loss 0.0012426036 save model\n",
      "epoch: 14931 train loss: 0.0012425991 ic_loss: 0.00041640052 data_loss: 0.0004097981 val_loss: 0.03190164 phy_loss: 394.97485 epoch time:  0.148 s\n",
      "best loss 0.0012425991 save model\n",
      "epoch: 14932 train loss: 0.0012425947 ic_loss: 0.0004163989 data_loss: 0.00040979686 val_loss: 0.031901635 phy_loss: 394.97412 epoch time:  0.182 s\n",
      "best loss 0.0012425947 save model\n",
      "epoch: 14933 train loss: 0.0012425905 ic_loss: 0.00041639758 data_loss: 0.0004097953 val_loss: 0.031901643 phy_loss: 394.9748 epoch time:  0.156 s\n",
      "best loss 0.0012425905 save model\n",
      "epoch: 14934 train loss: 0.0012425861 ic_loss: 0.0004163959 data_loss: 0.00040979424 val_loss: 0.031901643 phy_loss: 394.97424 epoch time:  0.143 s\n",
      "best loss 0.0012425861 save model\n",
      "epoch: 14935 train loss: 0.0012425815 ic_loss: 0.0004163944 data_loss: 0.00040979276 val_loss: 0.031901646 phy_loss: 394.97473 epoch time:  0.168 s\n",
      "best loss 0.0012425815 save model\n",
      "epoch: 14936 train loss: 0.0012425772 ic_loss: 0.00041639304 data_loss: 0.00040979113 val_loss: 0.031901643 phy_loss: 394.97443 epoch time:  0.134 s\n",
      "best loss 0.0012425772 save model\n",
      "epoch: 14937 train loss: 0.0012425729 ic_loss: 0.00041639153 data_loss: 0.0004097898 val_loss: 0.031901646 phy_loss: 394.97455 epoch time:  0.151 s\n",
      "best loss 0.0012425729 save model\n",
      "epoch: 14938 train loss: 0.0012425685 ic_loss: 0.00041638993 data_loss: 0.00040978866 val_loss: 0.031901646 phy_loss: 394.9746 epoch time:  0.144 s\n",
      "best loss 0.0012425685 save model\n",
      "epoch: 14939 train loss: 0.001242564 ic_loss: 0.00041638833 data_loss: 0.00040978732 val_loss: 0.031901646 phy_loss: 394.9743 epoch time:  0.153 s\n",
      "best loss 0.001242564 save model\n",
      "epoch: 14940 train loss: 0.0012425598 ic_loss: 0.0004163869 data_loss: 0.00040978595 val_loss: 0.031901646 phy_loss: 394.97467 epoch time:  0.150 s\n",
      "best loss 0.0012425598 save model\n",
      "epoch: 14941 train loss: 0.0012425552 ic_loss: 0.00041638542 data_loss: 0.0004097844 val_loss: 0.031901646 phy_loss: 394.97427 epoch time:  0.204 s\n",
      "best loss 0.0012425552 save model\n",
      "epoch: 14942 train loss: 0.001242551 ic_loss: 0.00041638393 data_loss: 0.00040978313 val_loss: 0.031901646 phy_loss: 394.9746 epoch time:  0.159 s\n",
      "best loss 0.001242551 save model\n",
      "epoch: 14943 train loss: 0.0012425466 ic_loss: 0.0004163823 data_loss: 0.00040978193 val_loss: 0.031901646 phy_loss: 394.97443 epoch time:  0.145 s\n",
      "best loss 0.0012425466 save model\n",
      "epoch: 14944 train loss: 0.0012425422 ic_loss: 0.00041638085 data_loss: 0.0004097805 val_loss: 0.03190165 phy_loss: 394.97443 epoch time:  0.143 s\n",
      "best loss 0.0012425422 save model\n",
      "epoch: 14945 train loss: 0.0012425377 ic_loss: 0.00041637936 data_loss: 0.00040977896 val_loss: 0.03190165 phy_loss: 394.97458 epoch time:  0.154 s\n",
      "best loss 0.0012425377 save model\n",
      "epoch: 14946 train loss: 0.0012425334 ic_loss: 0.00041637768 data_loss: 0.00040977806 val_loss: 0.031901654 phy_loss: 394.9743 epoch time:  0.202 s\n",
      "best loss 0.0012425334 save model\n",
      "epoch: 14947 train loss: 0.0012425289 ic_loss: 0.00041637616 data_loss: 0.00040977658 val_loss: 0.031901658 phy_loss: 394.9747 epoch time:  0.143 s\n",
      "best loss 0.0012425289 save model\n",
      "epoch: 14948 train loss: 0.0012425245 ic_loss: 0.00041637477 data_loss: 0.00040977498 val_loss: 0.031901654 phy_loss: 394.97418 epoch time:  0.156 s\n",
      "best loss 0.0012425245 save model\n",
      "epoch: 14949 train loss: 0.0012425202 ic_loss: 0.00041637316 data_loss: 0.0004097738 val_loss: 0.031901658 phy_loss: 394.9748 epoch time:  0.141 s\n",
      "best loss 0.0012425202 save model\n",
      "epoch: 14950 train loss: 0.0012425157 ic_loss: 0.00041637153 data_loss: 0.00040977268 val_loss: 0.031901658 phy_loss: 394.97412 epoch time:  0.137 s\n",
      "best loss 0.0012425157 save model\n",
      "epoch: 14951 train loss: 0.0012425113 ic_loss: 0.0004163702 data_loss: 0.00040977093 val_loss: 0.031901658 phy_loss: 394.97473 epoch time:  0.148 s\n",
      "best loss 0.0012425113 save model\n",
      "epoch: 14952 train loss: 0.0012425069 ic_loss: 0.0004163687 data_loss: 0.00040976945 val_loss: 0.031901658 phy_loss: 394.97424 epoch time:  0.148 s\n",
      "best loss 0.0012425069 save model\n",
      "epoch: 14953 train loss: 0.0012425026 ic_loss: 0.00041636714 data_loss: 0.00040976825 val_loss: 0.03190166 phy_loss: 394.97446 epoch time:  0.168 s\n",
      "best loss 0.0012425026 save model\n",
      "epoch: 14954 train loss: 0.001242498 ic_loss: 0.00041636563 data_loss: 0.00040976683 val_loss: 0.031901658 phy_loss: 394.97452 epoch time:  0.130 s\n",
      "best loss 0.001242498 save model\n",
      "epoch: 14955 train loss: 0.0012424939 ic_loss: 0.0004163642 data_loss: 0.00040976543 val_loss: 0.031901658 phy_loss: 394.97418 epoch time:  0.178 s\n",
      "best loss 0.0012424939 save model\n",
      "epoch: 14956 train loss: 0.0012424893 ic_loss: 0.00041636266 data_loss: 0.000409764 val_loss: 0.03190166 phy_loss: 394.97482 epoch time:  0.147 s\n",
      "best loss 0.0012424893 save model\n",
      "epoch: 14957 train loss: 0.0012424849 ic_loss: 0.0004163611 data_loss: 0.00040976275 val_loss: 0.031901658 phy_loss: 394.97388 epoch time:  0.170 s\n",
      "best loss 0.0012424849 save model\n",
      "epoch: 14958 train loss: 0.0012424804 ic_loss: 0.0004163596 data_loss: 0.00040976118 val_loss: 0.031901665 phy_loss: 394.97504 epoch time:  0.153 s\n",
      "best loss 0.0012424804 save model\n",
      "epoch: 14959 train loss: 0.001242476 ic_loss: 0.0004163581 data_loss: 0.0004097599 val_loss: 0.03190166 phy_loss: 394.97375 epoch time:  0.149 s\n",
      "best loss 0.001242476 save model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 14960 train loss: 0.0012424716 ic_loss: 0.00041635666 data_loss: 0.00040975827 val_loss: 0.03190167 phy_loss: 394.97516 epoch time:  0.154 s\n",
      "best loss 0.0012424716 save model\n",
      "epoch: 14961 train loss: 0.0012424672 ic_loss: 0.00041635495 data_loss: 0.00040975728 val_loss: 0.03190166 phy_loss: 394.97363 epoch time:  0.173 s\n",
      "best loss 0.0012424672 save model\n",
      "epoch: 14962 train loss: 0.0012424629 ic_loss: 0.0004163536 data_loss: 0.00040975565 val_loss: 0.031901665 phy_loss: 394.97522 epoch time:  0.144 s\n",
      "best loss 0.0012424629 save model\n",
      "epoch: 14963 train loss: 0.0012424582 ic_loss: 0.0004163517 data_loss: 0.00040975484 val_loss: 0.031901665 phy_loss: 394.9736 epoch time:  0.181 s\n",
      "best loss 0.0012424582 save model\n",
      "epoch: 14964 train loss: 0.0012424539 ic_loss: 0.00041635038 data_loss: 0.00040975312 val_loss: 0.03190167 phy_loss: 394.97522 epoch time:  0.157 s\n",
      "best loss 0.0012424539 save model\n",
      "epoch: 14965 train loss: 0.0012424495 ic_loss: 0.00041634883 data_loss: 0.0004097518 val_loss: 0.031901665 phy_loss: 394.97363 epoch time:  0.178 s\n",
      "best loss 0.0012424495 save model\n",
      "epoch: 14966 train loss: 0.0012424451 ic_loss: 0.0004163475 data_loss: 0.00040975004 val_loss: 0.03190167 phy_loss: 394.97516 epoch time:  0.147 s\n",
      "best loss 0.0012424451 save model\n",
      "epoch: 14967 train loss: 0.0012424407 ic_loss: 0.00041634578 data_loss: 0.00040974907 val_loss: 0.031901665 phy_loss: 394.97354 epoch time:  0.172 s\n",
      "best loss 0.0012424407 save model\n",
      "epoch: 14968 train loss: 0.0012424362 ic_loss: 0.00041634444 data_loss: 0.00040974733 val_loss: 0.031901672 phy_loss: 394.97528 epoch time:  0.143 s\n",
      "best loss 0.0012424362 save model\n",
      "epoch: 14969 train loss: 0.0012424318 ic_loss: 0.00041634255 data_loss: 0.00040974675 val_loss: 0.031901665 phy_loss: 394.97345 epoch time:  0.147 s\n",
      "best loss 0.0012424318 save model\n",
      "epoch: 14970 train loss: 0.0012424274 ic_loss: 0.0004163414 data_loss: 0.00040974453 val_loss: 0.031901672 phy_loss: 394.9753 epoch time:  0.192 s\n",
      "best loss 0.0012424274 save model\n",
      "epoch: 14971 train loss: 0.001242423 ic_loss: 0.00041633964 data_loss: 0.00040974375 val_loss: 0.03190167 phy_loss: 394.9735 epoch time:  0.135 s\n",
      "best loss 0.001242423 save model\n",
      "epoch: 14972 train loss: 0.0012424184 ic_loss: 0.00041633824 data_loss: 0.00040974197 val_loss: 0.031901676 phy_loss: 394.97516 epoch time:  0.220 s\n",
      "best loss 0.0012424184 save model\n",
      "epoch: 14973 train loss: 0.0012424141 ic_loss: 0.00041633652 data_loss: 0.00040974113 val_loss: 0.031901672 phy_loss: 394.97357 epoch time:  0.155 s\n",
      "best loss 0.0012424141 save model\n",
      "epoch: 14974 train loss: 0.0012424096 ic_loss: 0.0004163353 data_loss: 0.000409739 val_loss: 0.03190168 phy_loss: 394.97528 epoch time:  0.165 s\n",
      "best loss 0.0012424096 save model\n",
      "epoch: 14975 train loss: 0.0012424053 ic_loss: 0.00041633344 data_loss: 0.00040973845 val_loss: 0.031901672 phy_loss: 394.9735 epoch time:  0.151 s\n",
      "best loss 0.0012424053 save model\n",
      "epoch: 14976 train loss: 0.0012424007 ic_loss: 0.00041633233 data_loss: 0.00040973607 val_loss: 0.03190168 phy_loss: 394.97534 epoch time:  0.149 s\n",
      "best loss 0.0012424007 save model\n",
      "epoch: 14977 train loss: 0.0012423964 ic_loss: 0.00041633053 data_loss: 0.00040973534 val_loss: 0.031901676 phy_loss: 394.97327 epoch time:  0.156 s\n",
      "best loss 0.0012423964 save model\n",
      "epoch: 14978 train loss: 0.001242392 ic_loss: 0.00041632928 data_loss: 0.00040973345 val_loss: 0.03190168 phy_loss: 394.97546 epoch time:  0.154 s\n",
      "best loss 0.001242392 save model\n",
      "epoch: 14979 train loss: 0.0012423875 ic_loss: 0.00041632738 data_loss: 0.00040973272 val_loss: 0.031901676 phy_loss: 394.97308 epoch time:  0.138 s\n",
      "best loss 0.0012423875 save model\n",
      "epoch: 14980 train loss: 0.0012423832 ic_loss: 0.00041632642 data_loss: 0.0004097303 val_loss: 0.03190168 phy_loss: 394.9757 epoch time:  0.138 s\n",
      "best loss 0.0012423832 save model\n",
      "epoch: 14981 train loss: 0.0012423786 ic_loss: 0.00041632442 data_loss: 0.0004097298 val_loss: 0.031901676 phy_loss: 394.97287 epoch time:  0.171 s\n",
      "best loss 0.0012423786 save model\n",
      "epoch: 14982 train loss: 0.0012423741 ic_loss: 0.00041632337 data_loss: 0.0004097274 val_loss: 0.031901687 phy_loss: 394.9759 epoch time:  0.155 s\n",
      "best loss 0.0012423741 save model\n",
      "epoch: 14983 train loss: 0.0012423698 ic_loss: 0.00041632133 data_loss: 0.0004097271 val_loss: 0.03190168 phy_loss: 394.97272 epoch time:  0.181 s\n",
      "best loss 0.0012423698 save model\n",
      "epoch: 14984 train loss: 0.0012423653 ic_loss: 0.0004163203 data_loss: 0.00040972471 val_loss: 0.03190169 phy_loss: 394.9762 epoch time:  0.187 s\n",
      "best loss 0.0012423653 save model\n",
      "epoch: 14985 train loss: 0.0012423608 ic_loss: 0.00041631836 data_loss: 0.0004097241 val_loss: 0.031901676 phy_loss: 394.97247 epoch time:  0.146 s\n",
      "best loss 0.0012423608 save model\n",
      "epoch: 14986 train loss: 0.0012423564 ic_loss: 0.00041631729 data_loss: 0.0004097218 val_loss: 0.031901687 phy_loss: 394.97632 epoch time:  0.161 s\n",
      "best loss 0.0012423564 save model\n",
      "epoch: 14987 train loss: 0.001242352 ic_loss: 0.0004163153 data_loss: 0.00040972137 val_loss: 0.031901676 phy_loss: 394.9722 epoch time:  0.146 s\n",
      "best loss 0.001242352 save model\n",
      "epoch: 14988 train loss: 0.0012423474 ic_loss: 0.00041631417 data_loss: 0.00040971904 val_loss: 0.03190169 phy_loss: 394.97635 epoch time:  0.157 s\n",
      "best loss 0.0012423474 save model\n",
      "epoch: 14989 train loss: 0.001242343 ic_loss: 0.00041631202 data_loss: 0.00040971898 val_loss: 0.03190168 phy_loss: 394.97223 epoch time:  0.144 s\n",
      "best loss 0.001242343 save model\n",
      "epoch: 14990 train loss: 0.0012423384 ic_loss: 0.00041631097 data_loss: 0.0004097165 val_loss: 0.03190169 phy_loss: 394.97626 epoch time:  0.156 s\n",
      "best loss 0.0012423384 save model\n",
      "epoch: 14991 train loss: 0.0012423343 ic_loss: 0.00041630908 data_loss: 0.00040971604 val_loss: 0.03190168 phy_loss: 394.9725 epoch time:  0.163 s\n",
      "best loss 0.0012423343 save model\n",
      "epoch: 14992 train loss: 0.0012423296 ic_loss: 0.0004163079 data_loss: 0.00040971374 val_loss: 0.0319017 phy_loss: 394.97614 epoch time:  0.158 s\n",
      "best loss 0.0012423296 save model\n",
      "epoch: 14993 train loss: 0.0012423252 ic_loss: 0.00041630593 data_loss: 0.00040971336 val_loss: 0.031901684 phy_loss: 394.97284 epoch time:  0.196 s\n",
      "best loss 0.0012423252 save model\n",
      "epoch: 14994 train loss: 0.0012423208 ic_loss: 0.00041630483 data_loss: 0.00040971107 val_loss: 0.0319017 phy_loss: 394.97568 epoch time:  0.173 s\n",
      "best loss 0.0012423208 save model\n",
      "epoch: 14995 train loss: 0.0012423162 ic_loss: 0.0004163029 data_loss: 0.00040971037 val_loss: 0.031901684 phy_loss: 394.97305 epoch time:  0.194 s\n",
      "best loss 0.0012423162 save model\n",
      "epoch: 14996 train loss: 0.0012423118 ic_loss: 0.00041630171 data_loss: 0.00040970836 val_loss: 0.031901695 phy_loss: 394.9754 epoch time:  0.175 s\n",
      "best loss 0.0012423118 save model\n",
      "epoch: 14997 train loss: 0.0012423072 ic_loss: 0.0004162999 data_loss: 0.00040970749 val_loss: 0.031901684 phy_loss: 394.97308 epoch time:  0.164 s\n",
      "best loss 0.0012423072 save model\n",
      "epoch: 14998 train loss: 0.0012423028 ic_loss: 0.00041629872 data_loss: 0.00040970545 val_loss: 0.0319017 phy_loss: 394.97534 epoch time:  0.135 s\n",
      "best loss 0.0012423028 save model\n",
      "epoch: 14999 train loss: 0.0012422984 ic_loss: 0.00041629683 data_loss: 0.00040970472 val_loss: 0.031901687 phy_loss: 394.97314 epoch time:  0.135 s\n",
      "best loss 0.0012422984 save model\n",
      "epoch: 15000 train loss: 0.0012422939 ic_loss: 0.00041629552 data_loss: 0.00040970283 val_loss: 0.0319017 phy_loss: 394.97556 epoch time:  0.153 s\n",
      "best loss 0.0012422939 save model\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Model Evaluation and Visualization\n",
    "\n",
    "After training, all data points in the flow field can be inferred. And related results can be visualized.\n",
    "![](./images/results.gif)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "fd69f43f58546b570e94fd7eba7b65e6bcc7a5bbc4eab0408017d18902915d69"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
